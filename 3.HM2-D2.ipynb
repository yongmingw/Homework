{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://admissions.ntust.edu.tw/front_index/images/logo.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<h1><center>MACHINE LEARNING : HOMEWORK 3</center></h1>\n",
    "<h1><center>Data 2 : Authors prediction</center></h1>\n",
    "<h2>Hector LANDES - M10601810</h2>\n",
    "<h2>Machine Learning - CS5087701</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "import scipy.interpolate as interp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nltk.corpus import stopwords\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from ann_visualizer.visualize import ann_viz;\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Users\\Hector Landes\\graphviz-2.38\\release\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv (r'C:\\Users\\Hector Landes\\Desktop\\Data Science Projects\\20191005 Homework_1_ML\\2. Prepared Data\\test.csv')\n",
    "train = pd.read_csv (r'C:\\Users\\Hector Landes\\Desktop\\Data Science Projects\\20191005 Homework_1_ML\\2. Prepared Data\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create boolean value if author is EAP, HPL or MWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in data.author])\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Preprocessing from the HM1</3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "#test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def RemoveStopwords(word_list):\n",
    "    return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "def GetWordnetPos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def LemmatizeTokens(word_list):\n",
    "    return [lemmatizer.lemmatize(word, GetWordnetPos(word)) for word in word_list]\n",
    "   \n",
    "def CountPartOfSpeech(word_list):\n",
    "    tagged = nltk.pos_tag(nltk.Text(word_list))\n",
    "    counts = Counter(tag for word, tag in tagged)\n",
    "    total = sum(counts.values())\n",
    "    ret = dict((word, float(count) / total) for word, count in counts.items())\n",
    "    return ret\n",
    "\n",
    "def ExtractPartOfSpeech(pos_dict, pos_to_extract):\n",
    "    if pos_to_extract in pos_dict.keys():\n",
    "\n",
    "\n",
    "        return pos_dict[pos_to_extract]\n",
    "   \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiki = train.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"tokens\"] = kiki.text.apply(lambda x: LemmatizeTokens(RemoveStopwords(Tokenize(x))))\n",
    "\n",
    "#train[\"tokens\"] = train.text.apply(lambda x: LemmatizeTokens(RemoveStopwords(Tokenize(x))))\n",
    "#test[\"tokens\"] = test.text.apply(lambda x: LemmatizeTokens(RemoveStopwords(Tokenize(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"pos\"] = kiki.tokens.apply(CountPartOfSpeech)\n",
    "\n",
    "#train[\"pos\"] = train.tokens.apply(CountPartOfSpeech)\n",
    "#test[\"pos\"] = test.tokens.apply(CountPartOfSpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"prop_noun\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"NN\"))\n",
    "kiki[\"prop_verb\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"VB\"))\n",
    "kiki[\"prop_adj\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"JJ\"))\n",
    "kiki[\"prop_adv\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"RB\"))\n",
    "\n",
    "#train[\"prop_noun\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"NN\"))\n",
    "#train[\"prop_verb\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"VB\"))\n",
    "#train[\"prop_adj\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"JJ\"))\n",
    "#train[\"prop_adv\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"RB\"))\n",
    "#test[\"prop_noun\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"NN\"))\n",
    "#test[\"prop_verb\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"VB\"))\n",
    "#test[\"prop_adj\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"JJ\"))\n",
    "#test[\"prop_adv\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"RB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_words\"] = kiki[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "#test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"token_num_words\"] = kiki[\"tokens\"].apply(lambda x: len(str(x).split()))\n",
    "#test[\"token_num_words\"] = test[\"tokens\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_unique_words\"] = kiki[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "#test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_chars\"] = kiki[\"text\"].apply(lambda x: len(str(x)))\n",
    "#test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_punctuations\"] = kiki['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "#test[\"num_punctuations\"] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_words_upper\"] = kiki[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_words_title\"] = kiki[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "#test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"mean_word_len\"] = kiki[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "#test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"mean_word_len\"] = kiki[\"tokens\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "#test[\"mean_word_len\"] = test[\"tokens\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"std_word_len\"] = kiki[\"text\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))\n",
    "#test[\"std_word_len\"] = test[\"text\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"std_word_len\"] = kiki[\"tokens\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))\n",
    "#test[\"std_word_len\"] = test[\"tokens\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki['label_author'] = LabelEncoder().fit_transform(kiki['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'text',\n",
       " 'author',\n",
       " 'num_stopwords',\n",
       " 'tokens',\n",
       " 'pos',\n",
       " 'prop_noun',\n",
       " 'prop_verb',\n",
       " 'prop_adj',\n",
       " 'prop_adv',\n",
       " 'num_words',\n",
       " 'token_num_words',\n",
       " 'num_unique_words',\n",
       " 'num_chars',\n",
       " 'num_punctuations',\n",
       " 'num_words_upper',\n",
       " 'num_words_title',\n",
       " 'mean_word_len',\n",
       " 'std_word_len',\n",
       " 'label_author']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = kiki.columns.tolist()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiki = kiki[['id',\n",
    " 'text',\n",
    " 'author',\n",
    " 'tokens',\n",
    " 'pos',\n",
    "'num_stopwords',             \n",
    " 'prop_noun',\n",
    " 'prop_verb',\n",
    " 'prop_adj',\n",
    " 'prop_adv',\n",
    " 'num_words',\n",
    " 'token_num_words',\n",
    " 'num_unique_words',\n",
    " 'num_chars',\n",
    " 'num_punctuations',\n",
    " 'num_words_upper',\n",
    " 'num_words_title',\n",
    " 'mean_word_len',\n",
    " 'std_word_len',\n",
    " 'label_author']]\n",
    "#test = test[['id',\n",
    "# 'text',\n",
    "# 'tokens',\n",
    "# 'pos',\n",
    "#'num_stopwords',             \n",
    "# 'prop_noun',\n",
    "# 'prop_verb',\n",
    "# 'prop_adj',\n",
    "# 'prop_adv',\n",
    "# 'num_words',\n",
    "# 'token_num_words',\n",
    "# 'num_unique_words',\n",
    "# 'num_chars',\n",
    "# 'num_punctuations',\n",
    "# 'num_words_upper',\n",
    "# 'num_words_title',\n",
    "# 'mean_word_len',\n",
    "# 'std_word_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<b> X </b> as the <b> Feature Matrix and normalization </b> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>prop_noun</th>\n",
       "      <th>prop_verb</th>\n",
       "      <th>prop_adj</th>\n",
       "      <th>prop_adv</th>\n",
       "      <th>num_words</th>\n",
       "      <th>token_num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>std_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.422472</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.254042</td>\n",
       "      <td>0.38581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.062921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_stopwords  prop_noun  prop_verb  prop_adj  prop_adv  num_words  \\\n",
       "0       0.365854    0.52381   0.142857  0.571429     0.500     0.4125   \n",
       "1       0.097561    0.00000   1.000000  0.666667     0.875     0.0750   \n",
       "\n",
       "   token_num_words  num_unique_words  num_chars  num_punctuations  \\\n",
       "0         0.447368          0.465517   0.422472               0.6   \n",
       "1         0.052632          0.103448   0.062921               0.0   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  std_word_len  \n",
       "0              0.4         0.333333       0.254042       0.38581  \n",
       "1              0.0         0.000000       0.012702       0.00000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = kiki[kiki.columns[5:19]]\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "X[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> <b> Y </b> as the <b> Target Value </b> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    2\n",
       "4    1\n",
       "Name: label_author, dtype: int32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = kiki['label_author']\n",
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data in training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainset, X_testset, Y_trainset, Y_testset = train_test_split(X, Y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix ->   Training: (8, 14)    Testing: (2, 14)\n",
      "Target value   ->   Training: (8,)       Testing: (2,)\n"
     ]
    }
   ],
   "source": [
    "print('Feature matrix ->   Training:',X_trainset.shape,'   Testing:',X_testset.shape)\n",
    "print('Target value   ->   Training:',Y_trainset.shape,'      Testing:',Y_testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(data)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 3\n",
    "embedding_dims = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [19579, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-361-fe7c31f917f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m hist = model.fit(x_train, y_train,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [19579, 6]"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=1600,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Fit Keras Model<h/3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [19579, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-354-7e036c5b3773>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_trainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_testset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_trainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_testset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [19579, 6]"
     ]
    }
   ],
   "source": [
    "X_trainset, X_testset, Y_trainset, Y_testset = train_test_split(docs, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0543 - accuracy: 0.41 - ETA: 0s - loss: 1.0495 - accuracy: 0.42 - ETA: 0s - loss: 1.0472 - accuracy: 0.43 - ETA: 0s - loss: 1.0470 - accuracy: 0.43 - ETA: 0s - loss: 1.0477 - accuracy: 0.43 - ETA: 0s - loss: 1.0493 - accuracy: 0.42 - ETA: 0s - loss: 1.0493 - accuracy: 0.42 - ETA: 0s - loss: 1.0492 - accuracy: 0.42 - 1s 47us/step - loss: 1.0490 - accuracy: 0.4290 - val_loss: 1.0548 - val_accuracy: 0.4221\n",
      "Epoch 2/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0494 - accuracy: 0.42 - ETA: 0s - loss: 1.0443 - accuracy: 0.44 - ETA: 0s - loss: 1.0437 - accuracy: 0.44 - ETA: 0s - loss: 1.0444 - accuracy: 0.44 - ETA: 0s - loss: 1.0439 - accuracy: 0.44 - ETA: 0s - loss: 1.0445 - accuracy: 0.44 - ETA: 0s - loss: 1.0452 - accuracy: 0.44 - ETA: 0s - loss: 1.0451 - accuracy: 0.44 - 1s 48us/step - loss: 1.0448 - accuracy: 0.4449 - val_loss: 1.0510 - val_accuracy: 0.4311\n",
      "Epoch 3/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0487 - accuracy: 0.44 - ETA: 0s - loss: 1.0464 - accuracy: 0.44 - ETA: 0s - loss: 1.0433 - accuracy: 0.45 - ETA: 0s - loss: 1.0431 - accuracy: 0.45 - ETA: 0s - loss: 1.0426 - accuracy: 0.45 - ETA: 0s - loss: 1.0408 - accuracy: 0.45 - ETA: 0s - loss: 1.0400 - accuracy: 0.45 - ETA: 0s - loss: 1.0399 - accuracy: 0.45 - 1s 47us/step - loss: 1.0402 - accuracy: 0.4571 - val_loss: 1.0469 - val_accuracy: 0.4387\n",
      "Epoch 4/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0311 - accuracy: 0.48 - ETA: 0s - loss: 1.0379 - accuracy: 0.46 - ETA: 0s - loss: 1.0381 - accuracy: 0.46 - ETA: 0s - loss: 1.0382 - accuracy: 0.46 - ETA: 0s - loss: 1.0391 - accuracy: 0.45 - ETA: 0s - loss: 1.0372 - accuracy: 0.46 - ETA: 0s - loss: 1.0362 - accuracy: 0.46 - ETA: 0s - loss: 1.0355 - accuracy: 0.46 - 1s 47us/step - loss: 1.0350 - accuracy: 0.4673 - val_loss: 1.0423 - val_accuracy: 0.4494\n",
      "Epoch 5/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0361 - accuracy: 0.46 - ETA: 0s - loss: 1.0363 - accuracy: 0.46 - ETA: 0s - loss: 1.0322 - accuracy: 0.47 - ETA: 0s - loss: 1.0338 - accuracy: 0.46 - ETA: 0s - loss: 1.0328 - accuracy: 0.47 - ETA: 0s - loss: 1.0304 - accuracy: 0.47 - ETA: 0s - loss: 1.0300 - accuracy: 0.47 - ETA: 0s - loss: 1.0295 - accuracy: 0.48 - 1s 47us/step - loss: 1.0293 - accuracy: 0.4809 - val_loss: 1.0373 - val_accuracy: 0.4599\n",
      "Epoch 6/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0223 - accuracy: 0.49 - ETA: 0s - loss: 1.0236 - accuracy: 0.49 - ETA: 0s - loss: 1.0242 - accuracy: 0.49 - ETA: 0s - loss: 1.0243 - accuracy: 0.48 - ETA: 0s - loss: 1.0238 - accuracy: 0.49 - ETA: 0s - loss: 1.0246 - accuracy: 0.48 - ETA: 0s - loss: 1.0245 - accuracy: 0.48 - ETA: 0s - loss: 1.0231 - accuracy: 0.49 - 1s 46us/step - loss: 1.0230 - accuracy: 0.4942 - val_loss: 1.0316 - val_accuracy: 0.4732\n",
      "Epoch 7/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0195 - accuracy: 0.49 - ETA: 0s - loss: 1.0209 - accuracy: 0.49 - ETA: 0s - loss: 1.0208 - accuracy: 0.49 - ETA: 0s - loss: 1.0196 - accuracy: 0.49 - ETA: 0s - loss: 1.0171 - accuracy: 0.50 - ETA: 0s - loss: 1.0173 - accuracy: 0.50 - ETA: 0s - loss: 1.0172 - accuracy: 0.50 - ETA: 0s - loss: 1.0162 - accuracy: 0.50 - 1s 47us/step - loss: 1.0161 - accuracy: 0.5067 - val_loss: 1.0258 - val_accuracy: 0.4837\n",
      "Epoch 8/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0155 - accuracy: 0.49 - ETA: 0s - loss: 1.0098 - accuracy: 0.51 - ETA: 0s - loss: 1.0124 - accuracy: 0.50 - ETA: 0s - loss: 1.0115 - accuracy: 0.51 - ETA: 0s - loss: 1.0102 - accuracy: 0.51 - ETA: 0s - loss: 1.0099 - accuracy: 0.51 - ETA: 0s - loss: 1.0095 - accuracy: 0.51 - ETA: 0s - loss: 1.0089 - accuracy: 0.51 - 1s 47us/step - loss: 1.0087 - accuracy: 0.5203 - val_loss: 1.0189 - val_accuracy: 0.5026\n",
      "Epoch 9/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.53 - ETA: 0s - loss: 1.0062 - accuracy: 0.53 - ETA: 0s - loss: 1.0048 - accuracy: 0.53 - ETA: 0s - loss: 1.0031 - accuracy: 0.53 - ETA: 0s - loss: 1.0020 - accuracy: 0.53 - ETA: 0s - loss: 1.0028 - accuracy: 0.53 - ETA: 0s - loss: 1.0019 - accuracy: 0.53 - ETA: 0s - loss: 1.0017 - accuracy: 0.53 - ETA: 0s - loss: 1.0011 - accuracy: 0.53 - 1s 49us/step - loss: 1.0006 - accuracy: 0.5351 - val_loss: 1.0120 - val_accuracy: 0.5130\n",
      "Epoch 10/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9860 - accuracy: 0.55 - ETA: 0s - loss: 0.9888 - accuracy: 0.56 - ETA: 0s - loss: 0.9896 - accuracy: 0.56 - ETA: 0s - loss: 0.9905 - accuracy: 0.56 - ETA: 0s - loss: 0.9910 - accuracy: 0.56 - ETA: 0s - loss: 0.9913 - accuracy: 0.56 - ETA: 0s - loss: 0.9912 - accuracy: 0.56 - ETA: 0s - loss: 0.9911 - accuracy: 0.56 - ETA: 0s - loss: 0.9915 - accuracy: 0.56 - ETA: 0s - loss: 0.9920 - accuracy: 0.55 - 1s 50us/step - loss: 0.9920 - accuracy: 0.5580 - val_loss: 1.0049 - val_accuracy: 0.5181\n",
      "Epoch 11/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9878 - accuracy: 0.55 - ETA: 0s - loss: 0.9819 - accuracy: 0.56 - ETA: 0s - loss: 0.9818 - accuracy: 0.56 - ETA: 0s - loss: 0.9826 - accuracy: 0.56 - ETA: 0s - loss: 0.9820 - accuracy: 0.56 - ETA: 0s - loss: 0.9821 - accuracy: 0.56 - ETA: 0s - loss: 0.9820 - accuracy: 0.56 - ETA: 0s - loss: 0.9826 - accuracy: 0.56 - 1s 46us/step - loss: 0.9827 - accuracy: 0.5626 - val_loss: 0.9968 - val_accuracy: 0.5358\n",
      "Epoch 12/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9766 - accuracy: 0.57 - ETA: 0s - loss: 0.9795 - accuracy: 0.56 - ETA: 0s - loss: 0.9756 - accuracy: 0.57 - ETA: 0s - loss: 0.9778 - accuracy: 0.57 - ETA: 0s - loss: 0.9771 - accuracy: 0.57 - ETA: 0s - loss: 0.9762 - accuracy: 0.57 - ETA: 0s - loss: 0.9759 - accuracy: 0.57 - ETA: 0s - loss: 0.9751 - accuracy: 0.57 - ETA: 0s - loss: 0.9745 - accuracy: 0.57 - ETA: 0s - loss: 0.9744 - accuracy: 0.57 - ETA: 0s - loss: 0.9729 - accuracy: 0.57 - 1s 50us/step - loss: 0.9729 - accuracy: 0.5774 - val_loss: 0.9886 - val_accuracy: 0.5506\n",
      "Epoch 13/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9664 - accuracy: 0.57 - ETA: 0s - loss: 0.9663 - accuracy: 0.58 - ETA: 0s - loss: 0.9668 - accuracy: 0.58 - ETA: 0s - loss: 0.9653 - accuracy: 0.58 - ETA: 0s - loss: 0.9645 - accuracy: 0.58 - ETA: 0s - loss: 0.9647 - accuracy: 0.58 - ETA: 0s - loss: 0.9639 - accuracy: 0.58 - ETA: 0s - loss: 0.9631 - accuracy: 0.58 - 1s 47us/step - loss: 0.9627 - accuracy: 0.5897 - val_loss: 0.9800 - val_accuracy: 0.5621\n",
      "Epoch 14/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9595 - accuracy: 0.59 - ETA: 0s - loss: 0.9553 - accuracy: 0.59 - ETA: 0s - loss: 0.9565 - accuracy: 0.59 - ETA: 0s - loss: 0.9563 - accuracy: 0.58 - ETA: 0s - loss: 0.9543 - accuracy: 0.58 - ETA: 0s - loss: 0.9527 - accuracy: 0.59 - ETA: 0s - loss: 0.9520 - accuracy: 0.59 - ETA: 0s - loss: 0.9521 - accuracy: 0.59 - 1s 47us/step - loss: 0.9521 - accuracy: 0.5924 - val_loss: 0.9709 - val_accuracy: 0.5764\n",
      "Epoch 15/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9580 - accuracy: 0.57 - ETA: 0s - loss: 0.9509 - accuracy: 0.59 - ETA: 0s - loss: 0.9490 - accuracy: 0.60 - ETA: 0s - loss: 0.9477 - accuracy: 0.60 - ETA: 0s - loss: 0.9450 - accuracy: 0.61 - ETA: 0s - loss: 0.9446 - accuracy: 0.61 - ETA: 0s - loss: 0.9436 - accuracy: 0.61 - ETA: 0s - loss: 0.9424 - accuracy: 0.61 - ETA: 0s - loss: 0.9410 - accuracy: 0.61 - 1s 50us/step - loss: 0.9412 - accuracy: 0.6116 - val_loss: 0.9627 - val_accuracy: 0.5715\n",
      "Epoch 16/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.62 - ETA: 0s - loss: 0.9333 - accuracy: 0.61 - ETA: 0s - loss: 0.9324 - accuracy: 0.61 - ETA: 0s - loss: 0.9343 - accuracy: 0.61 - ETA: 0s - loss: 0.9336 - accuracy: 0.61 - ETA: 0s - loss: 0.9339 - accuracy: 0.61 - ETA: 0s - loss: 0.9318 - accuracy: 0.61 - ETA: 0s - loss: 0.9301 - accuracy: 0.61 - 1s 49us/step - loss: 0.9301 - accuracy: 0.6159 - val_loss: 0.9543 - val_accuracy: 0.5712\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9287 - accuracy: 0.59 - ETA: 0s - loss: 0.9250 - accuracy: 0.60 - ETA: 0s - loss: 0.9204 - accuracy: 0.61 - ETA: 0s - loss: 0.9234 - accuracy: 0.61 - ETA: 0s - loss: 0.9231 - accuracy: 0.61 - ETA: 0s - loss: 0.9228 - accuracy: 0.61 - ETA: 0s - loss: 0.9206 - accuracy: 0.61 - ETA: 0s - loss: 0.9193 - accuracy: 0.61 - 1s 48us/step - loss: 0.9188 - accuracy: 0.6204 - val_loss: 0.9443 - val_accuracy: 0.5855\n",
      "Epoch 18/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.60 - ETA: 0s - loss: 0.9108 - accuracy: 0.62 - ETA: 0s - loss: 0.9141 - accuracy: 0.61 - ETA: 0s - loss: 0.9116 - accuracy: 0.61 - ETA: 0s - loss: 0.9085 - accuracy: 0.62 - ETA: 0s - loss: 0.9095 - accuracy: 0.62 - ETA: 0s - loss: 0.9100 - accuracy: 0.62 - ETA: 0s - loss: 0.9071 - accuracy: 0.62 - 1s 48us/step - loss: 0.9074 - accuracy: 0.6253 - val_loss: 0.9353 - val_accuracy: 0.5896\n",
      "Epoch 19/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8972 - accuracy: 0.63 - ETA: 0s - loss: 0.9052 - accuracy: 0.61 - ETA: 0s - loss: 0.8997 - accuracy: 0.62 - ETA: 0s - loss: 0.8976 - accuracy: 0.62 - ETA: 0s - loss: 0.8977 - accuracy: 0.62 - ETA: 0s - loss: 0.8974 - accuracy: 0.62 - ETA: 0s - loss: 0.8967 - accuracy: 0.62 - ETA: 0s - loss: 0.8965 - accuracy: 0.62 - 1s 47us/step - loss: 0.8959 - accuracy: 0.6300 - val_loss: 0.9262 - val_accuracy: 0.6001\n",
      "Epoch 20/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8771 - accuracy: 0.65 - ETA: 0s - loss: 0.8874 - accuracy: 0.64 - ETA: 0s - loss: 0.8883 - accuracy: 0.63 - ETA: 0s - loss: 0.8892 - accuracy: 0.63 - ETA: 0s - loss: 0.8893 - accuracy: 0.63 - ETA: 0s - loss: 0.8867 - accuracy: 0.63 - ETA: 0s - loss: 0.8865 - accuracy: 0.63 - ETA: 0s - loss: 0.8850 - accuracy: 0.63 - 1s 46us/step - loss: 0.8846 - accuracy: 0.6349 - val_loss: 0.9175 - val_accuracy: 0.6001\n",
      "Epoch 21/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8700 - accuracy: 0.66 - ETA: 0s - loss: 0.8843 - accuracy: 0.63 - ETA: 0s - loss: 0.8826 - accuracy: 0.63 - ETA: 0s - loss: 0.8785 - accuracy: 0.63 - ETA: 0s - loss: 0.8754 - accuracy: 0.64 - ETA: 0s - loss: 0.8747 - accuracy: 0.64 - ETA: 0s - loss: 0.8734 - accuracy: 0.64 - ETA: 0s - loss: 0.8729 - accuracy: 0.64 - 1s 46us/step - loss: 0.8733 - accuracy: 0.6418 - val_loss: 0.9090 - val_accuracy: 0.6024\n",
      "Epoch 22/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8786 - accuracy: 0.63 - ETA: 0s - loss: 0.8712 - accuracy: 0.63 - ETA: 0s - loss: 0.8640 - accuracy: 0.64 - ETA: 0s - loss: 0.8632 - accuracy: 0.64 - ETA: 0s - loss: 0.8633 - accuracy: 0.64 - ETA: 0s - loss: 0.8636 - accuracy: 0.64 - ETA: 0s - loss: 0.8619 - accuracy: 0.64 - ETA: 0s - loss: 0.8619 - accuracy: 0.64 - 1s 45us/step - loss: 0.8622 - accuracy: 0.6436 - val_loss: 0.9009 - val_accuracy: 0.6034\n",
      "Epoch 23/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8491 - accuracy: 0.64 - ETA: 0s - loss: 0.8575 - accuracy: 0.64 - ETA: 0s - loss: 0.8557 - accuracy: 0.63 - ETA: 0s - loss: 0.8544 - accuracy: 0.64 - ETA: 0s - loss: 0.8533 - accuracy: 0.64 - ETA: 0s - loss: 0.8526 - accuracy: 0.64 - ETA: 0s - loss: 0.8522 - accuracy: 0.64 - ETA: 0s - loss: 0.8514 - accuracy: 0.64 - 1s 46us/step - loss: 0.8513 - accuracy: 0.6476 - val_loss: 0.8930 - val_accuracy: 0.6055\n",
      "Epoch 24/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8475 - accuracy: 0.64 - ETA: 0s - loss: 0.8453 - accuracy: 0.65 - ETA: 0s - loss: 0.8468 - accuracy: 0.64 - ETA: 0s - loss: 0.8425 - accuracy: 0.65 - ETA: 0s - loss: 0.8417 - accuracy: 0.65 - ETA: 0s - loss: 0.8401 - accuracy: 0.65 - ETA: 0s - loss: 0.8419 - accuracy: 0.65 - ETA: 0s - loss: 0.8414 - accuracy: 0.65 - 1s 47us/step - loss: 0.8407 - accuracy: 0.6515 - val_loss: 0.8858 - val_accuracy: 0.6037\n",
      "Epoch 25/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8218 - accuracy: 0.66 - ETA: 0s - loss: 0.8321 - accuracy: 0.65 - ETA: 0s - loss: 0.8344 - accuracy: 0.64 - ETA: 0s - loss: 0.8328 - accuracy: 0.65 - ETA: 0s - loss: 0.8322 - accuracy: 0.65 - ETA: 0s - loss: 0.8316 - accuracy: 0.65 - ETA: 0s - loss: 0.8300 - accuracy: 0.65 - ETA: 0s - loss: 0.8305 - accuracy: 0.65 - ETA: 0s - loss: 0.8306 - accuracy: 0.65 - ETA: 0s - loss: 0.8303 - accuracy: 0.65 - 1s 50us/step - loss: 0.8303 - accuracy: 0.6538 - val_loss: 0.8772 - val_accuracy: 0.6126\n",
      "Epoch 26/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8299 - accuracy: 0.65 - ETA: 0s - loss: 0.8243 - accuracy: 0.66 - ETA: 0s - loss: 0.8213 - accuracy: 0.66 - ETA: 0s - loss: 0.8222 - accuracy: 0.65 - ETA: 0s - loss: 0.8200 - accuracy: 0.65 - ETA: 0s - loss: 0.8214 - accuracy: 0.65 - ETA: 0s - loss: 0.8219 - accuracy: 0.65 - ETA: 0s - loss: 0.8213 - accuracy: 0.65 - 1s 47us/step - loss: 0.8201 - accuracy: 0.6570 - val_loss: 0.8702 - val_accuracy: 0.6136\n",
      "Epoch 27/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8044 - accuracy: 0.67 - ETA: 0s - loss: 0.8070 - accuracy: 0.67 - ETA: 0s - loss: 0.8106 - accuracy: 0.66 - ETA: 0s - loss: 0.8132 - accuracy: 0.66 - ETA: 0s - loss: 0.8134 - accuracy: 0.65 - ETA: 0s - loss: 0.8124 - accuracy: 0.65 - ETA: 0s - loss: 0.8116 - accuracy: 0.65 - ETA: 0s - loss: 0.8107 - accuracy: 0.65 - 1s 45us/step - loss: 0.8101 - accuracy: 0.6598 - val_loss: 0.8625 - val_accuracy: 0.6200\n",
      "Epoch 28/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7962 - accuracy: 0.68 - ETA: 0s - loss: 0.7965 - accuracy: 0.67 - ETA: 0s - loss: 0.8001 - accuracy: 0.66 - ETA: 0s - loss: 0.7986 - accuracy: 0.66 - ETA: 0s - loss: 0.8015 - accuracy: 0.66 - ETA: 0s - loss: 0.8026 - accuracy: 0.66 - ETA: 0s - loss: 0.8025 - accuracy: 0.66 - ETA: 0s - loss: 0.8017 - accuracy: 0.66 - 1s 47us/step - loss: 0.8007 - accuracy: 0.6619 - val_loss: 0.8557 - val_accuracy: 0.6208\n",
      "Epoch 29/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7936 - accuracy: 0.67 - ETA: 0s - loss: 0.8034 - accuracy: 0.66 - ETA: 0s - loss: 0.7962 - accuracy: 0.66 - ETA: 0s - loss: 0.7970 - accuracy: 0.66 - ETA: 0s - loss: 0.7945 - accuracy: 0.66 - ETA: 0s - loss: 0.7942 - accuracy: 0.66 - ETA: 0s - loss: 0.7928 - accuracy: 0.66 - ETA: 0s - loss: 0.7914 - accuracy: 0.66 - 1s 49us/step - loss: 0.7912 - accuracy: 0.6649 - val_loss: 0.8491 - val_accuracy: 0.6221\n",
      "Epoch 30/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7691 - accuracy: 0.68 - ETA: 0s - loss: 0.7807 - accuracy: 0.67 - ETA: 0s - loss: 0.7808 - accuracy: 0.67 - ETA: 0s - loss: 0.7784 - accuracy: 0.67 - ETA: 0s - loss: 0.7815 - accuracy: 0.67 - ETA: 0s - loss: 0.7818 - accuracy: 0.67 - ETA: 0s - loss: 0.7838 - accuracy: 0.66 - ETA: 0s - loss: 0.7824 - accuracy: 0.66 - 1s 49us/step - loss: 0.7821 - accuracy: 0.6670 - val_loss: 0.8443 - val_accuracy: 0.6172\n",
      "Epoch 31/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7631 - accuracy: 0.68 - ETA: 0s - loss: 0.7711 - accuracy: 0.68 - ETA: 0s - loss: 0.7751 - accuracy: 0.67 - ETA: 0s - loss: 0.7719 - accuracy: 0.67 - ETA: 0s - loss: 0.7719 - accuracy: 0.67 - ETA: 0s - loss: 0.7719 - accuracy: 0.67 - ETA: 0s - loss: 0.7734 - accuracy: 0.67 - ETA: 0s - loss: 0.7731 - accuracy: 0.67 - 1s 48us/step - loss: 0.7734 - accuracy: 0.6693 - val_loss: 0.8380 - val_accuracy: 0.6213\n",
      "Epoch 32/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7583 - accuracy: 0.68 - ETA: 0s - loss: 0.7637 - accuracy: 0.67 - ETA: 0s - loss: 0.7684 - accuracy: 0.66 - ETA: 0s - loss: 0.7646 - accuracy: 0.67 - ETA: 0s - loss: 0.7648 - accuracy: 0.67 - ETA: 0s - loss: 0.7630 - accuracy: 0.67 - ETA: 0s - loss: 0.7636 - accuracy: 0.67 - ETA: 0s - loss: 0.7639 - accuracy: 0.67 - 1s 48us/step - loss: 0.7647 - accuracy: 0.6708 - val_loss: 0.8310 - val_accuracy: 0.6251\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7578 - accuracy: 0.67 - ETA: 0s - loss: 0.7661 - accuracy: 0.66 - ETA: 0s - loss: 0.7557 - accuracy: 0.67 - ETA: 0s - loss: 0.7599 - accuracy: 0.67 - ETA: 0s - loss: 0.7611 - accuracy: 0.67 - ETA: 0s - loss: 0.7566 - accuracy: 0.67 - ETA: 0s - loss: 0.7576 - accuracy: 0.67 - ETA: 0s - loss: 0.7574 - accuracy: 0.67 - ETA: 0s - loss: 0.7574 - accuracy: 0.67 - 1s 50us/step - loss: 0.7564 - accuracy: 0.6738 - val_loss: 0.8261 - val_accuracy: 0.6251\n",
      "Epoch 34/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7346 - accuracy: 0.69 - ETA: 0s - loss: 0.7497 - accuracy: 0.67 - ETA: 0s - loss: 0.7465 - accuracy: 0.67 - ETA: 0s - loss: 0.7460 - accuracy: 0.67 - ETA: 0s - loss: 0.7497 - accuracy: 0.67 - ETA: 0s - loss: 0.7506 - accuracy: 0.67 - ETA: 0s - loss: 0.7492 - accuracy: 0.67 - ETA: 0s - loss: 0.7487 - accuracy: 0.67 - 1s 48us/step - loss: 0.7484 - accuracy: 0.6751 - val_loss: 0.8214 - val_accuracy: 0.6246\n",
      "Epoch 35/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7403 - accuracy: 0.69 - ETA: 0s - loss: 0.7408 - accuracy: 0.68 - ETA: 0s - loss: 0.7399 - accuracy: 0.68 - ETA: 0s - loss: 0.7378 - accuracy: 0.68 - ETA: 0s - loss: 0.7391 - accuracy: 0.68 - ETA: 0s - loss: 0.7400 - accuracy: 0.68 - ETA: 0s - loss: 0.7400 - accuracy: 0.68 - ETA: 0s - loss: 0.7404 - accuracy: 0.67 - ETA: 0s - loss: 0.7381 - accuracy: 0.68 - ETA: 0s - loss: 0.7392 - accuracy: 0.67 - ETA: 0s - loss: 0.7386 - accuracy: 0.68 - ETA: 0s - loss: 0.7395 - accuracy: 0.67 - 1s 52us/step - loss: 0.7407 - accuracy: 0.6773 - val_loss: 0.8148 - val_accuracy: 0.6277\n",
      "Epoch 36/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7320 - accuracy: 0.69 - ETA: 0s - loss: 0.7389 - accuracy: 0.67 - ETA: 0s - loss: 0.7391 - accuracy: 0.67 - ETA: 0s - loss: 0.7377 - accuracy: 0.67 - ETA: 0s - loss: 0.7362 - accuracy: 0.67 - ETA: 0s - loss: 0.7336 - accuracy: 0.67 - ETA: 0s - loss: 0.7335 - accuracy: 0.67 - ETA: 0s - loss: 0.7333 - accuracy: 0.67 - 1s 49us/step - loss: 0.7331 - accuracy: 0.6780 - val_loss: 0.8099 - val_accuracy: 0.6292\n",
      "Epoch 37/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.66 - ETA: 0s - loss: 0.7367 - accuracy: 0.66 - ETA: 0s - loss: 0.7360 - accuracy: 0.66 - ETA: 0s - loss: 0.7333 - accuracy: 0.66 - ETA: 0s - loss: 0.7297 - accuracy: 0.67 - ETA: 0s - loss: 0.7275 - accuracy: 0.67 - ETA: 0s - loss: 0.7266 - accuracy: 0.67 - ETA: 0s - loss: 0.7262 - accuracy: 0.67 - 1s 49us/step - loss: 0.7259 - accuracy: 0.6796 - val_loss: 0.8052 - val_accuracy: 0.6302\n",
      "Epoch 38/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7229 - accuracy: 0.67 - ETA: 0s - loss: 0.7182 - accuracy: 0.67 - ETA: 0s - loss: 0.7208 - accuracy: 0.67 - ETA: 0s - loss: 0.7235 - accuracy: 0.67 - ETA: 0s - loss: 0.7214 - accuracy: 0.67 - ETA: 0s - loss: 0.7211 - accuracy: 0.67 - ETA: 0s - loss: 0.7200 - accuracy: 0.67 - ETA: 0s - loss: 0.7185 - accuracy: 0.68 - 1s 49us/step - loss: 0.7187 - accuracy: 0.6810 - val_loss: 0.8018 - val_accuracy: 0.6277\n",
      "Epoch 39/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.66 - ETA: 0s - loss: 0.7109 - accuracy: 0.68 - ETA: 0s - loss: 0.7132 - accuracy: 0.68 - ETA: 0s - loss: 0.7183 - accuracy: 0.67 - ETA: 0s - loss: 0.7140 - accuracy: 0.68 - ETA: 0s - loss: 0.7116 - accuracy: 0.68 - ETA: 0s - loss: 0.7105 - accuracy: 0.68 - ETA: 0s - loss: 0.7110 - accuracy: 0.68 - 1s 49us/step - loss: 0.7120 - accuracy: 0.6812 - val_loss: 0.7962 - val_accuracy: 0.6307\n",
      "Epoch 40/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7236 - accuracy: 0.65 - ETA: 0s - loss: 0.7057 - accuracy: 0.67 - ETA: 0s - loss: 0.7087 - accuracy: 0.67 - ETA: 0s - loss: 0.7105 - accuracy: 0.67 - ETA: 0s - loss: 0.7099 - accuracy: 0.67 - ETA: 0s - loss: 0.7073 - accuracy: 0.68 - ETA: 0s - loss: 0.7071 - accuracy: 0.68 - ETA: 0s - loss: 0.7057 - accuracy: 0.68 - 1s 46us/step - loss: 0.7053 - accuracy: 0.6831 - val_loss: 0.7922 - val_accuracy: 0.6315\n",
      "Epoch 41/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.69 - ETA: 0s - loss: 0.7081 - accuracy: 0.67 - ETA: 0s - loss: 0.6998 - accuracy: 0.68 - ETA: 0s - loss: 0.7000 - accuracy: 0.68 - ETA: 0s - loss: 0.6993 - accuracy: 0.68 - ETA: 0s - loss: 0.6977 - accuracy: 0.68 - ETA: 0s - loss: 0.6969 - accuracy: 0.68 - ETA: 0s - loss: 0.6993 - accuracy: 0.68 - 1s 46us/step - loss: 0.6990 - accuracy: 0.6845 - val_loss: 0.7886 - val_accuracy: 0.6300\n",
      "Epoch 42/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7128 - accuracy: 0.66 - ETA: 0s - loss: 0.6925 - accuracy: 0.68 - ETA: 0s - loss: 0.6959 - accuracy: 0.68 - ETA: 0s - loss: 0.6932 - accuracy: 0.68 - ETA: 0s - loss: 0.6930 - accuracy: 0.68 - ETA: 0s - loss: 0.6930 - accuracy: 0.68 - ETA: 0s - loss: 0.6933 - accuracy: 0.68 - ETA: 0s - loss: 0.6936 - accuracy: 0.68 - ETA: 0s - loss: 0.6928 - accuracy: 0.68 - ETA: 0s - loss: 0.6928 - accuracy: 0.68 - ETA: 0s - loss: 0.6932 - accuracy: 0.68 - 1s 52us/step - loss: 0.6929 - accuracy: 0.6856 - val_loss: 0.7847 - val_accuracy: 0.6302\n",
      "Epoch 43/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.67 - ETA: 0s - loss: 0.6791 - accuracy: 0.69 - ETA: 0s - loss: 0.6885 - accuracy: 0.68 - ETA: 0s - loss: 0.6901 - accuracy: 0.68 - ETA: 0s - loss: 0.6900 - accuracy: 0.68 - ETA: 0s - loss: 0.6896 - accuracy: 0.68 - ETA: 0s - loss: 0.6878 - accuracy: 0.68 - ETA: 0s - loss: 0.6871 - accuracy: 0.68 - 1s 47us/step - loss: 0.6869 - accuracy: 0.6859 - val_loss: 0.7807 - val_accuracy: 0.6325\n",
      "Epoch 44/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.69 - ETA: 0s - loss: 0.6893 - accuracy: 0.68 - ETA: 0s - loss: 0.6902 - accuracy: 0.68 - ETA: 0s - loss: 0.6857 - accuracy: 0.68 - ETA: 0s - loss: 0.6839 - accuracy: 0.68 - ETA: 0s - loss: 0.6826 - accuracy: 0.68 - ETA: 0s - loss: 0.6827 - accuracy: 0.68 - ETA: 0s - loss: 0.6814 - accuracy: 0.68 - 1s 47us/step - loss: 0.6813 - accuracy: 0.6872 - val_loss: 0.7779 - val_accuracy: 0.6310\n",
      "Epoch 45/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6749 - accuracy: 0.69 - ETA: 0s - loss: 0.6784 - accuracy: 0.68 - ETA: 0s - loss: 0.6776 - accuracy: 0.68 - ETA: 0s - loss: 0.6772 - accuracy: 0.68 - ETA: 0s - loss: 0.6767 - accuracy: 0.68 - ETA: 0s - loss: 0.6756 - accuracy: 0.68 - ETA: 0s - loss: 0.6752 - accuracy: 0.68 - ETA: 0s - loss: 0.6753 - accuracy: 0.68 - 1s 47us/step - loss: 0.6753 - accuracy: 0.6884 - val_loss: 0.7742 - val_accuracy: 0.6330\n",
      "Epoch 46/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6745 - accuracy: 0.67 - ETA: 0s - loss: 0.6700 - accuracy: 0.68 - ETA: 0s - loss: 0.6695 - accuracy: 0.69 - ETA: 0s - loss: 0.6680 - accuracy: 0.69 - ETA: 0s - loss: 0.6667 - accuracy: 0.69 - ETA: 0s - loss: 0.6693 - accuracy: 0.69 - ETA: 0s - loss: 0.6683 - accuracy: 0.69 - ETA: 0s - loss: 0.6696 - accuracy: 0.69 - 1s 47us/step - loss: 0.6699 - accuracy: 0.6898 - val_loss: 0.7715 - val_accuracy: 0.6325\n",
      "Epoch 47/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6581 - accuracy: 0.70 - ETA: 0s - loss: 0.6561 - accuracy: 0.70 - ETA: 0s - loss: 0.6575 - accuracy: 0.70 - ETA: 0s - loss: 0.6591 - accuracy: 0.69 - ETA: 0s - loss: 0.6621 - accuracy: 0.69 - ETA: 0s - loss: 0.6637 - accuracy: 0.69 - ETA: 0s - loss: 0.6640 - accuracy: 0.69 - ETA: 0s - loss: 0.6645 - accuracy: 0.69 - 1s 47us/step - loss: 0.6649 - accuracy: 0.6904 - val_loss: 0.7697 - val_accuracy: 0.6313\n",
      "Epoch 48/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6566 - accuracy: 0.67 - ETA: 0s - loss: 0.6552 - accuracy: 0.68 - ETA: 0s - loss: 0.6531 - accuracy: 0.69 - ETA: 0s - loss: 0.6562 - accuracy: 0.69 - ETA: 0s - loss: 0.6555 - accuracy: 0.69 - ETA: 0s - loss: 0.6575 - accuracy: 0.69 - ETA: 0s - loss: 0.6574 - accuracy: 0.69 - ETA: 0s - loss: 0.6602 - accuracy: 0.69 - 1s 47us/step - loss: 0.6598 - accuracy: 0.6910 - val_loss: 0.7667 - val_accuracy: 0.6313\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6609 - accuracy: 0.68 - ETA: 0s - loss: 0.6590 - accuracy: 0.69 - ETA: 0s - loss: 0.6593 - accuracy: 0.69 - ETA: 0s - loss: 0.6585 - accuracy: 0.69 - ETA: 0s - loss: 0.6562 - accuracy: 0.69 - ETA: 0s - loss: 0.6548 - accuracy: 0.69 - ETA: 0s - loss: 0.6536 - accuracy: 0.69 - ETA: 0s - loss: 0.6534 - accuracy: 0.69 - ETA: 0s - loss: 0.6544 - accuracy: 0.69 - 1s 48us/step - loss: 0.6548 - accuracy: 0.6923 - val_loss: 0.7630 - val_accuracy: 0.6338\n",
      "Epoch 50/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.70 - ETA: 0s - loss: 0.6465 - accuracy: 0.69 - ETA: 0s - loss: 0.6439 - accuracy: 0.69 - ETA: 0s - loss: 0.6497 - accuracy: 0.69 - ETA: 0s - loss: 0.6495 - accuracy: 0.69 - ETA: 0s - loss: 0.6494 - accuracy: 0.69 - ETA: 0s - loss: 0.6465 - accuracy: 0.70 - ETA: 0s - loss: 0.6494 - accuracy: 0.69 - ETA: 0s - loss: 0.6512 - accuracy: 0.69 - ETA: 0s - loss: 0.6533 - accuracy: 0.69 - ETA: 0s - loss: 0.6519 - accuracy: 0.69 - ETA: 0s - loss: 0.6507 - accuracy: 0.69 - ETA: 0s - loss: 0.6497 - accuracy: 0.69 - 1s 54us/step - loss: 0.6501 - accuracy: 0.6925 - val_loss: 0.7593 - val_accuracy: 0.6320\n",
      "Epoch 51/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6443 - accuracy: 0.67 - ETA: 0s - loss: 0.6425 - accuracy: 0.68 - ETA: 0s - loss: 0.6436 - accuracy: 0.69 - ETA: 0s - loss: 0.6449 - accuracy: 0.69 - ETA: 0s - loss: 0.6444 - accuracy: 0.69 - ETA: 0s - loss: 0.6458 - accuracy: 0.69 - ETA: 0s - loss: 0.6468 - accuracy: 0.69 - ETA: 0s - loss: 0.6463 - accuracy: 0.69 - 1s 48us/step - loss: 0.6454 - accuracy: 0.6942 - val_loss: 0.7565 - val_accuracy: 0.6341\n",
      "Epoch 52/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6375 - accuracy: 0.69 - ETA: 0s - loss: 0.6454 - accuracy: 0.69 - ETA: 0s - loss: 0.6438 - accuracy: 0.69 - ETA: 0s - loss: 0.6416 - accuracy: 0.69 - ETA: 0s - loss: 0.6395 - accuracy: 0.69 - ETA: 0s - loss: 0.6396 - accuracy: 0.69 - ETA: 0s - loss: 0.6395 - accuracy: 0.69 - ETA: 0s - loss: 0.6400 - accuracy: 0.69 - 1s 46us/step - loss: 0.6408 - accuracy: 0.6942 - val_loss: 0.7561 - val_accuracy: 0.6320\n",
      "Epoch 53/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6494 - accuracy: 0.68 - ETA: 0s - loss: 0.6399 - accuracy: 0.69 - ETA: 0s - loss: 0.6464 - accuracy: 0.68 - ETA: 0s - loss: 0.6422 - accuracy: 0.68 - ETA: 0s - loss: 0.6426 - accuracy: 0.68 - ETA: 0s - loss: 0.6403 - accuracy: 0.69 - ETA: 0s - loss: 0.6363 - accuracy: 0.69 - ETA: 0s - loss: 0.6371 - accuracy: 0.69 - 1s 46us/step - loss: 0.6368 - accuracy: 0.6946 - val_loss: 0.7530 - val_accuracy: 0.6338\n",
      "Epoch 54/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.70 - ETA: 0s - loss: 0.6346 - accuracy: 0.69 - ETA: 0s - loss: 0.6331 - accuracy: 0.69 - ETA: 0s - loss: 0.6322 - accuracy: 0.69 - ETA: 0s - loss: 0.6317 - accuracy: 0.69 - ETA: 0s - loss: 0.6323 - accuracy: 0.69 - ETA: 0s - loss: 0.6315 - accuracy: 0.69 - ETA: 0s - loss: 0.6321 - accuracy: 0.69 - 1s 46us/step - loss: 0.6326 - accuracy: 0.6961 - val_loss: 0.7518 - val_accuracy: 0.6320\n",
      "Epoch 55/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.69 - ETA: 0s - loss: 0.6268 - accuracy: 0.70 - ETA: 0s - loss: 0.6258 - accuracy: 0.70 - ETA: 0s - loss: 0.6288 - accuracy: 0.69 - ETA: 0s - loss: 0.6277 - accuracy: 0.70 - ETA: 0s - loss: 0.6289 - accuracy: 0.69 - ETA: 0s - loss: 0.6280 - accuracy: 0.69 - ETA: 0s - loss: 0.6281 - accuracy: 0.69 - ETA: 0s - loss: 0.6283 - accuracy: 0.69 - ETA: 0s - loss: 0.6292 - accuracy: 0.69 - ETA: 0s - loss: 0.6287 - accuracy: 0.69 - 1s 52us/step - loss: 0.6284 - accuracy: 0.6966 - val_loss: 0.7478 - val_accuracy: 0.6346\n",
      "Epoch 56/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6161 - accuracy: 0.71 - ETA: 0s - loss: 0.6113 - accuracy: 0.71 - ETA: 0s - loss: 0.6189 - accuracy: 0.70 - ETA: 0s - loss: 0.6204 - accuracy: 0.70 - ETA: 0s - loss: 0.6252 - accuracy: 0.69 - ETA: 0s - loss: 0.6260 - accuracy: 0.69 - ETA: 0s - loss: 0.6260 - accuracy: 0.69 - ETA: 0s - loss: 0.6255 - accuracy: 0.69 - ETA: 0s - loss: 0.6265 - accuracy: 0.69 - ETA: 0s - loss: 0.6255 - accuracy: 0.69 - ETA: 0s - loss: 0.6249 - accuracy: 0.69 - 1s 52us/step - loss: 0.6245 - accuracy: 0.6966 - val_loss: 0.7452 - val_accuracy: 0.6351\n",
      "Epoch 57/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6262 - accuracy: 0.71 - ETA: 0s - loss: 0.6307 - accuracy: 0.69 - ETA: 0s - loss: 0.6296 - accuracy: 0.69 - ETA: 0s - loss: 0.6278 - accuracy: 0.69 - ETA: 0s - loss: 0.6309 - accuracy: 0.69 - ETA: 0s - loss: 0.6283 - accuracy: 0.69 - ETA: 0s - loss: 0.6240 - accuracy: 0.69 - ETA: 0s - loss: 0.6242 - accuracy: 0.69 - ETA: 0s - loss: 0.6231 - accuracy: 0.69 - ETA: 0s - loss: 0.6231 - accuracy: 0.69 - ETA: 0s - loss: 0.6245 - accuracy: 0.69 - ETA: 0s - loss: 0.6233 - accuracy: 0.69 - ETA: 0s - loss: 0.6212 - accuracy: 0.69 - 1s 56us/step - loss: 0.6207 - accuracy: 0.6968 - val_loss: 0.7432 - val_accuracy: 0.6351\n",
      "Epoch 58/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.70 - ETA: 0s - loss: 0.6277 - accuracy: 0.69 - ETA: 0s - loss: 0.6188 - accuracy: 0.69 - ETA: 0s - loss: 0.6184 - accuracy: 0.69 - ETA: 0s - loss: 0.6172 - accuracy: 0.69 - ETA: 0s - loss: 0.6170 - accuracy: 0.69 - ETA: 0s - loss: 0.6169 - accuracy: 0.69 - ETA: 0s - loss: 0.6174 - accuracy: 0.69 - ETA: 0s - loss: 0.6156 - accuracy: 0.69 - ETA: 0s - loss: 0.6158 - accuracy: 0.69 - ETA: 0s - loss: 0.6166 - accuracy: 0.69 - ETA: 0s - loss: 0.6165 - accuracy: 0.69 - 1s 54us/step - loss: 0.6169 - accuracy: 0.6980 - val_loss: 0.7414 - val_accuracy: 0.6351\n",
      "Epoch 59/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.70 - ETA: 0s - loss: 0.6107 - accuracy: 0.69 - ETA: 0s - loss: 0.6124 - accuracy: 0.69 - ETA: 0s - loss: 0.6096 - accuracy: 0.69 - ETA: 0s - loss: 0.6127 - accuracy: 0.69 - ETA: 0s - loss: 0.6144 - accuracy: 0.69 - ETA: 0s - loss: 0.6153 - accuracy: 0.69 - ETA: 0s - loss: 0.6137 - accuracy: 0.69 - ETA: 0s - loss: 0.6134 - accuracy: 0.69 - 1s 50us/step - loss: 0.6133 - accuracy: 0.6988 - val_loss: 0.7395 - val_accuracy: 0.6348\n",
      "Epoch 60/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6052 - accuracy: 0.69 - ETA: 0s - loss: 0.6136 - accuracy: 0.68 - ETA: 0s - loss: 0.6116 - accuracy: 0.69 - ETA: 0s - loss: 0.6105 - accuracy: 0.69 - ETA: 0s - loss: 0.6099 - accuracy: 0.69 - ETA: 0s - loss: 0.6116 - accuracy: 0.69 - ETA: 0s - loss: 0.6157 - accuracy: 0.69 - ETA: 0s - loss: 0.6146 - accuracy: 0.69 - ETA: 0s - loss: 0.6133 - accuracy: 0.69 - ETA: 0s - loss: 0.6115 - accuracy: 0.69 - 1s 51us/step - loss: 0.6101 - accuracy: 0.6987 - val_loss: 0.7375 - val_accuracy: 0.6348\n",
      "Epoch 61/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.71 - ETA: 0s - loss: 0.6101 - accuracy: 0.70 - ETA: 0s - loss: 0.6137 - accuracy: 0.69 - ETA: 0s - loss: 0.6130 - accuracy: 0.69 - ETA: 0s - loss: 0.6130 - accuracy: 0.69 - ETA: 0s - loss: 0.6101 - accuracy: 0.69 - ETA: 0s - loss: 0.6092 - accuracy: 0.69 - ETA: 0s - loss: 0.6083 - accuracy: 0.69 - 1s 49us/step - loss: 0.6072 - accuracy: 0.6991 - val_loss: 0.7360 - val_accuracy: 0.6353\n",
      "Epoch 62/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.68 - ETA: 0s - loss: 0.6133 - accuracy: 0.68 - ETA: 0s - loss: 0.6065 - accuracy: 0.69 - ETA: 0s - loss: 0.6041 - accuracy: 0.70 - ETA: 0s - loss: 0.6046 - accuracy: 0.70 - ETA: 0s - loss: 0.6041 - accuracy: 0.69 - ETA: 0s - loss: 0.6027 - accuracy: 0.70 - ETA: 0s - loss: 0.6029 - accuracy: 0.69 - 1s 48us/step - loss: 0.6032 - accuracy: 0.6997 - val_loss: 0.7348 - val_accuracy: 0.6361\n",
      "Epoch 63/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6181 - accuracy: 0.67 - ETA: 0s - loss: 0.6019 - accuracy: 0.70 - ETA: 0s - loss: 0.5989 - accuracy: 0.70 - ETA: 0s - loss: 0.5984 - accuracy: 0.70 - ETA: 0s - loss: 0.5994 - accuracy: 0.70 - ETA: 0s - loss: 0.6006 - accuracy: 0.69 - ETA: 0s - loss: 0.6004 - accuracy: 0.70 - ETA: 0s - loss: 0.6005 - accuracy: 0.69 - 1s 49us/step - loss: 0.6000 - accuracy: 0.7001 - val_loss: 0.7317 - val_accuracy: 0.6371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6123 - accuracy: 0.68 - ETA: 0s - loss: 0.6132 - accuracy: 0.68 - ETA: 0s - loss: 0.6106 - accuracy: 0.69 - ETA: 0s - loss: 0.6072 - accuracy: 0.69 - ETA: 0s - loss: 0.6055 - accuracy: 0.69 - ETA: 0s - loss: 0.6035 - accuracy: 0.69 - ETA: 0s - loss: 0.5998 - accuracy: 0.70 - ETA: 0s - loss: 0.5981 - accuracy: 0.70 - ETA: 0s - loss: 0.5985 - accuracy: 0.69 - ETA: 0s - loss: 0.5981 - accuracy: 0.69 - 1s 50us/step - loss: 0.5972 - accuracy: 0.7007 - val_loss: 0.7310 - val_accuracy: 0.6361\n",
      "Epoch 65/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5884 - accuracy: 0.70 - ETA: 0s - loss: 0.5966 - accuracy: 0.70 - ETA: 0s - loss: 0.5911 - accuracy: 0.70 - ETA: 0s - loss: 0.5918 - accuracy: 0.70 - ETA: 0s - loss: 0.5925 - accuracy: 0.70 - ETA: 0s - loss: 0.5942 - accuracy: 0.70 - ETA: 0s - loss: 0.5948 - accuracy: 0.70 - ETA: 0s - loss: 0.5937 - accuracy: 0.70 - 1s 46us/step - loss: 0.5940 - accuracy: 0.7010 - val_loss: 0.7293 - val_accuracy: 0.6359\n",
      "Epoch 66/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.64 - ETA: 0s - loss: 0.6067 - accuracy: 0.68 - ETA: 0s - loss: 0.5979 - accuracy: 0.69 - ETA: 0s - loss: 0.5952 - accuracy: 0.69 - ETA: 0s - loss: 0.5914 - accuracy: 0.69 - ETA: 0s - loss: 0.5908 - accuracy: 0.69 - ETA: 0s - loss: 0.5891 - accuracy: 0.70 - ETA: 0s - loss: 0.5904 - accuracy: 0.70 - 1s 48us/step - loss: 0.5910 - accuracy: 0.7018 - val_loss: 0.7288 - val_accuracy: 0.6359\n",
      "Epoch 67/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.67 - ETA: 0s - loss: 0.5968 - accuracy: 0.69 - ETA: 0s - loss: 0.5937 - accuracy: 0.69 - ETA: 0s - loss: 0.5932 - accuracy: 0.69 - ETA: 0s - loss: 0.5918 - accuracy: 0.69 - ETA: 0s - loss: 0.5895 - accuracy: 0.70 - ETA: 0s - loss: 0.5901 - accuracy: 0.70 - ETA: 0s - loss: 0.5874 - accuracy: 0.70 - 1s 47us/step - loss: 0.5881 - accuracy: 0.7020 - val_loss: 0.7282 - val_accuracy: 0.6359\n",
      "Epoch 68/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.69 - ETA: 0s - loss: 0.5885 - accuracy: 0.69 - ETA: 0s - loss: 0.5891 - accuracy: 0.69 - ETA: 0s - loss: 0.5875 - accuracy: 0.70 - ETA: 0s - loss: 0.5860 - accuracy: 0.70 - ETA: 0s - loss: 0.5867 - accuracy: 0.70 - ETA: 0s - loss: 0.5840 - accuracy: 0.70 - ETA: 0s - loss: 0.5845 - accuracy: 0.70 - 1s 47us/step - loss: 0.5854 - accuracy: 0.7022 - val_loss: 0.7256 - val_accuracy: 0.6359\n",
      "Epoch 69/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.71 - ETA: 0s - loss: 0.5786 - accuracy: 0.71 - ETA: 0s - loss: 0.5850 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.70 - ETA: 0s - loss: 0.5829 - accuracy: 0.70 - ETA: 0s - loss: 0.5828 - accuracy: 0.70 - ETA: 0s - loss: 0.5833 - accuracy: 0.70 - ETA: 0s - loss: 0.5823 - accuracy: 0.70 - 1s 47us/step - loss: 0.5828 - accuracy: 0.7025 - val_loss: 0.7256 - val_accuracy: 0.6353\n",
      "Epoch 70/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.70 - ETA: 0s - loss: 0.5838 - accuracy: 0.70 - ETA: 0s - loss: 0.5766 - accuracy: 0.71 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.70 - 1s 46us/step - loss: 0.5801 - accuracy: 0.7032 - val_loss: 0.7241 - val_accuracy: 0.6359\n",
      "Epoch 71/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.72 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.70 - ETA: 0s - loss: 0.5784 - accuracy: 0.70 - 1s 46us/step - loss: 0.5778 - accuracy: 0.7034 - val_loss: 0.7251 - val_accuracy: 0.6343\n",
      "Epoch 72/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.70 - 1s 47us/step - loss: 0.5755 - accuracy: 0.7038 - val_loss: 0.7231 - val_accuracy: 0.6348\n",
      "Epoch 73/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.71 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - 1s 47us/step - loss: 0.5730 - accuracy: 0.7037 - val_loss: 0.7207 - val_accuracy: 0.6353\n",
      "Epoch 74/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.68 - ETA: 0s - loss: 0.5745 - accuracy: 0.69 - ETA: 0s - loss: 0.5755 - accuracy: 0.69 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - 1s 49us/step - loss: 0.5705 - accuracy: 0.7041 - val_loss: 0.7195 - val_accuracy: 0.6359\n",
      "Epoch 75/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.70 - ETA: 0s - loss: 0.5685 - accuracy: 0.69 - ETA: 0s - loss: 0.5705 - accuracy: 0.69 - ETA: 0s - loss: 0.5721 - accuracy: 0.69 - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5685 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - 1s 53us/step - loss: 0.5681 - accuracy: 0.7043 - val_loss: 0.7186 - val_accuracy: 0.6356\n",
      "Epoch 76/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.69 - ETA: 0s - loss: 0.5655 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5663 - accuracy: 0.70 - ETA: 0s - loss: 0.5669 - accuracy: 0.70 - ETA: 0s - loss: 0.5661 - accuracy: 0.70 - ETA: 0s - loss: 0.5659 - accuracy: 0.70 - 1s 49us/step - loss: 0.5658 - accuracy: 0.7048 - val_loss: 0.7166 - val_accuracy: 0.6374\n",
      "Epoch 77/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.70 - ETA: 0s - loss: 0.5594 - accuracy: 0.70 - ETA: 0s - loss: 0.5627 - accuracy: 0.70 - ETA: 0s - loss: 0.5647 - accuracy: 0.70 - ETA: 0s - loss: 0.5642 - accuracy: 0.70 - ETA: 0s - loss: 0.5628 - accuracy: 0.70 - ETA: 0s - loss: 0.5638 - accuracy: 0.70 - ETA: 0s - loss: 0.5637 - accuracy: 0.70 - 1s 47us/step - loss: 0.5637 - accuracy: 0.7049 - val_loss: 0.7163 - val_accuracy: 0.6364\n",
      "Epoch 78/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.70 - ETA: 0s - loss: 0.5614 - accuracy: 0.71 - ETA: 0s - loss: 0.5606 - accuracy: 0.70 - ETA: 0s - loss: 0.5618 - accuracy: 0.70 - ETA: 0s - loss: 0.5599 - accuracy: 0.70 - ETA: 0s - loss: 0.5623 - accuracy: 0.70 - ETA: 0s - loss: 0.5615 - accuracy: 0.70 - ETA: 0s - loss: 0.5616 - accuracy: 0.70 - 1s 51us/step - loss: 0.5616 - accuracy: 0.7055 - val_loss: 0.7149 - val_accuracy: 0.6371\n",
      "Epoch 79/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5743 - accuracy: 0.69 - ETA: 0s - loss: 0.5718 - accuracy: 0.69 - ETA: 0s - loss: 0.5649 - accuracy: 0.70 - ETA: 0s - loss: 0.5628 - accuracy: 0.70 - ETA: 0s - loss: 0.5621 - accuracy: 0.70 - ETA: 0s - loss: 0.5598 - accuracy: 0.70 - ETA: 0s - loss: 0.5592 - accuracy: 0.70 - ETA: 0s - loss: 0.5594 - accuracy: 0.70 - ETA: 0s - loss: 0.5598 - accuracy: 0.70 - 1s 54us/step - loss: 0.5596 - accuracy: 0.7057 - val_loss: 0.7138 - val_accuracy: 0.6369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5677 - accuracy: 0.70 - ETA: 0s - loss: 0.5576 - accuracy: 0.71 - ETA: 0s - loss: 0.5584 - accuracy: 0.71 - ETA: 0s - loss: 0.5576 - accuracy: 0.71 - ETA: 0s - loss: 0.5554 - accuracy: 0.71 - ETA: 0s - loss: 0.5558 - accuracy: 0.71 - ETA: 0s - loss: 0.5560 - accuracy: 0.71 - ETA: 0s - loss: 0.5572 - accuracy: 0.71 - ETA: 0s - loss: 0.5564 - accuracy: 0.70 - ETA: 0s - loss: 0.5575 - accuracy: 0.70 - ETA: 0s - loss: 0.5577 - accuracy: 0.70 - ETA: 0s - loss: 0.5576 - accuracy: 0.70 - 1s 56us/step - loss: 0.5577 - accuracy: 0.7054 - val_loss: 0.7141 - val_accuracy: 0.6364\n",
      "Epoch 81/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.71 - ETA: 0s - loss: 0.5521 - accuracy: 0.71 - ETA: 0s - loss: 0.5566 - accuracy: 0.70 - ETA: 0s - loss: 0.5573 - accuracy: 0.70 - ETA: 0s - loss: 0.5575 - accuracy: 0.70 - ETA: 0s - loss: 0.5580 - accuracy: 0.70 - ETA: 0s - loss: 0.5567 - accuracy: 0.70 - ETA: 0s - loss: 0.5560 - accuracy: 0.70 - 1s 49us/step - loss: 0.5556 - accuracy: 0.7061 - val_loss: 0.7133 - val_accuracy: 0.6361\n",
      "Epoch 82/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.69 - ETA: 0s - loss: 0.5599 - accuracy: 0.69 - ETA: 0s - loss: 0.5536 - accuracy: 0.70 - ETA: 0s - loss: 0.5543 - accuracy: 0.70 - ETA: 0s - loss: 0.5566 - accuracy: 0.70 - ETA: 0s - loss: 0.5582 - accuracy: 0.70 - ETA: 0s - loss: 0.5561 - accuracy: 0.70 - ETA: 0s - loss: 0.5553 - accuracy: 0.70 - 1s 47us/step - loss: 0.5537 - accuracy: 0.7064 - val_loss: 0.7135 - val_accuracy: 0.6359\n",
      "Epoch 83/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.69 - ETA: 0s - loss: 0.5553 - accuracy: 0.71 - ETA: 0s - loss: 0.5590 - accuracy: 0.70 - ETA: 0s - loss: 0.5566 - accuracy: 0.70 - ETA: 0s - loss: 0.5520 - accuracy: 0.70 - ETA: 0s - loss: 0.5514 - accuracy: 0.70 - ETA: 0s - loss: 0.5521 - accuracy: 0.70 - ETA: 0s - loss: 0.5515 - accuracy: 0.70 - 1s 46us/step - loss: 0.5518 - accuracy: 0.7065 - val_loss: 0.7115 - val_accuracy: 0.6361\n",
      "Epoch 84/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.69 - ETA: 0s - loss: 0.5513 - accuracy: 0.70 - ETA: 0s - loss: 0.5478 - accuracy: 0.71 - ETA: 0s - loss: 0.5485 - accuracy: 0.70 - ETA: 0s - loss: 0.5483 - accuracy: 0.70 - ETA: 0s - loss: 0.5504 - accuracy: 0.70 - ETA: 0s - loss: 0.5481 - accuracy: 0.71 - ETA: 0s - loss: 0.5485 - accuracy: 0.70 - 1s 48us/step - loss: 0.5500 - accuracy: 0.7066 - val_loss: 0.7112 - val_accuracy: 0.6364\n",
      "Epoch 85/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.71 - ETA: 0s - loss: 0.5407 - accuracy: 0.71 - ETA: 0s - loss: 0.5393 - accuracy: 0.71 - ETA: 0s - loss: 0.5448 - accuracy: 0.70 - ETA: 0s - loss: 0.5459 - accuracy: 0.70 - ETA: 0s - loss: 0.5458 - accuracy: 0.70 - ETA: 0s - loss: 0.5478 - accuracy: 0.70 - ETA: 0s - loss: 0.5490 - accuracy: 0.70 - ETA: 0s - loss: 0.5485 - accuracy: 0.70 - ETA: 0s - loss: 0.5487 - accuracy: 0.70 - 1s 50us/step - loss: 0.5482 - accuracy: 0.7066 - val_loss: 0.7103 - val_accuracy: 0.6364\n",
      "Epoch 86/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.71 - ETA: 0s - loss: 0.5413 - accuracy: 0.71 - ETA: 0s - loss: 0.5430 - accuracy: 0.71 - ETA: 0s - loss: 0.5449 - accuracy: 0.70 - ETA: 0s - loss: 0.5455 - accuracy: 0.70 - ETA: 0s - loss: 0.5449 - accuracy: 0.71 - ETA: 0s - loss: 0.5462 - accuracy: 0.70 - ETA: 0s - loss: 0.5451 - accuracy: 0.71 - ETA: 0s - loss: 0.5457 - accuracy: 0.70 - ETA: 0s - loss: 0.5458 - accuracy: 0.70 - 1s 49us/step - loss: 0.5464 - accuracy: 0.7070 - val_loss: 0.7105 - val_accuracy: 0.6356\n",
      "Epoch 87/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5535 - accuracy: 0.70 - ETA: 0s - loss: 0.5486 - accuracy: 0.70 - ETA: 0s - loss: 0.5452 - accuracy: 0.71 - ETA: 0s - loss: 0.5472 - accuracy: 0.70 - ETA: 0s - loss: 0.5460 - accuracy: 0.70 - ETA: 0s - loss: 0.5448 - accuracy: 0.70 - ETA: 0s - loss: 0.5454 - accuracy: 0.70 - ETA: 0s - loss: 0.5440 - accuracy: 0.70 - ETA: 0s - loss: 0.5459 - accuracy: 0.70 - ETA: 0s - loss: 0.5454 - accuracy: 0.70 - 1s 49us/step - loss: 0.5447 - accuracy: 0.7072 - val_loss: 0.7090 - val_accuracy: 0.6366\n",
      "Epoch 88/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.71 - ETA: 0s - loss: 0.5402 - accuracy: 0.71 - ETA: 0s - loss: 0.5428 - accuracy: 0.71 - ETA: 0s - loss: 0.5412 - accuracy: 0.71 - ETA: 0s - loss: 0.5407 - accuracy: 0.71 - ETA: 0s - loss: 0.5434 - accuracy: 0.70 - ETA: 0s - loss: 0.5438 - accuracy: 0.70 - ETA: 0s - loss: 0.5432 - accuracy: 0.70 - 1s 47us/step - loss: 0.5430 - accuracy: 0.7071 - val_loss: 0.7088 - val_accuracy: 0.6369\n",
      "Epoch 89/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5390 - accuracy: 0.70 - ETA: 0s - loss: 0.5382 - accuracy: 0.71 - ETA: 0s - loss: 0.5432 - accuracy: 0.70 - ETA: 0s - loss: 0.5425 - accuracy: 0.70 - ETA: 0s - loss: 0.5426 - accuracy: 0.70 - ETA: 0s - loss: 0.5391 - accuracy: 0.70 - ETA: 0s - loss: 0.5413 - accuracy: 0.70 - ETA: 0s - loss: 0.5410 - accuracy: 0.70 - 1s 48us/step - loss: 0.5413 - accuracy: 0.7074 - val_loss: 0.7081 - val_accuracy: 0.6366\n",
      "Epoch 90/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.71 - ETA: 0s - loss: 0.5397 - accuracy: 0.71 - ETA: 0s - loss: 0.5432 - accuracy: 0.70 - ETA: 0s - loss: 0.5406 - accuracy: 0.71 - ETA: 0s - loss: 0.5389 - accuracy: 0.71 - ETA: 0s - loss: 0.5379 - accuracy: 0.71 - ETA: 0s - loss: 0.5393 - accuracy: 0.70 - ETA: 0s - loss: 0.5394 - accuracy: 0.70 - 1s 47us/step - loss: 0.5397 - accuracy: 0.7077 - val_loss: 0.7074 - val_accuracy: 0.6384\n",
      "Epoch 91/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.70 - ETA: 0s - loss: 0.5427 - accuracy: 0.70 - ETA: 0s - loss: 0.5391 - accuracy: 0.70 - ETA: 0s - loss: 0.5369 - accuracy: 0.71 - ETA: 0s - loss: 0.5361 - accuracy: 0.71 - ETA: 0s - loss: 0.5359 - accuracy: 0.70 - ETA: 0s - loss: 0.5372 - accuracy: 0.70 - ETA: 0s - loss: 0.5378 - accuracy: 0.70 - 1s 47us/step - loss: 0.5381 - accuracy: 0.7076 - val_loss: 0.7074 - val_accuracy: 0.6366\n",
      "Epoch 92/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.69 - ETA: 0s - loss: 0.5287 - accuracy: 0.71 - ETA: 0s - loss: 0.5290 - accuracy: 0.71 - ETA: 0s - loss: 0.5310 - accuracy: 0.71 - ETA: 0s - loss: 0.5347 - accuracy: 0.71 - ETA: 0s - loss: 0.5356 - accuracy: 0.70 - ETA: 0s - loss: 0.5388 - accuracy: 0.70 - ETA: 0s - loss: 0.5370 - accuracy: 0.70 - 1s 47us/step - loss: 0.5364 - accuracy: 0.7078 - val_loss: 0.7076 - val_accuracy: 0.6371\n",
      "Epoch 93/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.68 - ETA: 0s - loss: 0.5391 - accuracy: 0.69 - ETA: 0s - loss: 0.5346 - accuracy: 0.70 - ETA: 0s - loss: 0.5381 - accuracy: 0.70 - ETA: 0s - loss: 0.5358 - accuracy: 0.70 - ETA: 0s - loss: 0.5357 - accuracy: 0.70 - ETA: 0s - loss: 0.5349 - accuracy: 0.70 - ETA: 0s - loss: 0.5352 - accuracy: 0.70 - 1s 47us/step - loss: 0.5349 - accuracy: 0.7081 - val_loss: 0.7075 - val_accuracy: 0.6374\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=1000, validation_data=(x_test, y_test), epochs=200,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model1.add(GlobalAveragePooling1D())\n",
    "model1.add(Dense(4, activation='softmax'))\n",
    "model1.add(Dense(3, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/1000\n",
      "15663/15663 [==============================] - ETA: 3s - loss: 0.0362 - accuracy: 0.99 - ETA: 3s - loss: 0.0324 - accuracy: 0.99 - ETA: 3s - loss: 0.0341 - accuracy: 0.99 - ETA: 3s - loss: 0.0332 - accuracy: 0.99 - ETA: 3s - loss: 0.0310 - accuracy: 0.99 - ETA: 3s - loss: 0.0309 - accuracy: 0.99 - ETA: 3s - loss: 0.0304 - accuracy: 0.99 - ETA: 3s - loss: 0.0313 - accuracy: 0.99 - ETA: 3s - loss: 0.0351 - accuracy: 0.99 - ETA: 3s - loss: 0.0365 - accuracy: 0.99 - ETA: 3s - loss: 0.0362 - accuracy: 0.99 - ETA: 3s - loss: 0.0355 - accuracy: 0.99 - ETA: 3s - loss: 0.0353 - accuracy: 0.99 - ETA: 3s - loss: 0.0347 - accuracy: 0.99 - ETA: 3s - loss: 0.0347 - accuracy: 0.99 - ETA: 3s - loss: 0.0341 - accuracy: 0.99 - ETA: 3s - loss: 0.0342 - accuracy: 0.99 - ETA: 2s - loss: 0.0342 - accuracy: 0.99 - ETA: 2s - loss: 0.0338 - accuracy: 0.99 - ETA: 2s - loss: 0.0342 - accuracy: 0.99 - ETA: 2s - loss: 0.0336 - accuracy: 0.99 - ETA: 2s - loss: 0.0337 - accuracy: 0.99 - ETA: 2s - loss: 0.0344 - accuracy: 0.99 - ETA: 2s - loss: 0.0343 - accuracy: 0.99 - ETA: 2s - loss: 0.0344 - accuracy: 0.99 - ETA: 2s - loss: 0.0344 - accuracy: 0.99 - ETA: 2s - loss: 0.0345 - accuracy: 0.99 - ETA: 2s - loss: 0.0344 - accuracy: 0.99 - ETA: 2s - loss: 0.0339 - accuracy: 0.99 - ETA: 2s - loss: 0.0336 - accuracy: 0.99 - ETA: 2s - loss: 0.0336 - accuracy: 0.99 - ETA: 2s - loss: 0.0342 - accuracy: 0.99 - ETA: 2s - loss: 0.0341 - accuracy: 0.99 - ETA: 1s - loss: 0.0341 - accuracy: 0.99 - ETA: 1s - loss: 0.0343 - accuracy: 0.99 - ETA: 1s - loss: 0.0340 - accuracy: 0.99 - ETA: 1s - loss: 0.0343 - accuracy: 0.99 - ETA: 1s - loss: 0.0341 - accuracy: 0.99 - ETA: 1s - loss: 0.0343 - accuracy: 0.99 - ETA: 1s - loss: 0.0341 - accuracy: 0.99 - ETA: 1s - loss: 0.0344 - accuracy: 0.99 - ETA: 1s - loss: 0.0348 - accuracy: 0.99 - ETA: 1s - loss: 0.0348 - accuracy: 0.99 - ETA: 1s - loss: 0.0352 - accuracy: 0.99 - ETA: 1s - loss: 0.0351 - accuracy: 0.99 - ETA: 1s - loss: 0.0352 - accuracy: 0.99 - ETA: 1s - loss: 0.0350 - accuracy: 0.99 - ETA: 1s - loss: 0.0349 - accuracy: 0.99 - ETA: 0s - loss: 0.0348 - accuracy: 0.99 - ETA: 0s - loss: 0.0352 - accuracy: 0.99 - ETA: 0s - loss: 0.0351 - accuracy: 0.99 - ETA: 0s - loss: 0.0351 - accuracy: 0.99 - ETA: 0s - loss: 0.0351 - accuracy: 0.99 - ETA: 0s - loss: 0.0350 - accuracy: 0.99 - ETA: 0s - loss: 0.0349 - accuracy: 0.99 - ETA: 0s - loss: 0.0349 - accuracy: 0.99 - ETA: 0s - loss: 0.0350 - accuracy: 0.99 - ETA: 0s - loss: 0.0349 - accuracy: 0.99 - ETA: 0s - loss: 0.0350 - accuracy: 0.99 - ETA: 0s - loss: 0.0351 - accuracy: 0.99 - ETA: 0s - loss: 0.0354 - accuracy: 0.99 - ETA: 0s - loss: 0.0354 - accuracy: 0.99 - ETA: 0s - loss: 0.0356 - accuracy: 0.99 - ETA: 0s - loss: 0.0355 - accuracy: 0.99 - ETA: 0s - loss: 0.0356 - accuracy: 0.99 - ETA: 0s - loss: 0.0356 - accuracy: 0.99 - ETA: 0s - loss: 0.0357 - accuracy: 0.99 - 4s 257us/step - loss: 0.0356 - accuracy: 0.9953 - val_loss: 0.3460 - val_accuracy: 0.8705\n",
      "Epoch 2/1000\n",
      "15663/15663 [==============================] - ETA: 3s - loss: 0.0200 - accuracy: 1.00 - ETA: 4s - loss: 0.0234 - accuracy: 1.00 - ETA: 4s - loss: 0.0216 - accuracy: 1.00 - ETA: 4s - loss: 0.0305 - accuracy: 0.99 - ETA: 4s - loss: 0.0320 - accuracy: 0.99 - ETA: 3s - loss: 0.0307 - accuracy: 0.99 - ETA: 3s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0289 - accuracy: 0.99 - ETA: 3s - loss: 0.0304 - accuracy: 0.99 - ETA: 3s - loss: 0.0319 - accuracy: 0.99 - ETA: 3s - loss: 0.0322 - accuracy: 0.99 - ETA: 3s - loss: 0.0324 - accuracy: 0.99 - ETA: 3s - loss: 0.0335 - accuracy: 0.99 - ETA: 3s - loss: 0.0337 - accuracy: 0.99 - ETA: 3s - loss: 0.0336 - accuracy: 0.99 - ETA: 3s - loss: 0.0345 - accuracy: 0.99 - ETA: 3s - loss: 0.0343 - accuracy: 0.99 - ETA: 3s - loss: 0.0347 - accuracy: 0.99 - ETA: 3s - loss: 0.0349 - accuracy: 0.99 - ETA: 3s - loss: 0.0346 - accuracy: 0.99 - ETA: 3s - loss: 0.0340 - accuracy: 0.99 - ETA: 3s - loss: 0.0339 - accuracy: 0.99 - ETA: 3s - loss: 0.0340 - accuracy: 0.99 - ETA: 3s - loss: 0.0338 - accuracy: 0.99 - ETA: 2s - loss: 0.0341 - accuracy: 0.99 - ETA: 2s - loss: 0.0340 - accuracy: 0.99 - ETA: 2s - loss: 0.0339 - accuracy: 0.99 - ETA: 2s - loss: 0.0336 - accuracy: 0.99 - ETA: 2s - loss: 0.0331 - accuracy: 0.99 - ETA: 2s - loss: 0.0328 - accuracy: 0.99 - ETA: 2s - loss: 0.0331 - accuracy: 0.99 - ETA: 2s - loss: 0.0340 - accuracy: 0.99 - ETA: 2s - loss: 0.0339 - accuracy: 0.99 - ETA: 2s - loss: 0.0338 - accuracy: 0.99 - ETA: 2s - loss: 0.0337 - accuracy: 0.99 - ETA: 2s - loss: 0.0337 - accuracy: 0.99 - ETA: 2s - loss: 0.0333 - accuracy: 0.99 - ETA: 2s - loss: 0.0333 - accuracy: 0.99 - ETA: 2s - loss: 0.0338 - accuracy: 0.99 - ETA: 2s - loss: 0.0340 - accuracy: 0.99 - ETA: 2s - loss: 0.0339 - accuracy: 0.99 - ETA: 2s - loss: 0.0338 - accuracy: 0.99 - ETA: 1s - loss: 0.0340 - accuracy: 0.99 - ETA: 1s - loss: 0.0339 - accuracy: 0.99 - ETA: 1s - loss: 0.0338 - accuracy: 0.99 - ETA: 1s - loss: 0.0337 - accuracy: 0.99 - ETA: 1s - loss: 0.0337 - accuracy: 0.99 - ETA: 1s - loss: 0.0337 - accuracy: 0.99 - ETA: 1s - loss: 0.0336 - accuracy: 0.99 - ETA: 1s - loss: 0.0337 - accuracy: 0.99 - ETA: 1s - loss: 0.0336 - accuracy: 0.99 - ETA: 1s - loss: 0.0336 - accuracy: 0.99 - ETA: 1s - loss: 0.0337 - accuracy: 0.99 - ETA: 1s - loss: 0.0338 - accuracy: 0.99 - ETA: 1s - loss: 0.0337 - accuracy: 0.99 - ETA: 1s - loss: 0.0340 - accuracy: 0.99 - ETA: 1s - loss: 0.0341 - accuracy: 0.99 - ETA: 1s - loss: 0.0342 - accuracy: 0.99 - ETA: 1s - loss: 0.0345 - accuracy: 0.99 - ETA: 1s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0343 - accuracy: 0.99 - ETA: 0s - loss: 0.0343 - accuracy: 0.99 - ETA: 0s - loss: 0.0346 - accuracy: 0.99 - ETA: 0s - loss: 0.0346 - accuracy: 0.99 - ETA: 0s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0344 - accuracy: 0.99 - ETA: 0s - loss: 0.0345 - accuracy: 0.99 - ETA: 0s - loss: 0.0344 - accuracy: 0.99 - ETA: 0s - loss: 0.0342 - accuracy: 0.99 - ETA: 0s - loss: 0.0342 - accuracy: 0.99 - ETA: 0s - loss: 0.0340 - accuracy: 0.99 - ETA: 0s - loss: 0.0339 - accuracy: 0.99 - ETA: 0s - loss: 0.0338 - accuracy: 0.99 - ETA: 0s - loss: 0.0338 - accuracy: 0.99 - 4s 277us/step - loss: 0.0336 - accuracy: 0.9959 - val_loss: 0.3454 - val_accuracy: 0.8700\n",
      "Epoch 3/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 3s - loss: 0.0177 - accuracy: 1.00 - ETA: 3s - loss: 0.0209 - accuracy: 1.00 - ETA: 3s - loss: 0.0271 - accuracy: 0.99 - ETA: 3s - loss: 0.0309 - accuracy: 0.99 - ETA: 3s - loss: 0.0282 - accuracy: 0.99 - ETA: 3s - loss: 0.0265 - accuracy: 0.99 - ETA: 3s - loss: 0.0260 - accuracy: 0.99 - ETA: 3s - loss: 0.0252 - accuracy: 0.99 - ETA: 3s - loss: 0.0262 - accuracy: 0.99 - ETA: 3s - loss: 0.0264 - accuracy: 0.99 - ETA: 3s - loss: 0.0264 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0293 - accuracy: 0.99 - ETA: 3s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0291 - accuracy: 0.99 - ETA: 3s - loss: 0.0283 - accuracy: 0.99 - ETA: 3s - loss: 0.0281 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0281 - accuracy: 0.99 - ETA: 3s - loss: 0.0280 - accuracy: 0.99 - ETA: 3s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0289 - accuracy: 0.99 - ETA: 3s - loss: 0.0305 - accuracy: 0.99 - ETA: 3s - loss: 0.0302 - accuracy: 0.99 - ETA: 3s - loss: 0.0305 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.99 - ETA: 2s - loss: 0.0320 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0320 - accuracy: 0.99 - ETA: 2s - loss: 0.0319 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0312 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0315 - accuracy: 0.99 - ETA: 2s - loss: 0.0315 - accuracy: 0.99 - ETA: 2s - loss: 0.0313 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0318 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0316 - accuracy: 0.99 - ETA: 1s - loss: 0.0315 - accuracy: 0.99 - ETA: 1s - loss: 0.0314 - accuracy: 0.99 - ETA: 1s - loss: 0.0315 - accuracy: 0.99 - ETA: 1s - loss: 0.0315 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0318 - accuracy: 0.99 - ETA: 1s - loss: 0.0319 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0317 - accuracy: 0.99 - ETA: 1s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0315 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0314 - accuracy: 0.99 - ETA: 0s - loss: 0.0313 - accuracy: 0.99 - ETA: 0s - loss: 0.0312 - accuracy: 0.99 - ETA: 0s - loss: 0.0313 - accuracy: 0.99 - ETA: 0s - loss: 0.0318 - accuracy: 0.99 - ETA: 0s - loss: 0.0317 - accuracy: 0.99 - ETA: 0s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0318 - accuracy: 0.99 - ETA: 0s - loss: 0.0317 - accuracy: 0.99 - ETA: 0s - loss: 0.0319 - accuracy: 0.99 - ETA: 0s - loss: 0.0317 - accuracy: 0.99 - ETA: 0s - loss: 0.0318 - accuracy: 0.99 - 4s 284us/step - loss: 0.0319 - accuracy: 0.9962 - val_loss: 0.3470 - val_accuracy: 0.8703\n",
      "Epoch 4/1000\n",
      "15663/15663 [==============================] - ETA: 4s - loss: 0.0450 - accuracy: 0.99 - ETA: 4s - loss: 0.0403 - accuracy: 0.99 - ETA: 4s - loss: 0.0330 - accuracy: 0.99 - ETA: 4s - loss: 0.0308 - accuracy: 0.99 - ETA: 4s - loss: 0.0294 - accuracy: 0.99 - ETA: 3s - loss: 0.0269 - accuracy: 0.99 - ETA: 3s - loss: 0.0256 - accuracy: 0.99 - ETA: 3s - loss: 0.0258 - accuracy: 0.99 - ETA: 3s - loss: 0.0270 - accuracy: 0.99 - ETA: 3s - loss: 0.0263 - accuracy: 0.99 - ETA: 3s - loss: 0.0271 - accuracy: 0.99 - ETA: 3s - loss: 0.0300 - accuracy: 0.99 - ETA: 3s - loss: 0.0305 - accuracy: 0.99 - ETA: 3s - loss: 0.0301 - accuracy: 0.99 - ETA: 3s - loss: 0.0297 - accuracy: 0.99 - ETA: 3s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0291 - accuracy: 0.99 - ETA: 3s - loss: 0.0290 - accuracy: 0.99 - ETA: 3s - loss: 0.0295 - accuracy: 0.99 - ETA: 3s - loss: 0.0296 - accuracy: 0.99 - ETA: 3s - loss: 0.0304 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0298 - accuracy: 0.99 - ETA: 2s - loss: 0.0293 - accuracy: 0.99 - ETA: 2s - loss: 0.0296 - accuracy: 0.99 - ETA: 2s - loss: 0.0301 - accuracy: 0.99 - ETA: 2s - loss: 0.0300 - accuracy: 0.99 - ETA: 2s - loss: 0.0301 - accuracy: 0.99 - ETA: 2s - loss: 0.0299 - accuracy: 0.99 - ETA: 2s - loss: 0.0297 - accuracy: 0.99 - ETA: 2s - loss: 0.0298 - accuracy: 0.99 - ETA: 2s - loss: 0.0301 - accuracy: 0.99 - ETA: 2s - loss: 0.0302 - accuracy: 0.99 - ETA: 2s - loss: 0.0306 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.99 - ETA: 2s - loss: 0.0309 - accuracy: 0.99 - ETA: 2s - loss: 0.0310 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0308 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.99 - ETA: 1s - loss: 0.0308 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0310 - accuracy: 0.99 - ETA: 1s - loss: 0.0309 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.99 - ETA: 1s - loss: 0.0309 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0314 - accuracy: 0.99 - ETA: 1s - loss: 0.0313 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0313 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0309 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0310 - accuracy: 0.99 - ETA: 0s - loss: 0.0316 - accuracy: 0.99 - ETA: 0s - loss: 0.0317 - accuracy: 0.99 - ETA: 0s - loss: 0.0315 - accuracy: 0.99 - ETA: 0s - loss: 0.0315 - accuracy: 0.99 - ETA: 0s - loss: 0.0313 - accuracy: 0.99 - ETA: 0s - loss: 0.0311 - accuracy: 0.99 - ETA: 0s - loss: 0.0310 - accuracy: 0.99 - ETA: 0s - loss: 0.0308 - accuracy: 0.99 - ETA: 0s - loss: 0.0307 - accuracy: 0.99 - ETA: 0s - loss: 0.0307 - accuracy: 0.99 - ETA: 0s - loss: 0.0304 - accuracy: 0.99 - ETA: 0s - loss: 0.0303 - accuracy: 0.99 - ETA: 0s - loss: 0.0303 - accuracy: 0.99 - ETA: 0s - loss: 0.0303 - accuracy: 0.99 - 4s 265us/step - loss: 0.0303 - accuracy: 0.9962 - val_loss: 0.3509 - val_accuracy: 0.8703\n"
     ]
    }
   ],
   "source": [
    "hist1 = model1.fit(x_train, y_train, batch_size=100, validation_data=(x_test, y_test), epochs=1000,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "model2.add(Dense(5, activation='softmax'))\n",
    "model2.add(Dense(4, activation='softmax'))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/200\n",
      "15663/15663 [==============================] - ETA: 1s - loss: 1.0949 - accuracy: 0.38 - ETA: 0s - loss: 1.0899 - accuracy: 0.40 - ETA: 0s - loss: 1.0889 - accuracy: 0.40 - ETA: 0s - loss: 1.0888 - accuracy: 0.40 - ETA: 0s - loss: 1.0893 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - 1s 51us/step - loss: 1.0880 - accuracy: 0.4051 - val_loss: 1.0900 - val_accuracy: 0.3971\n",
      "Epoch 2/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0918 - accuracy: 0.39 - ETA: 0s - loss: 1.0898 - accuracy: 0.39 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0866 - accuracy: 0.40 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - 1s 46us/step - loss: 1.0873 - accuracy: 0.4051 - val_loss: 1.0894 - val_accuracy: 0.3971\n",
      "Epoch 3/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0835 - accuracy: 0.41 - ETA: 0s - loss: 1.0824 - accuracy: 0.41 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0865 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - 1s 47us/step - loss: 1.0871 - accuracy: 0.4051 - val_loss: 1.0892 - val_accuracy: 0.3971\n",
      "Epoch 4/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0893 - accuracy: 0.39 - ETA: 0s - loss: 1.0858 - accuracy: 0.40 - ETA: 0s - loss: 1.0861 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0865 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - 1s 48us/step - loss: 1.0870 - accuracy: 0.4051 - val_loss: 1.0891 - val_accuracy: 0.3971\n",
      "Epoch 5/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0868 - accuracy: 0.41 - ETA: 0s - loss: 1.0849 - accuracy: 0.41 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - ETA: 0s - loss: 1.0860 - accuracy: 0.40 - ETA: 0s - loss: 1.0857 - accuracy: 0.40 - ETA: 0s - loss: 1.0845 - accuracy: 0.41 - ETA: 0s - loss: 1.0855 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - 1s 49us/step - loss: 1.0870 - accuracy: 0.4051 - val_loss: 1.0891 - val_accuracy: 0.3971\n",
      "Epoch 6/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0926 - accuracy: 0.38 - ETA: 0s - loss: 1.0898 - accuracy: 0.39 - ETA: 0s - loss: 1.0898 - accuracy: 0.39 - ETA: 0s - loss: 1.0891 - accuracy: 0.39 - ETA: 0s - loss: 1.0889 - accuracy: 0.39 - ETA: 0s - loss: 1.0893 - accuracy: 0.39 - ETA: 0s - loss: 1.0885 - accuracy: 0.39 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - 1s 52us/step - loss: 1.0870 - accuracy: 0.4051 - val_loss: 1.0889 - val_accuracy: 0.3971\n",
      "Epoch 7/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0815 - accuracy: 0.42 - ETA: 0s - loss: 1.0845 - accuracy: 0.41 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0866 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - 1s 48us/step - loss: 1.0869 - accuracy: 0.4051 - val_loss: 1.0889 - val_accuracy: 0.3971\n",
      "Epoch 8/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0777 - accuracy: 0.43 - ETA: 0s - loss: 1.0844 - accuracy: 0.41 - ETA: 0s - loss: 1.0852 - accuracy: 0.41 - ETA: 0s - loss: 1.0861 - accuracy: 0.40 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0865 - accuracy: 0.40 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - 1s 46us/step - loss: 1.0868 - accuracy: 0.4051 - val_loss: 1.0888 - val_accuracy: 0.3971\n",
      "Epoch 9/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0894 - accuracy: 0.39 - ETA: 0s - loss: 1.0852 - accuracy: 0.40 - ETA: 0s - loss: 1.0846 - accuracy: 0.40 - ETA: 0s - loss: 1.0851 - accuracy: 0.40 - ETA: 0s - loss: 1.0861 - accuracy: 0.40 - ETA: 0s - loss: 1.0866 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - 1s 46us/step - loss: 1.0867 - accuracy: 0.4051 - val_loss: 1.0887 - val_accuracy: 0.3971\n",
      "Epoch 10/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0840 - accuracy: 0.41 - ETA: 0s - loss: 1.0863 - accuracy: 0.40 - ETA: 0s - loss: 1.0862 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - 1s 46us/step - loss: 1.0866 - accuracy: 0.4051 - val_loss: 1.0887 - val_accuracy: 0.3971\n",
      "Epoch 11/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0890 - accuracy: 0.39 - ETA: 0s - loss: 1.0911 - accuracy: 0.38 - ETA: 0s - loss: 1.0885 - accuracy: 0.39 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0862 - accuracy: 0.40 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - 1s 46us/step - loss: 1.0864 - accuracy: 0.4051 - val_loss: 1.0884 - val_accuracy: 0.3971\n",
      "Epoch 12/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.42 - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - ETA: 0s - loss: 1.0840 - accuracy: 0.41 - ETA: 0s - loss: 1.0861 - accuracy: 0.40 - ETA: 0s - loss: 1.0847 - accuracy: 0.40 - ETA: 0s - loss: 1.0854 - accuracy: 0.40 - ETA: 0s - loss: 1.0860 - accuracy: 0.40 - ETA: 0s - loss: 1.0863 - accuracy: 0.40 - 1s 45us/step - loss: 1.0862 - accuracy: 0.4051 - val_loss: 1.0883 - val_accuracy: 0.3971\n",
      "Epoch 13/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0813 - accuracy: 0.42 - ETA: 0s - loss: 1.0878 - accuracy: 0.39 - ETA: 0s - loss: 1.0877 - accuracy: 0.39 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0862 - accuracy: 0.40 - ETA: 0s - loss: 1.0858 - accuracy: 0.40 - ETA: 0s - loss: 1.0856 - accuracy: 0.40 - ETA: 0s - loss: 1.0860 - accuracy: 0.40 - 1s 48us/step - loss: 1.0860 - accuracy: 0.4051 - val_loss: 1.0880 - val_accuracy: 0.3971\n",
      "Epoch 14/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0972 - accuracy: 0.37 - ETA: 0s - loss: 1.0881 - accuracy: 0.39 - ETA: 0s - loss: 1.0876 - accuracy: 0.39 - ETA: 0s - loss: 1.0866 - accuracy: 0.40 - ETA: 0s - loss: 1.0865 - accuracy: 0.40 - ETA: 0s - loss: 1.0862 - accuracy: 0.40 - ETA: 0s - loss: 1.0860 - accuracy: 0.40 - ETA: 0s - loss: 1.0855 - accuracy: 0.40 - 1s 46us/step - loss: 1.0857 - accuracy: 0.4051 - val_loss: 1.0877 - val_accuracy: 0.3971\n",
      "Epoch 15/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0845 - accuracy: 0.41 - ETA: 0s - loss: 1.0821 - accuracy: 0.41 - ETA: 0s - loss: 1.0834 - accuracy: 0.41 - ETA: 0s - loss: 1.0855 - accuracy: 0.40 - ETA: 0s - loss: 1.0845 - accuracy: 0.40 - ETA: 0s - loss: 1.0850 - accuracy: 0.40 - ETA: 0s - loss: 1.0855 - accuracy: 0.40 - ETA: 0s - loss: 1.0856 - accuracy: 0.40 - 1s 47us/step - loss: 1.0853 - accuracy: 0.4051 - val_loss: 1.0873 - val_accuracy: 0.3971\n",
      "Epoch 16/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0831 - accuracy: 0.41 - ETA: 0s - loss: 1.0790 - accuracy: 0.42 - ETA: 0s - loss: 1.0812 - accuracy: 0.41 - ETA: 0s - loss: 1.0830 - accuracy: 0.41 - ETA: 0s - loss: 1.0830 - accuracy: 0.41 - ETA: 0s - loss: 1.0833 - accuracy: 0.41 - ETA: 0s - loss: 1.0842 - accuracy: 0.40 - ETA: 0s - loss: 1.0846 - accuracy: 0.40 - 1s 46us/step - loss: 1.0848 - accuracy: 0.4051 - val_loss: 1.0868 - val_accuracy: 0.3971\n",
      "Epoch 17/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0815 - accuracy: 0.41 - ETA: 0s - loss: 1.0804 - accuracy: 0.41 - ETA: 0s - loss: 1.0831 - accuracy: 0.41 - ETA: 0s - loss: 1.0836 - accuracy: 0.40 - ETA: 0s - loss: 1.0841 - accuracy: 0.40 - ETA: 0s - loss: 1.0837 - accuracy: 0.40 - ETA: 0s - loss: 1.0841 - accuracy: 0.40 - ETA: 0s - loss: 1.0838 - accuracy: 0.40 - 1s 46us/step - loss: 1.0842 - accuracy: 0.4051 - val_loss: 1.0861 - val_accuracy: 0.3971\n",
      "Epoch 18/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.39 - ETA: 0s - loss: 1.0839 - accuracy: 0.40 - ETA: 0s - loss: 1.0834 - accuracy: 0.40 - ETA: 0s - loss: 1.0842 - accuracy: 0.40 - ETA: 0s - loss: 1.0837 - accuracy: 0.40 - ETA: 0s - loss: 1.0841 - accuracy: 0.40 - ETA: 0s - loss: 1.0839 - accuracy: 0.40 - ETA: 0s - loss: 1.0834 - accuracy: 0.40 - 1s 46us/step - loss: 1.0833 - accuracy: 0.4051 - val_loss: 1.0851 - val_accuracy: 0.3971\n",
      "Epoch 19/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0841 - accuracy: 0.40 - ETA: 0s - loss: 1.0845 - accuracy: 0.39 - ETA: 0s - loss: 1.0840 - accuracy: 0.40 - ETA: 0s - loss: 1.0831 - accuracy: 0.40 - ETA: 0s - loss: 1.0828 - accuracy: 0.40 - ETA: 0s - loss: 1.0823 - accuracy: 0.40 - ETA: 0s - loss: 1.0822 - accuracy: 0.40 - ETA: 0s - loss: 1.0824 - accuracy: 0.40 - 1s 46us/step - loss: 1.0821 - accuracy: 0.4051 - val_loss: 1.0839 - val_accuracy: 0.3971\n",
      "Epoch 20/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0838 - accuracy: 0.39 - ETA: 0s - loss: 1.0797 - accuracy: 0.41 - ETA: 0s - loss: 1.0800 - accuracy: 0.40 - ETA: 0s - loss: 1.0814 - accuracy: 0.40 - ETA: 0s - loss: 1.0804 - accuracy: 0.40 - ETA: 0s - loss: 1.0804 - accuracy: 0.40 - ETA: 0s - loss: 1.0804 - accuracy: 0.40 - ETA: 0s - loss: 1.0805 - accuracy: 0.40 - 1s 47us/step - loss: 1.0806 - accuracy: 0.4051 - val_loss: 1.0824 - val_accuracy: 0.3971\n",
      "Epoch 21/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0757 - accuracy: 0.41 - ETA: 0s - loss: 1.0769 - accuracy: 0.41 - ETA: 0s - loss: 1.0803 - accuracy: 0.40 - ETA: 0s - loss: 1.0784 - accuracy: 0.40 - ETA: 0s - loss: 1.0777 - accuracy: 0.40 - ETA: 0s - loss: 1.0789 - accuracy: 0.40 - ETA: 0s - loss: 1.0782 - accuracy: 0.40 - ETA: 0s - loss: 1.0786 - accuracy: 0.40 - 1s 46us/step - loss: 1.0785 - accuracy: 0.4051 - val_loss: 1.0802 - val_accuracy: 0.3971\n",
      "Epoch 22/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.41 - ETA: 0s - loss: 1.0757 - accuracy: 0.41 - ETA: 0s - loss: 1.0745 - accuracy: 0.41 - ETA: 0s - loss: 1.0758 - accuracy: 0.40 - ETA: 0s - loss: 1.0753 - accuracy: 0.40 - ETA: 0s - loss: 1.0760 - accuracy: 0.40 - ETA: 0s - loss: 1.0760 - accuracy: 0.40 - ETA: 0s - loss: 1.0760 - accuracy: 0.40 - 1s 46us/step - loss: 1.0757 - accuracy: 0.4051 - val_loss: 1.0773 - val_accuracy: 0.3971\n",
      "Epoch 23/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0717 - accuracy: 0.41 - ETA: 0s - loss: 1.0741 - accuracy: 0.40 - ETA: 0s - loss: 1.0743 - accuracy: 0.40 - ETA: 0s - loss: 1.0745 - accuracy: 0.40 - ETA: 0s - loss: 1.0729 - accuracy: 0.40 - ETA: 0s - loss: 1.0733 - accuracy: 0.40 - ETA: 0s - loss: 1.0720 - accuracy: 0.40 - ETA: 0s - loss: 1.0719 - accuracy: 0.40 - 1s 47us/step - loss: 1.0719 - accuracy: 0.4052 - val_loss: 1.0734 - val_accuracy: 0.3971\n",
      "Epoch 24/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.38 - ETA: 0s - loss: 1.0693 - accuracy: 0.40 - ETA: 0s - loss: 1.0682 - accuracy: 0.40 - ETA: 0s - loss: 1.0686 - accuracy: 0.40 - ETA: 0s - loss: 1.0677 - accuracy: 0.40 - ETA: 0s - loss: 1.0677 - accuracy: 0.40 - ETA: 0s - loss: 1.0684 - accuracy: 0.40 - ETA: 0s - loss: 1.0674 - accuracy: 0.40 - ETA: 0s - loss: 1.0667 - accuracy: 0.40 - 1s 47us/step - loss: 1.0669 - accuracy: 0.4054 - val_loss: 1.0682 - val_accuracy: 0.3986\n",
      "Epoch 25/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0663 - accuracy: 0.39 - ETA: 0s - loss: 1.0651 - accuracy: 0.39 - ETA: 0s - loss: 1.0632 - accuracy: 0.40 - ETA: 0s - loss: 1.0621 - accuracy: 0.40 - ETA: 0s - loss: 1.0614 - accuracy: 0.40 - ETA: 0s - loss: 1.0614 - accuracy: 0.40 - ETA: 0s - loss: 1.0608 - accuracy: 0.40 - ETA: 0s - loss: 1.0604 - accuracy: 0.40 - 1s 47us/step - loss: 1.0604 - accuracy: 0.4087 - val_loss: 1.0616 - val_accuracy: 0.4042\n",
      "Epoch 26/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0469 - accuracy: 0.44 - ETA: 0s - loss: 1.0533 - accuracy: 0.41 - ETA: 0s - loss: 1.0524 - accuracy: 0.41 - ETA: 0s - loss: 1.0546 - accuracy: 0.41 - ETA: 0s - loss: 1.0557 - accuracy: 0.40 - ETA: 0s - loss: 1.0552 - accuracy: 0.40 - ETA: 0s - loss: 1.0537 - accuracy: 0.41 - ETA: 0s - loss: 1.0529 - accuracy: 0.41 - 1s 50us/step - loss: 1.0521 - accuracy: 0.4198 - val_loss: 1.0536 - val_accuracy: 0.4198\n",
      "Epoch 27/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0456 - accuracy: 0.44 - ETA: 0s - loss: 1.0444 - accuracy: 0.44 - ETA: 0s - loss: 1.0428 - accuracy: 0.44 - ETA: 0s - loss: 1.0425 - accuracy: 0.44 - ETA: 0s - loss: 1.0423 - accuracy: 0.44 - ETA: 0s - loss: 1.0426 - accuracy: 0.44 - ETA: 0s - loss: 1.0429 - accuracy: 0.44 - ETA: 0s - loss: 1.0429 - accuracy: 0.44 - ETA: 0s - loss: 1.0421 - accuracy: 0.44 - ETA: 0s - loss: 1.0421 - accuracy: 0.44 - ETA: 0s - loss: 1.0420 - accuracy: 0.44 - 1s 53us/step - loss: 1.0418 - accuracy: 0.4468 - val_loss: 1.0439 - val_accuracy: 0.4441\n",
      "Epoch 28/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0361 - accuracy: 0.46 - ETA: 0s - loss: 1.0333 - accuracy: 0.47 - ETA: 0s - loss: 1.0329 - accuracy: 0.47 - ETA: 0s - loss: 1.0302 - accuracy: 0.48 - ETA: 0s - loss: 1.0298 - accuracy: 0.48 - ETA: 0s - loss: 1.0306 - accuracy: 0.48 - ETA: 0s - loss: 1.0307 - accuracy: 0.48 - ETA: 0s - loss: 1.0309 - accuracy: 0.48 - ETA: 0s - loss: 1.0308 - accuracy: 0.48 - ETA: 0s - loss: 1.0298 - accuracy: 0.48 - 1s 51us/step - loss: 1.0296 - accuracy: 0.4827 - val_loss: 1.0327 - val_accuracy: 0.4816\n",
      "Epoch 29/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0197 - accuracy: 0.52 - ETA: 0s - loss: 1.0197 - accuracy: 0.52 - ETA: 0s - loss: 1.0196 - accuracy: 0.52 - ETA: 0s - loss: 1.0192 - accuracy: 0.52 - ETA: 0s - loss: 1.0192 - accuracy: 0.52 - ETA: 0s - loss: 1.0202 - accuracy: 0.52 - ETA: 0s - loss: 1.0189 - accuracy: 0.52 - ETA: 0s - loss: 1.0186 - accuracy: 0.52 - ETA: 0s - loss: 1.0173 - accuracy: 0.51 - ETA: 0s - loss: 1.0163 - accuracy: 0.52 - 1s 49us/step - loss: 1.0160 - accuracy: 0.5201 - val_loss: 1.0203 - val_accuracy: 0.5169\n",
      "Epoch 30/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0056 - accuracy: 0.54 - ETA: 0s - loss: 1.0069 - accuracy: 0.55 - ETA: 0s - loss: 1.0050 - accuracy: 0.56 - ETA: 0s - loss: 1.0040 - accuracy: 0.55 - ETA: 0s - loss: 1.0037 - accuracy: 0.55 - ETA: 0s - loss: 1.0031 - accuracy: 0.55 - ETA: 0s - loss: 1.0023 - accuracy: 0.55 - ETA: 0s - loss: 1.0008 - accuracy: 0.55 - 1s 45us/step - loss: 1.0010 - accuracy: 0.5530 - val_loss: 1.0074 - val_accuracy: 0.5470\n",
      "Epoch 31/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9923 - accuracy: 0.58 - ETA: 0s - loss: 0.9917 - accuracy: 0.58 - ETA: 0s - loss: 0.9886 - accuracy: 0.59 - ETA: 0s - loss: 0.9886 - accuracy: 0.58 - ETA: 0s - loss: 0.9870 - accuracy: 0.58 - ETA: 0s - loss: 0.9869 - accuracy: 0.57 - ETA: 0s - loss: 0.9863 - accuracy: 0.57 - ETA: 0s - loss: 0.9864 - accuracy: 0.57 - 1s 47us/step - loss: 0.9856 - accuracy: 0.5729 - val_loss: 0.9940 - val_accuracy: 0.5592\n",
      "Epoch 32/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9797 - accuracy: 0.57 - ETA: 0s - loss: 0.9791 - accuracy: 0.58 - ETA: 0s - loss: 0.9782 - accuracy: 0.58 - ETA: 0s - loss: 0.9757 - accuracy: 0.58 - ETA: 0s - loss: 0.9735 - accuracy: 0.58 - ETA: 0s - loss: 0.9727 - accuracy: 0.58 - ETA: 0s - loss: 0.9700 - accuracy: 0.58 - ETA: 0s - loss: 0.9690 - accuracy: 0.58 - 1s 45us/step - loss: 0.9688 - accuracy: 0.5892 - val_loss: 0.9803 - val_accuracy: 0.5603\n",
      "Epoch 33/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9521 - accuracy: 0.61 - ETA: 0s - loss: 0.9555 - accuracy: 0.60 - ETA: 0s - loss: 0.9552 - accuracy: 0.60 - ETA: 0s - loss: 0.9552 - accuracy: 0.60 - ETA: 0s - loss: 0.9547 - accuracy: 0.60 - ETA: 0s - loss: 0.9539 - accuracy: 0.59 - ETA: 0s - loss: 0.9538 - accuracy: 0.59 - ETA: 0s - loss: 0.9521 - accuracy: 0.59 - 1s 48us/step - loss: 0.9521 - accuracy: 0.5978 - val_loss: 0.9667 - val_accuracy: 0.5728\n",
      "Epoch 34/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9512 - accuracy: 0.58 - ETA: 0s - loss: 0.9496 - accuracy: 0.59 - ETA: 0s - loss: 0.9450 - accuracy: 0.60 - ETA: 0s - loss: 0.9439 - accuracy: 0.60 - ETA: 0s - loss: 0.9435 - accuracy: 0.60 - ETA: 0s - loss: 0.9419 - accuracy: 0.60 - ETA: 0s - loss: 0.9404 - accuracy: 0.60 - ETA: 0s - loss: 0.9410 - accuracy: 0.60 - ETA: 0s - loss: 0.9388 - accuracy: 0.60 - ETA: 0s - loss: 0.9380 - accuracy: 0.60 - ETA: 0s - loss: 0.9374 - accuracy: 0.60 - ETA: 0s - loss: 0.9364 - accuracy: 0.60 - 1s 52us/step - loss: 0.9353 - accuracy: 0.6082 - val_loss: 0.9531 - val_accuracy: 0.5776\n",
      "Epoch 35/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.63 - ETA: 0s - loss: 0.9218 - accuracy: 0.62 - ETA: 0s - loss: 0.9234 - accuracy: 0.62 - ETA: 0s - loss: 0.9220 - accuracy: 0.62 - ETA: 0s - loss: 0.9225 - accuracy: 0.61 - ETA: 0s - loss: 0.9213 - accuracy: 0.61 - ETA: 0s - loss: 0.9208 - accuracy: 0.61 - ETA: 0s - loss: 0.9199 - accuracy: 0.61 - ETA: 0s - loss: 0.9197 - accuracy: 0.61 - ETA: 0s - loss: 0.9184 - accuracy: 0.61 - 1s 51us/step - loss: 0.9186 - accuracy: 0.6159 - val_loss: 0.9397 - val_accuracy: 0.5822\n",
      "Epoch 36/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9246 - accuracy: 0.58 - ETA: 0s - loss: 0.9132 - accuracy: 0.60 - ETA: 0s - loss: 0.9089 - accuracy: 0.61 - ETA: 0s - loss: 0.9080 - accuracy: 0.61 - ETA: 0s - loss: 0.9065 - accuracy: 0.62 - ETA: 0s - loss: 0.9047 - accuracy: 0.62 - ETA: 0s - loss: 0.9042 - accuracy: 0.61 - ETA: 0s - loss: 0.9030 - accuracy: 0.62 - ETA: 0s - loss: 0.9026 - accuracy: 0.62 - 1s 49us/step - loss: 0.9021 - accuracy: 0.6219 - val_loss: 0.9272 - val_accuracy: 0.5827\n",
      "Epoch 37/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8995 - accuracy: 0.60 - ETA: 0s - loss: 0.8889 - accuracy: 0.62 - ETA: 0s - loss: 0.8912 - accuracy: 0.61 - ETA: 0s - loss: 0.8913 - accuracy: 0.61 - ETA: 0s - loss: 0.8915 - accuracy: 0.61 - ETA: 0s - loss: 0.8902 - accuracy: 0.62 - ETA: 0s - loss: 0.8903 - accuracy: 0.61 - ETA: 0s - loss: 0.8893 - accuracy: 0.61 - ETA: 0s - loss: 0.8879 - accuracy: 0.62 - ETA: 0s - loss: 0.8873 - accuracy: 0.62 - ETA: 0s - loss: 0.8869 - accuracy: 0.62 - ETA: 0s - loss: 0.8864 - accuracy: 0.62 - 1s 52us/step - loss: 0.8859 - accuracy: 0.6250 - val_loss: 0.9146 - val_accuracy: 0.5889\n",
      "Epoch 38/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8690 - accuracy: 0.63 - ETA: 0s - loss: 0.8742 - accuracy: 0.63 - ETA: 0s - loss: 0.8773 - accuracy: 0.63 - ETA: 0s - loss: 0.8770 - accuracy: 0.63 - ETA: 0s - loss: 0.8742 - accuracy: 0.63 - ETA: 0s - loss: 0.8731 - accuracy: 0.63 - ETA: 0s - loss: 0.8725 - accuracy: 0.63 - ETA: 0s - loss: 0.8721 - accuracy: 0.63 - ETA: 0s - loss: 0.8710 - accuracy: 0.63 - ETA: 0s - loss: 0.8705 - accuracy: 0.63 - 1s 51us/step - loss: 0.8700 - accuracy: 0.6320 - val_loss: 0.9024 - val_accuracy: 0.5927\n",
      "Epoch 39/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8586 - accuracy: 0.62 - ETA: 0s - loss: 0.8611 - accuracy: 0.63 - ETA: 0s - loss: 0.8588 - accuracy: 0.63 - ETA: 0s - loss: 0.8587 - accuracy: 0.63 - ETA: 0s - loss: 0.8573 - accuracy: 0.63 - ETA: 0s - loss: 0.8582 - accuracy: 0.63 - ETA: 0s - loss: 0.8561 - accuracy: 0.63 - ETA: 0s - loss: 0.8549 - accuracy: 0.63 - ETA: 0s - loss: 0.8542 - accuracy: 0.63 - 1s 49us/step - loss: 0.8547 - accuracy: 0.6356 - val_loss: 0.8910 - val_accuracy: 0.5958\n",
      "Epoch 40/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8562 - accuracy: 0.62 - ETA: 0s - loss: 0.8504 - accuracy: 0.63 - ETA: 0s - loss: 0.8491 - accuracy: 0.63 - ETA: 0s - loss: 0.8445 - accuracy: 0.64 - ETA: 0s - loss: 0.8434 - accuracy: 0.64 - ETA: 0s - loss: 0.8425 - accuracy: 0.63 - ETA: 0s - loss: 0.8405 - accuracy: 0.64 - ETA: 0s - loss: 0.8405 - accuracy: 0.63 - 1s 47us/step - loss: 0.8398 - accuracy: 0.6397 - val_loss: 0.8802 - val_accuracy: 0.6019\n",
      "Epoch 41/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8343 - accuracy: 0.64 - ETA: 0s - loss: 0.8438 - accuracy: 0.62 - ETA: 0s - loss: 0.8366 - accuracy: 0.63 - ETA: 0s - loss: 0.8356 - accuracy: 0.64 - ETA: 0s - loss: 0.8324 - accuracy: 0.64 - ETA: 0s - loss: 0.8281 - accuracy: 0.64 - ETA: 0s - loss: 0.8283 - accuracy: 0.64 - ETA: 0s - loss: 0.8269 - accuracy: 0.64 - ETA: 0s - loss: 0.8254 - accuracy: 0.64 - ETA: 0s - loss: 0.8259 - accuracy: 0.64 - 1s 49us/step - loss: 0.8256 - accuracy: 0.6440 - val_loss: 0.8703 - val_accuracy: 0.5981\n",
      "Epoch 42/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8313 - accuracy: 0.61 - ETA: 0s - loss: 0.8216 - accuracy: 0.63 - ETA: 0s - loss: 0.8178 - accuracy: 0.63 - ETA: 0s - loss: 0.8140 - accuracy: 0.64 - ETA: 0s - loss: 0.8128 - accuracy: 0.64 - ETA: 0s - loss: 0.8125 - accuracy: 0.64 - ETA: 0s - loss: 0.8105 - accuracy: 0.64 - ETA: 0s - loss: 0.8117 - accuracy: 0.64 - 1s 47us/step - loss: 0.8116 - accuracy: 0.6460 - val_loss: 0.8596 - val_accuracy: 0.6037\n",
      "Epoch 43/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8021 - accuracy: 0.64 - ETA: 0s - loss: 0.8056 - accuracy: 0.64 - ETA: 0s - loss: 0.8063 - accuracy: 0.64 - ETA: 0s - loss: 0.8031 - accuracy: 0.64 - ETA: 0s - loss: 0.8027 - accuracy: 0.64 - ETA: 0s - loss: 0.8017 - accuracy: 0.64 - ETA: 0s - loss: 0.8013 - accuracy: 0.64 - ETA: 0s - loss: 0.7984 - accuracy: 0.64 - 1s 47us/step - loss: 0.7980 - accuracy: 0.6494 - val_loss: 0.8499 - val_accuracy: 0.6052\n",
      "Epoch 44/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7849 - accuracy: 0.65 - ETA: 0s - loss: 0.7855 - accuracy: 0.65 - ETA: 0s - loss: 0.7880 - accuracy: 0.65 - ETA: 0s - loss: 0.7857 - accuracy: 0.65 - ETA: 0s - loss: 0.7850 - accuracy: 0.65 - ETA: 0s - loss: 0.7868 - accuracy: 0.65 - ETA: 0s - loss: 0.7864 - accuracy: 0.65 - ETA: 0s - loss: 0.7856 - accuracy: 0.65 - 1s 47us/step - loss: 0.7852 - accuracy: 0.6526 - val_loss: 0.8409 - val_accuracy: 0.6065\n",
      "Epoch 45/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7807 - accuracy: 0.64 - ETA: 0s - loss: 0.7706 - accuracy: 0.66 - ETA: 0s - loss: 0.7711 - accuracy: 0.66 - ETA: 0s - loss: 0.7716 - accuracy: 0.66 - ETA: 0s - loss: 0.7716 - accuracy: 0.66 - ETA: 0s - loss: 0.7725 - accuracy: 0.65 - ETA: 0s - loss: 0.7738 - accuracy: 0.65 - ETA: 0s - loss: 0.7729 - accuracy: 0.65 - 1s 47us/step - loss: 0.7730 - accuracy: 0.6555 - val_loss: 0.8322 - val_accuracy: 0.6078\n",
      "Epoch 46/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7784 - accuracy: 0.63 - ETA: 0s - loss: 0.7659 - accuracy: 0.64 - ETA: 0s - loss: 0.7663 - accuracy: 0.65 - ETA: 0s - loss: 0.7659 - accuracy: 0.65 - ETA: 0s - loss: 0.7659 - accuracy: 0.65 - ETA: 0s - loss: 0.7644 - accuracy: 0.65 - ETA: 0s - loss: 0.7639 - accuracy: 0.65 - ETA: 0s - loss: 0.7630 - accuracy: 0.65 - ETA: 0s - loss: 0.7618 - accuracy: 0.65 - 1s 50us/step - loss: 0.7609 - accuracy: 0.6580 - val_loss: 0.8238 - val_accuracy: 0.6093\n",
      "Epoch 47/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7577 - accuracy: 0.64 - ETA: 0s - loss: 0.7535 - accuracy: 0.64 - ETA: 0s - loss: 0.7562 - accuracy: 0.64 - ETA: 0s - loss: 0.7557 - accuracy: 0.64 - ETA: 0s - loss: 0.7553 - accuracy: 0.65 - ETA: 0s - loss: 0.7543 - accuracy: 0.65 - ETA: 0s - loss: 0.7522 - accuracy: 0.65 - ETA: 0s - loss: 0.7521 - accuracy: 0.65 - ETA: 0s - loss: 0.7518 - accuracy: 0.65 - ETA: 0s - loss: 0.7497 - accuracy: 0.65 - 1s 51us/step - loss: 0.7495 - accuracy: 0.6602 - val_loss: 0.8159 - val_accuracy: 0.6116\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.66 - ETA: 0s - loss: 0.7423 - accuracy: 0.65 - ETA: 0s - loss: 0.7450 - accuracy: 0.65 - ETA: 0s - loss: 0.7433 - accuracy: 0.65 - ETA: 0s - loss: 0.7448 - accuracy: 0.65 - ETA: 0s - loss: 0.7425 - accuracy: 0.65 - ETA: 0s - loss: 0.7414 - accuracy: 0.65 - ETA: 0s - loss: 0.7396 - accuracy: 0.65 - ETA: 0s - loss: 0.7394 - accuracy: 0.66 - ETA: 0s - loss: 0.7381 - accuracy: 0.66 - ETA: 0s - loss: 0.7372 - accuracy: 0.66 - ETA: 0s - loss: 0.7378 - accuracy: 0.66 - ETA: 0s - loss: 0.7382 - accuracy: 0.66 - ETA: 0s - loss: 0.7382 - accuracy: 0.66 - 1s 57us/step - loss: 0.7386 - accuracy: 0.6623 - val_loss: 0.8083 - val_accuracy: 0.6126\n",
      "Epoch 49/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.67 - ETA: 0s - loss: 0.7285 - accuracy: 0.67 - ETA: 0s - loss: 0.7304 - accuracy: 0.67 - ETA: 0s - loss: 0.7300 - accuracy: 0.67 - ETA: 0s - loss: 0.7321 - accuracy: 0.66 - ETA: 0s - loss: 0.7320 - accuracy: 0.66 - ETA: 0s - loss: 0.7302 - accuracy: 0.66 - ETA: 0s - loss: 0.7284 - accuracy: 0.66 - ETA: 0s - loss: 0.7264 - accuracy: 0.67 - ETA: 0s - loss: 0.7272 - accuracy: 0.66 - ETA: 0s - loss: 0.7273 - accuracy: 0.66 - ETA: 0s - loss: 0.7280 - accuracy: 0.66 - 1s 54us/step - loss: 0.7282 - accuracy: 0.6639 - val_loss: 0.8014 - val_accuracy: 0.6147\n",
      "Epoch 50/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7122 - accuracy: 0.68 - ETA: 0s - loss: 0.7161 - accuracy: 0.67 - ETA: 0s - loss: 0.7212 - accuracy: 0.67 - ETA: 0s - loss: 0.7222 - accuracy: 0.66 - ETA: 0s - loss: 0.7238 - accuracy: 0.66 - ETA: 0s - loss: 0.7235 - accuracy: 0.66 - ETA: 0s - loss: 0.7242 - accuracy: 0.66 - ETA: 0s - loss: 0.7232 - accuracy: 0.66 - ETA: 0s - loss: 0.7209 - accuracy: 0.66 - ETA: 0s - loss: 0.7207 - accuracy: 0.66 - ETA: 0s - loss: 0.7201 - accuracy: 0.66 - ETA: 0s - loss: 0.7195 - accuracy: 0.66 - ETA: 0s - loss: 0.7187 - accuracy: 0.66 - ETA: 0s - loss: 0.7186 - accuracy: 0.66 - 1s 58us/step - loss: 0.7178 - accuracy: 0.6663 - val_loss: 0.7944 - val_accuracy: 0.6144\n",
      "Epoch 51/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.66 - ETA: 0s - loss: 0.7020 - accuracy: 0.67 - ETA: 0s - loss: 0.7050 - accuracy: 0.67 - ETA: 0s - loss: 0.7078 - accuracy: 0.67 - ETA: 0s - loss: 0.7115 - accuracy: 0.66 - ETA: 0s - loss: 0.7112 - accuracy: 0.66 - ETA: 0s - loss: 0.7110 - accuracy: 0.66 - ETA: 0s - loss: 0.7104 - accuracy: 0.66 - ETA: 0s - loss: 0.7085 - accuracy: 0.66 - 1s 50us/step - loss: 0.7083 - accuracy: 0.6682 - val_loss: 0.7879 - val_accuracy: 0.6152\n",
      "Epoch 52/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7050 - accuracy: 0.69 - ETA: 0s - loss: 0.6964 - accuracy: 0.68 - ETA: 0s - loss: 0.6972 - accuracy: 0.67 - ETA: 0s - loss: 0.6964 - accuracy: 0.68 - ETA: 0s - loss: 0.6977 - accuracy: 0.67 - ETA: 0s - loss: 0.6965 - accuracy: 0.67 - ETA: 0s - loss: 0.6989 - accuracy: 0.67 - ETA: 0s - loss: 0.7005 - accuracy: 0.67 - ETA: 0s - loss: 0.6991 - accuracy: 0.67 - ETA: 0s - loss: 0.6992 - accuracy: 0.67 - ETA: 0s - loss: 0.6988 - accuracy: 0.66 - 1s 51us/step - loss: 0.6991 - accuracy: 0.6696 - val_loss: 0.7819 - val_accuracy: 0.6167\n",
      "Epoch 53/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.66 - ETA: 0s - loss: 0.6993 - accuracy: 0.66 - ETA: 0s - loss: 0.6971 - accuracy: 0.67 - ETA: 0s - loss: 0.6919 - accuracy: 0.67 - ETA: 0s - loss: 0.6918 - accuracy: 0.67 - ETA: 0s - loss: 0.6915 - accuracy: 0.67 - ETA: 0s - loss: 0.6905 - accuracy: 0.67 - ETA: 0s - loss: 0.6907 - accuracy: 0.67 - 1s 49us/step - loss: 0.6903 - accuracy: 0.6715 - val_loss: 0.7762 - val_accuracy: 0.6177\n",
      "Epoch 54/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.66 - ETA: 0s - loss: 0.6821 - accuracy: 0.66 - ETA: 0s - loss: 0.6852 - accuracy: 0.66 - ETA: 0s - loss: 0.6863 - accuracy: 0.66 - ETA: 0s - loss: 0.6884 - accuracy: 0.66 - ETA: 0s - loss: 0.6863 - accuracy: 0.66 - ETA: 0s - loss: 0.6849 - accuracy: 0.67 - ETA: 0s - loss: 0.6826 - accuracy: 0.67 - 1s 49us/step - loss: 0.6821 - accuracy: 0.6733 - val_loss: 0.7704 - val_accuracy: 0.6195\n",
      "Epoch 55/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.68 - ETA: 0s - loss: 0.6715 - accuracy: 0.67 - ETA: 0s - loss: 0.6696 - accuracy: 0.67 - ETA: 0s - loss: 0.6716 - accuracy: 0.68 - ETA: 0s - loss: 0.6719 - accuracy: 0.67 - ETA: 0s - loss: 0.6728 - accuracy: 0.67 - ETA: 0s - loss: 0.6745 - accuracy: 0.67 - ETA: 0s - loss: 0.6744 - accuracy: 0.67 - 1s 49us/step - loss: 0.6740 - accuracy: 0.6743 - val_loss: 0.7680 - val_accuracy: 0.6185\n",
      "Epoch 56/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.66 - ETA: 0s - loss: 0.6650 - accuracy: 0.68 - ETA: 0s - loss: 0.6676 - accuracy: 0.68 - ETA: 0s - loss: 0.6692 - accuracy: 0.67 - ETA: 0s - loss: 0.6684 - accuracy: 0.67 - ETA: 0s - loss: 0.6684 - accuracy: 0.67 - ETA: 0s - loss: 0.6675 - accuracy: 0.67 - ETA: 0s - loss: 0.6669 - accuracy: 0.67 - 1s 49us/step - loss: 0.6666 - accuracy: 0.6753 - val_loss: 0.7617 - val_accuracy: 0.6195\n",
      "Epoch 57/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6542 - accuracy: 0.67 - ETA: 0s - loss: 0.6543 - accuracy: 0.67 - ETA: 0s - loss: 0.6571 - accuracy: 0.67 - ETA: 0s - loss: 0.6545 - accuracy: 0.67 - ETA: 0s - loss: 0.6571 - accuracy: 0.67 - ETA: 0s - loss: 0.6595 - accuracy: 0.67 - ETA: 0s - loss: 0.6586 - accuracy: 0.67 - ETA: 0s - loss: 0.6591 - accuracy: 0.67 - ETA: 0s - loss: 0.6595 - accuracy: 0.67 - ETA: 0s - loss: 0.6591 - accuracy: 0.67 - 1s 50us/step - loss: 0.6593 - accuracy: 0.6764 - val_loss: 0.7564 - val_accuracy: 0.6203\n",
      "Epoch 58/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6523 - accuracy: 0.68 - ETA: 0s - loss: 0.6519 - accuracy: 0.67 - ETA: 0s - loss: 0.6577 - accuracy: 0.67 - ETA: 0s - loss: 0.6573 - accuracy: 0.67 - ETA: 0s - loss: 0.6559 - accuracy: 0.67 - ETA: 0s - loss: 0.6557 - accuracy: 0.67 - ETA: 0s - loss: 0.6550 - accuracy: 0.67 - ETA: 0s - loss: 0.6531 - accuracy: 0.67 - 1s 49us/step - loss: 0.6522 - accuracy: 0.6776 - val_loss: 0.7521 - val_accuracy: 0.6208\n",
      "Epoch 59/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6552 - accuracy: 0.66 - ETA: 0s - loss: 0.6602 - accuracy: 0.65 - ETA: 0s - loss: 0.6570 - accuracy: 0.65 - ETA: 0s - loss: 0.6519 - accuracy: 0.66 - ETA: 0s - loss: 0.6498 - accuracy: 0.66 - ETA: 0s - loss: 0.6494 - accuracy: 0.67 - ETA: 0s - loss: 0.6482 - accuracy: 0.67 - ETA: 0s - loss: 0.6483 - accuracy: 0.67 - ETA: 0s - loss: 0.6466 - accuracy: 0.67 - ETA: 0s - loss: 0.6458 - accuracy: 0.67 - 1s 49us/step - loss: 0.6458 - accuracy: 0.6784 - val_loss: 0.7471 - val_accuracy: 0.6221\n",
      "Epoch 60/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.70 - ETA: 0s - loss: 0.6395 - accuracy: 0.68 - ETA: 0s - loss: 0.6382 - accuracy: 0.68 - ETA: 0s - loss: 0.6384 - accuracy: 0.68 - ETA: 0s - loss: 0.6410 - accuracy: 0.68 - ETA: 0s - loss: 0.6402 - accuracy: 0.67 - ETA: 0s - loss: 0.6380 - accuracy: 0.68 - ETA: 0s - loss: 0.6393 - accuracy: 0.67 - 1s 48us/step - loss: 0.6395 - accuracy: 0.6796 - val_loss: 0.7435 - val_accuracy: 0.6218\n",
      "Epoch 61/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.68 - ETA: 0s - loss: 0.6354 - accuracy: 0.67 - ETA: 0s - loss: 0.6339 - accuracy: 0.68 - ETA: 0s - loss: 0.6304 - accuracy: 0.68 - ETA: 0s - loss: 0.6299 - accuracy: 0.68 - ETA: 0s - loss: 0.6320 - accuracy: 0.68 - ETA: 0s - loss: 0.6330 - accuracy: 0.68 - ETA: 0s - loss: 0.6337 - accuracy: 0.67 - 1s 48us/step - loss: 0.6336 - accuracy: 0.6798 - val_loss: 0.7406 - val_accuracy: 0.6228\n",
      "Epoch 62/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.67 - ETA: 0s - loss: 0.6268 - accuracy: 0.68 - ETA: 0s - loss: 0.6276 - accuracy: 0.68 - ETA: 0s - loss: 0.6275 - accuracy: 0.68 - ETA: 0s - loss: 0.6286 - accuracy: 0.68 - ETA: 0s - loss: 0.6297 - accuracy: 0.67 - ETA: 0s - loss: 0.6286 - accuracy: 0.68 - ETA: 0s - loss: 0.6282 - accuracy: 0.68 - 1s 48us/step - loss: 0.6279 - accuracy: 0.6812 - val_loss: 0.7361 - val_accuracy: 0.6228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6356 - accuracy: 0.66 - ETA: 0s - loss: 0.6245 - accuracy: 0.67 - ETA: 0s - loss: 0.6232 - accuracy: 0.67 - ETA: 0s - loss: 0.6248 - accuracy: 0.67 - ETA: 0s - loss: 0.6230 - accuracy: 0.68 - ETA: 0s - loss: 0.6240 - accuracy: 0.68 - ETA: 0s - loss: 0.6227 - accuracy: 0.68 - ETA: 0s - loss: 0.6231 - accuracy: 0.68 - 1s 49us/step - loss: 0.6226 - accuracy: 0.6817 - val_loss: 0.7335 - val_accuracy: 0.6226\n",
      "Epoch 64/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.68 - ETA: 0s - loss: 0.6214 - accuracy: 0.68 - ETA: 0s - loss: 0.6211 - accuracy: 0.68 - ETA: 0s - loss: 0.6197 - accuracy: 0.67 - ETA: 0s - loss: 0.6192 - accuracy: 0.67 - ETA: 0s - loss: 0.6185 - accuracy: 0.68 - ETA: 0s - loss: 0.6186 - accuracy: 0.68 - ETA: 0s - loss: 0.6186 - accuracy: 0.68 - 1s 47us/step - loss: 0.6175 - accuracy: 0.6822 - val_loss: 0.7318 - val_accuracy: 0.6223\n",
      "Epoch 65/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.66 - ETA: 0s - loss: 0.6122 - accuracy: 0.68 - ETA: 0s - loss: 0.6134 - accuracy: 0.68 - ETA: 0s - loss: 0.6108 - accuracy: 0.68 - ETA: 0s - loss: 0.6130 - accuracy: 0.68 - ETA: 0s - loss: 0.6132 - accuracy: 0.68 - ETA: 0s - loss: 0.6131 - accuracy: 0.68 - ETA: 0s - loss: 0.6126 - accuracy: 0.68 - 1s 46us/step - loss: 0.6127 - accuracy: 0.6824 - val_loss: 0.7279 - val_accuracy: 0.6233\n",
      "Epoch 66/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6107 - accuracy: 0.68 - ETA: 0s - loss: 0.6088 - accuracy: 0.68 - ETA: 0s - loss: 0.6062 - accuracy: 0.68 - ETA: 0s - loss: 0.6099 - accuracy: 0.68 - ETA: 0s - loss: 0.6094 - accuracy: 0.68 - ETA: 0s - loss: 0.6087 - accuracy: 0.68 - ETA: 0s - loss: 0.6075 - accuracy: 0.68 - ETA: 0s - loss: 0.6082 - accuracy: 0.68 - 1s 47us/step - loss: 0.6080 - accuracy: 0.6830 - val_loss: 0.7243 - val_accuracy: 0.6233\n",
      "Epoch 67/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.68 - ETA: 0s - loss: 0.6020 - accuracy: 0.68 - ETA: 0s - loss: 0.6027 - accuracy: 0.68 - ETA: 0s - loss: 0.6012 - accuracy: 0.68 - ETA: 0s - loss: 0.6020 - accuracy: 0.68 - ETA: 0s - loss: 0.6013 - accuracy: 0.68 - ETA: 0s - loss: 0.6024 - accuracy: 0.68 - ETA: 0s - loss: 0.6036 - accuracy: 0.68 - 1s 49us/step - loss: 0.6037 - accuracy: 0.6837 - val_loss: 0.7222 - val_accuracy: 0.6239\n",
      "Epoch 68/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.68 - ETA: 0s - loss: 0.6069 - accuracy: 0.68 - ETA: 0s - loss: 0.6043 - accuracy: 0.68 - ETA: 0s - loss: 0.6059 - accuracy: 0.68 - ETA: 0s - loss: 0.6031 - accuracy: 0.68 - ETA: 0s - loss: 0.6008 - accuracy: 0.68 - ETA: 0s - loss: 0.6015 - accuracy: 0.68 - ETA: 0s - loss: 0.6006 - accuracy: 0.68 - 1s 49us/step - loss: 0.5995 - accuracy: 0.6843 - val_loss: 0.7189 - val_accuracy: 0.6249\n",
      "Epoch 69/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.67 - ETA: 0s - loss: 0.5949 - accuracy: 0.68 - ETA: 0s - loss: 0.5965 - accuracy: 0.68 - ETA: 0s - loss: 0.5944 - accuracy: 0.68 - ETA: 0s - loss: 0.5933 - accuracy: 0.68 - ETA: 0s - loss: 0.5938 - accuracy: 0.68 - ETA: 0s - loss: 0.5957 - accuracy: 0.68 - ETA: 0s - loss: 0.5958 - accuracy: 0.68 - 1s 49us/step - loss: 0.5955 - accuracy: 0.6847 - val_loss: 0.7176 - val_accuracy: 0.6239\n",
      "Epoch 70/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.70 - ETA: 0s - loss: 0.5916 - accuracy: 0.68 - ETA: 0s - loss: 0.5961 - accuracy: 0.68 - ETA: 0s - loss: 0.5962 - accuracy: 0.68 - ETA: 0s - loss: 0.5943 - accuracy: 0.68 - ETA: 0s - loss: 0.5916 - accuracy: 0.68 - ETA: 0s - loss: 0.5914 - accuracy: 0.68 - ETA: 0s - loss: 0.5912 - accuracy: 0.68 - 1s 48us/step - loss: 0.5917 - accuracy: 0.6854 - val_loss: 0.7155 - val_accuracy: 0.6236\n",
      "Epoch 71/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5886 - accuracy: 0.67 - ETA: 0s - loss: 0.5903 - accuracy: 0.68 - ETA: 0s - loss: 0.5906 - accuracy: 0.68 - ETA: 0s - loss: 0.5930 - accuracy: 0.68 - ETA: 0s - loss: 0.5921 - accuracy: 0.68 - ETA: 0s - loss: 0.5906 - accuracy: 0.68 - ETA: 0s - loss: 0.5883 - accuracy: 0.68 - ETA: 0s - loss: 0.5874 - accuracy: 0.68 - 1s 46us/step - loss: 0.5881 - accuracy: 0.6858 - val_loss: 0.7122 - val_accuracy: 0.6256\n",
      "Epoch 72/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5893 - accuracy: 0.67 - ETA: 0s - loss: 0.5838 - accuracy: 0.67 - ETA: 0s - loss: 0.5807 - accuracy: 0.68 - ETA: 0s - loss: 0.5831 - accuracy: 0.68 - ETA: 0s - loss: 0.5868 - accuracy: 0.68 - ETA: 0s - loss: 0.5849 - accuracy: 0.68 - ETA: 0s - loss: 0.5854 - accuracy: 0.68 - ETA: 0s - loss: 0.5846 - accuracy: 0.68 - 1s 47us/step - loss: 0.5845 - accuracy: 0.6862 - val_loss: 0.7094 - val_accuracy: 0.6272\n",
      "Epoch 73/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.68 - ETA: 0s - loss: 0.5816 - accuracy: 0.68 - ETA: 0s - loss: 0.5791 - accuracy: 0.68 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.68 - ETA: 0s - loss: 0.5817 - accuracy: 0.68 - ETA: 0s - loss: 0.5809 - accuracy: 0.68 - ETA: 0s - loss: 0.5816 - accuracy: 0.68 - 1s 47us/step - loss: 0.5814 - accuracy: 0.6865 - val_loss: 0.7098 - val_accuracy: 0.6236\n",
      "Epoch 74/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.67 - ETA: 0s - loss: 0.5811 - accuracy: 0.68 - ETA: 0s - loss: 0.5811 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5797 - accuracy: 0.68 - ETA: 0s - loss: 0.5794 - accuracy: 0.68 - ETA: 0s - loss: 0.5787 - accuracy: 0.68 - ETA: 0s - loss: 0.5782 - accuracy: 0.68 - ETA: 0s - loss: 0.5789 - accuracy: 0.68 - 1s 49us/step - loss: 0.5783 - accuracy: 0.6870 - val_loss: 0.7078 - val_accuracy: 0.6239\n",
      "Epoch 75/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5808 - accuracy: 0.68 - ETA: 0s - loss: 0.5763 - accuracy: 0.68 - ETA: 0s - loss: 0.5753 - accuracy: 0.68 - ETA: 0s - loss: 0.5736 - accuracy: 0.69 - ETA: 0s - loss: 0.5742 - accuracy: 0.68 - ETA: 0s - loss: 0.5740 - accuracy: 0.68 - 1s 47us/step - loss: 0.5752 - accuracy: 0.6874 - val_loss: 0.7066 - val_accuracy: 0.6228\n",
      "Epoch 76/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5827 - accuracy: 0.67 - ETA: 0s - loss: 0.5803 - accuracy: 0.67 - ETA: 0s - loss: 0.5755 - accuracy: 0.68 - ETA: 0s - loss: 0.5778 - accuracy: 0.68 - ETA: 0s - loss: 0.5766 - accuracy: 0.68 - ETA: 0s - loss: 0.5760 - accuracy: 0.68 - ETA: 0s - loss: 0.5753 - accuracy: 0.68 - ETA: 0s - loss: 0.5736 - accuracy: 0.68 - ETA: 0s - loss: 0.5738 - accuracy: 0.68 - ETA: 0s - loss: 0.5724 - accuracy: 0.68 - 1s 50us/step - loss: 0.5723 - accuracy: 0.6878 - val_loss: 0.7035 - val_accuracy: 0.6254\n",
      "Epoch 77/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.67 - ETA: 0s - loss: 0.5750 - accuracy: 0.68 - ETA: 0s - loss: 0.5718 - accuracy: 0.68 - ETA: 0s - loss: 0.5749 - accuracy: 0.68 - ETA: 0s - loss: 0.5723 - accuracy: 0.68 - ETA: 0s - loss: 0.5674 - accuracy: 0.68 - ETA: 0s - loss: 0.5689 - accuracy: 0.69 - ETA: 0s - loss: 0.5708 - accuracy: 0.68 - ETA: 0s - loss: 0.5703 - accuracy: 0.68 - ETA: 0s - loss: 0.5705 - accuracy: 0.68 - ETA: 0s - loss: 0.5700 - accuracy: 0.68 - 1s 52us/step - loss: 0.5695 - accuracy: 0.6880 - val_loss: 0.7019 - val_accuracy: 0.6256\n",
      "Epoch 78/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.68 - ETA: 0s - loss: 0.5581 - accuracy: 0.69 - ETA: 0s - loss: 0.5611 - accuracy: 0.69 - ETA: 0s - loss: 0.5658 - accuracy: 0.69 - ETA: 0s - loss: 0.5677 - accuracy: 0.68 - ETA: 0s - loss: 0.5667 - accuracy: 0.68 - ETA: 0s - loss: 0.5670 - accuracy: 0.68 - ETA: 0s - loss: 0.5672 - accuracy: 0.68 - 1s 47us/step - loss: 0.5669 - accuracy: 0.6884 - val_loss: 0.7016 - val_accuracy: 0.6251\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.67 - ETA: 0s - loss: 0.5728 - accuracy: 0.67 - ETA: 0s - loss: 0.5659 - accuracy: 0.68 - ETA: 0s - loss: 0.5693 - accuracy: 0.67 - ETA: 0s - loss: 0.5670 - accuracy: 0.68 - ETA: 0s - loss: 0.5666 - accuracy: 0.68 - ETA: 0s - loss: 0.5653 - accuracy: 0.68 - ETA: 0s - loss: 0.5645 - accuracy: 0.68 - 1s 49us/step - loss: 0.5645 - accuracy: 0.6886 - val_loss: 0.7011 - val_accuracy: 0.6251\n",
      "Epoch 80/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.68 - ETA: 0s - loss: 0.5616 - accuracy: 0.69 - ETA: 0s - loss: 0.5666 - accuracy: 0.68 - ETA: 0s - loss: 0.5632 - accuracy: 0.68 - ETA: 0s - loss: 0.5612 - accuracy: 0.68 - ETA: 0s - loss: 0.5634 - accuracy: 0.68 - ETA: 0s - loss: 0.5612 - accuracy: 0.68 - ETA: 0s - loss: 0.5616 - accuracy: 0.68 - 1s 47us/step - loss: 0.5620 - accuracy: 0.6889 - val_loss: 0.6995 - val_accuracy: 0.6256\n",
      "Epoch 81/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5689 - accuracy: 0.68 - ETA: 0s - loss: 0.5503 - accuracy: 0.69 - ETA: 0s - loss: 0.5569 - accuracy: 0.69 - ETA: 0s - loss: 0.5610 - accuracy: 0.68 - ETA: 0s - loss: 0.5582 - accuracy: 0.69 - ETA: 0s - loss: 0.5593 - accuracy: 0.69 - ETA: 0s - loss: 0.5602 - accuracy: 0.68 - ETA: 0s - loss: 0.5594 - accuracy: 0.68 - 1s 47us/step - loss: 0.5598 - accuracy: 0.6889 - val_loss: 0.7001 - val_accuracy: 0.6241\n",
      "Epoch 82/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.64 - ETA: 0s - loss: 0.5686 - accuracy: 0.67 - ETA: 0s - loss: 0.5658 - accuracy: 0.67 - ETA: 0s - loss: 0.5630 - accuracy: 0.68 - ETA: 0s - loss: 0.5629 - accuracy: 0.68 - ETA: 0s - loss: 0.5607 - accuracy: 0.68 - ETA: 0s - loss: 0.5603 - accuracy: 0.68 - ETA: 0s - loss: 0.5581 - accuracy: 0.68 - 1s 49us/step - loss: 0.5576 - accuracy: 0.6891 - val_loss: 0.6975 - val_accuracy: 0.6261\n",
      "Epoch 83/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.69 - ETA: 0s - loss: 0.5592 - accuracy: 0.69 - ETA: 0s - loss: 0.5591 - accuracy: 0.69 - ETA: 0s - loss: 0.5595 - accuracy: 0.69 - ETA: 0s - loss: 0.5601 - accuracy: 0.69 - ETA: 0s - loss: 0.5596 - accuracy: 0.69 - ETA: 0s - loss: 0.5565 - accuracy: 0.69 - ETA: 0s - loss: 0.5558 - accuracy: 0.68 - 1s 48us/step - loss: 0.5555 - accuracy: 0.6893 - val_loss: 0.6958 - val_accuracy: 0.6251\n",
      "Epoch 84/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.71 - ETA: 0s - loss: 0.5501 - accuracy: 0.69 - ETA: 0s - loss: 0.5502 - accuracy: 0.69 - ETA: 0s - loss: 0.5519 - accuracy: 0.69 - ETA: 0s - loss: 0.5548 - accuracy: 0.69 - ETA: 0s - loss: 0.5554 - accuracy: 0.69 - ETA: 0s - loss: 0.5544 - accuracy: 0.69 - ETA: 0s - loss: 0.5545 - accuracy: 0.69 - 1s 47us/step - loss: 0.5535 - accuracy: 0.6898 - val_loss: 0.6939 - val_accuracy: 0.6259\n",
      "Epoch 85/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.69 - ETA: 0s - loss: 0.5511 - accuracy: 0.68 - ETA: 0s - loss: 0.5535 - accuracy: 0.68 - ETA: 0s - loss: 0.5523 - accuracy: 0.68 - ETA: 0s - loss: 0.5524 - accuracy: 0.68 - ETA: 0s - loss: 0.5524 - accuracy: 0.68 - ETA: 0s - loss: 0.5513 - accuracy: 0.68 - ETA: 0s - loss: 0.5510 - accuracy: 0.69 - 1s 49us/step - loss: 0.5514 - accuracy: 0.6900 - val_loss: 0.6931 - val_accuracy: 0.6259\n",
      "Epoch 86/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5394 - accuracy: 0.68 - ETA: 0s - loss: 0.5427 - accuracy: 0.68 - ETA: 0s - loss: 0.5494 - accuracy: 0.68 - ETA: 0s - loss: 0.5492 - accuracy: 0.68 - ETA: 0s - loss: 0.5493 - accuracy: 0.68 - ETA: 0s - loss: 0.5478 - accuracy: 0.68 - ETA: 0s - loss: 0.5495 - accuracy: 0.68 - ETA: 0s - loss: 0.5487 - accuracy: 0.68 - ETA: 0s - loss: 0.5505 - accuracy: 0.68 - ETA: 0s - loss: 0.5490 - accuracy: 0.68 - ETA: 0s - loss: 0.5484 - accuracy: 0.69 - ETA: 0s - loss: 0.5489 - accuracy: 0.68 - ETA: 0s - loss: 0.5490 - accuracy: 0.68 - 1s 57us/step - loss: 0.5495 - accuracy: 0.6900 - val_loss: 0.6926 - val_accuracy: 0.6256\n",
      "Epoch 87/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.70 - ETA: 0s - loss: 0.5456 - accuracy: 0.68 - ETA: 0s - loss: 0.5434 - accuracy: 0.69 - ETA: 0s - loss: 0.5495 - accuracy: 0.68 - ETA: 0s - loss: 0.5440 - accuracy: 0.69 - ETA: 0s - loss: 0.5481 - accuracy: 0.68 - ETA: 0s - loss: 0.5476 - accuracy: 0.69 - ETA: 0s - loss: 0.5479 - accuracy: 0.69 - ETA: 0s - loss: 0.5464 - accuracy: 0.69 - ETA: 0s - loss: 0.5477 - accuracy: 0.69 - 1s 51us/step - loss: 0.5479 - accuracy: 0.6902 - val_loss: 0.6915 - val_accuracy: 0.6261\n",
      "Epoch 88/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5468 - accuracy: 0.68 - ETA: 0s - loss: 0.5456 - accuracy: 0.68 - ETA: 0s - loss: 0.5474 - accuracy: 0.68 - ETA: 0s - loss: 0.5483 - accuracy: 0.69 - ETA: 0s - loss: 0.5482 - accuracy: 0.68 - ETA: 0s - loss: 0.5458 - accuracy: 0.68 - ETA: 0s - loss: 0.5445 - accuracy: 0.69 - ETA: 0s - loss: 0.5463 - accuracy: 0.68 - ETA: 0s - loss: 0.5461 - accuracy: 0.69 - 1s 50us/step - loss: 0.5461 - accuracy: 0.6904 - val_loss: 0.6908 - val_accuracy: 0.6259\n",
      "Epoch 89/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.70 - ETA: 0s - loss: 0.5503 - accuracy: 0.68 - ETA: 0s - loss: 0.5476 - accuracy: 0.68 - ETA: 0s - loss: 0.5463 - accuracy: 0.68 - ETA: 0s - loss: 0.5464 - accuracy: 0.68 - ETA: 0s - loss: 0.5476 - accuracy: 0.68 - ETA: 0s - loss: 0.5471 - accuracy: 0.68 - ETA: 0s - loss: 0.5469 - accuracy: 0.68 - ETA: 0s - loss: 0.5453 - accuracy: 0.69 - ETA: 0s - loss: 0.5460 - accuracy: 0.69 - ETA: 0s - loss: 0.5453 - accuracy: 0.69 - ETA: 0s - loss: 0.5447 - accuracy: 0.69 - 1s 56us/step - loss: 0.5444 - accuracy: 0.6906 - val_loss: 0.6918 - val_accuracy: 0.6267\n",
      "Epoch 90/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.70 - ETA: 0s - loss: 0.5377 - accuracy: 0.69 - ETA: 0s - loss: 0.5408 - accuracy: 0.69 - ETA: 0s - loss: 0.5379 - accuracy: 0.69 - ETA: 0s - loss: 0.5349 - accuracy: 0.69 - ETA: 0s - loss: 0.5411 - accuracy: 0.68 - ETA: 0s - loss: 0.5412 - accuracy: 0.68 - ETA: 0s - loss: 0.5403 - accuracy: 0.68 - ETA: 0s - loss: 0.5417 - accuracy: 0.68 - ETA: 0s - loss: 0.5418 - accuracy: 0.69 - ETA: 0s - loss: 0.5426 - accuracy: 0.69 - 1s 53us/step - loss: 0.5429 - accuracy: 0.6907 - val_loss: 0.6916 - val_accuracy: 0.6267\n"
     ]
    }
   ],
   "source": [
    "hist2 = model2.fit(x_train, y_train, batch_size=1000, validation_data=(x_test, y_test), epochs=200,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model3.add(GlobalAveragePooling1D())\n",
    "model3.add(Dense(4, activation='softmax'))\n",
    "model3.add(Dense(5, activation='softmax'))\n",
    "model3.add(Dense(4, activation='softmax'))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/200\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 1.1155 - accuracy: 0.29 - ETA: 0s - loss: 1.1134 - accuracy: 0.30 - ETA: 0s - loss: 1.1130 - accuracy: 0.29 - ETA: 0s - loss: 1.1124 - accuracy: 0.28 - ETA: 0s - loss: 1.1118 - accuracy: 0.28 - ETA: 0s - loss: 1.1110 - accuracy: 0.28 - ETA: 0s - loss: 1.1106 - accuracy: 0.28 - ETA: 0s - loss: 1.1098 - accuracy: 0.28 - 1s 55us/step - loss: 1.1097 - accuracy: 0.2883 - val_loss: 1.1049 - val_accuracy: 0.2858\n",
      "Epoch 2/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.1052 - accuracy: 0.30 - ETA: 0s - loss: 1.1053 - accuracy: 0.28 - ETA: 0s - loss: 1.1045 - accuracy: 0.28 - ETA: 0s - loss: 1.1041 - accuracy: 0.29 - ETA: 0s - loss: 1.1037 - accuracy: 0.29 - ETA: 0s - loss: 1.1033 - accuracy: 0.28 - ETA: 0s - loss: 1.1029 - accuracy: 0.28 - ETA: 0s - loss: 1.1025 - accuracy: 0.28 - 1s 49us/step - loss: 1.1024 - accuracy: 0.2892 - val_loss: 1.0990 - val_accuracy: 0.3156\n",
      "Epoch 3/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0990 - accuracy: 0.31 - ETA: 0s - loss: 1.0988 - accuracy: 0.31 - ETA: 0s - loss: 1.0987 - accuracy: 0.33 - ETA: 0s - loss: 1.0984 - accuracy: 0.36 - ETA: 0s - loss: 1.0980 - accuracy: 0.38 - ETA: 0s - loss: 1.0977 - accuracy: 0.38 - ETA: 0s - loss: 1.0974 - accuracy: 0.39 - ETA: 0s - loss: 1.0970 - accuracy: 0.39 - ETA: 0s - loss: 1.0969 - accuracy: 0.39 - 1s 49us/step - loss: 1.0968 - accuracy: 0.3940 - val_loss: 1.0948 - val_accuracy: 0.3971\n",
      "Epoch 4/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0955 - accuracy: 0.39 - ETA: 0s - loss: 1.0940 - accuracy: 0.40 - ETA: 0s - loss: 1.0936 - accuracy: 0.40 - ETA: 0s - loss: 1.0938 - accuracy: 0.40 - ETA: 0s - loss: 1.0936 - accuracy: 0.40 - ETA: 0s - loss: 1.0938 - accuracy: 0.40 - ETA: 0s - loss: 1.0935 - accuracy: 0.40 - ETA: 0s - loss: 1.0933 - accuracy: 0.40 - 1s 49us/step - loss: 1.0928 - accuracy: 0.4051 - val_loss: 1.0921 - val_accuracy: 0.3971\n",
      "Epoch 5/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0904 - accuracy: 0.41 - ETA: 0s - loss: 1.0912 - accuracy: 0.40 - ETA: 0s - loss: 1.0902 - accuracy: 0.41 - ETA: 0s - loss: 1.0903 - accuracy: 0.40 - ETA: 0s - loss: 1.0900 - accuracy: 0.41 - ETA: 0s - loss: 1.0900 - accuracy: 0.40 - ETA: 0s - loss: 1.0900 - accuracy: 0.40 - ETA: 0s - loss: 1.0901 - accuracy: 0.40 - ETA: 0s - loss: 1.0903 - accuracy: 0.40 - 1s 49us/step - loss: 1.0904 - accuracy: 0.4051 - val_loss: 1.0904 - val_accuracy: 0.3971\n",
      "Epoch 6/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0918 - accuracy: 0.38 - ETA: 0s - loss: 1.0904 - accuracy: 0.39 - ETA: 0s - loss: 1.0899 - accuracy: 0.39 - ETA: 0s - loss: 1.0891 - accuracy: 0.40 - ETA: 0s - loss: 1.0886 - accuracy: 0.40 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0884 - accuracy: 0.40 - ETA: 0s - loss: 1.0884 - accuracy: 0.40 - 1s 51us/step - loss: 1.0887 - accuracy: 0.4051 - val_loss: 1.0896 - val_accuracy: 0.3971\n",
      "Epoch 7/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0866 - accuracy: 0.41 - ETA: 0s - loss: 1.0888 - accuracy: 0.40 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0878 - accuracy: 0.40 - ETA: 0s - loss: 1.0879 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0876 - accuracy: 0.40 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - 1s 52us/step - loss: 1.0879 - accuracy: 0.4051 - val_loss: 1.0892 - val_accuracy: 0.3971\n",
      "Epoch 8/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0865 - accuracy: 0.40 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - ETA: 0s - loss: 1.0865 - accuracy: 0.40 - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - 1s 49us/step - loss: 1.0874 - accuracy: 0.4051 - val_loss: 1.0891 - val_accuracy: 0.3971\n",
      "Epoch 9/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0863 - accuracy: 0.40 - ETA: 0s - loss: 1.0903 - accuracy: 0.39 - ETA: 0s - loss: 1.0889 - accuracy: 0.39 - ETA: 0s - loss: 1.0884 - accuracy: 0.39 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - ETA: 0s - loss: 1.0874 - accuracy: 0.40 - ETA: 0s - loss: 1.0878 - accuracy: 0.40 - ETA: 0s - loss: 1.0876 - accuracy: 0.40 - 1s 49us/step - loss: 1.0872 - accuracy: 0.4051 - val_loss: 1.0890 - val_accuracy: 0.3971\n",
      "Epoch 10/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0890 - accuracy: 0.39 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0858 - accuracy: 0.40 - ETA: 0s - loss: 1.0864 - accuracy: 0.40 - ETA: 0s - loss: 1.0883 - accuracy: 0.39 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - ETA: 0s - loss: 1.0883 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - 1s 53us/step - loss: 1.0871 - accuracy: 0.4051 - val_loss: 1.0890 - val_accuracy: 0.3971\n",
      "Epoch 11/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0919 - accuracy: 0.39 - ETA: 0s - loss: 1.0860 - accuracy: 0.41 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0878 - accuracy: 0.40 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0862 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - 1s 51us/step - loss: 1.0871 - accuracy: 0.4051 - val_loss: 1.0891 - val_accuracy: 0.3971\n"
     ]
    }
   ],
   "source": [
    "hist3 = model3.fit(x_train, y_train, batch_size=1000, validation_data=(x_test, y_test), epochs=200,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model4.add(GlobalAveragePooling1D())\n",
    "model4.add(Dense(3, activation='softmax'))\n",
    "model4.add(Dense(4, activation='softmax'))\n",
    "model4.add(Dense(5, activation='softmax'))\n",
    "model4.add(Dense(4, activation='softmax'))\n",
    "model4.add(Dense(3, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/200\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 1.1280 - accuracy: 0.29 - ETA: 1s - loss: 1.1253 - accuracy: 0.30 - ETA: 0s - loss: 1.1245 - accuracy: 0.30 - ETA: 0s - loss: 1.1219 - accuracy: 0.30 - ETA: 0s - loss: 1.1215 - accuracy: 0.30 - ETA: 0s - loss: 1.1216 - accuracy: 0.30 - ETA: 0s - loss: 1.1217 - accuracy: 0.30 - ETA: 0s - loss: 1.1212 - accuracy: 0.30 - ETA: 0s - loss: 1.1209 - accuracy: 0.30 - 1s 58us/step - loss: 1.1204 - accuracy: 0.3066 - val_loss: 1.1122 - val_accuracy: 0.3172\n",
      "Epoch 2/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.1189 - accuracy: 0.29 - ETA: 0s - loss: 1.1151 - accuracy: 0.30 - ETA: 0s - loss: 1.1149 - accuracy: 0.30 - ETA: 0s - loss: 1.1144 - accuracy: 0.30 - ETA: 0s - loss: 1.1147 - accuracy: 0.30 - ETA: 0s - loss: 1.1126 - accuracy: 0.30 - ETA: 0s - loss: 1.1128 - accuracy: 0.30 - ETA: 0s - loss: 1.1127 - accuracy: 0.30 - ETA: 0s - loss: 1.1122 - accuracy: 0.30 - ETA: 0s - loss: 1.1119 - accuracy: 0.30 - 1s 50us/step - loss: 1.1117 - accuracy: 0.3066 - val_loss: 1.1052 - val_accuracy: 0.3172\n",
      "Epoch 3/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.1046 - accuracy: 0.31 - ETA: 0s - loss: 1.1043 - accuracy: 0.31 - ETA: 0s - loss: 1.1060 - accuracy: 0.31 - ETA: 0s - loss: 1.1058 - accuracy: 0.30 - ETA: 0s - loss: 1.1050 - accuracy: 0.31 - ETA: 0s - loss: 1.1046 - accuracy: 0.31 - ETA: 0s - loss: 1.1047 - accuracy: 0.31 - ETA: 0s - loss: 1.1044 - accuracy: 0.31 - ETA: 0s - loss: 1.1046 - accuracy: 0.30 - 1s 50us/step - loss: 1.1047 - accuracy: 0.3066 - val_loss: 1.1000 - val_accuracy: 0.3172\n",
      "Epoch 4/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.1020 - accuracy: 0.30 - ETA: 0s - loss: 1.0993 - accuracy: 0.30 - ETA: 0s - loss: 1.1005 - accuracy: 0.30 - ETA: 0s - loss: 1.1010 - accuracy: 0.30 - ETA: 0s - loss: 1.1006 - accuracy: 0.30 - ETA: 0s - loss: 1.1007 - accuracy: 0.30 - ETA: 0s - loss: 1.1006 - accuracy: 0.30 - ETA: 0s - loss: 1.1004 - accuracy: 0.30 - ETA: 0s - loss: 1.1008 - accuracy: 0.30 - ETA: 0s - loss: 1.1008 - accuracy: 0.30 - ETA: 0s - loss: 1.1001 - accuracy: 0.30 - ETA: 0s - loss: 1.0998 - accuracy: 0.30 - 1s 53us/step - loss: 1.0995 - accuracy: 0.3066 - val_loss: 1.0960 - val_accuracy: 0.3172\n",
      "Epoch 5/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0915 - accuracy: 0.31 - ETA: 0s - loss: 1.0926 - accuracy: 0.31 - ETA: 0s - loss: 1.0950 - accuracy: 0.31 - ETA: 0s - loss: 1.0952 - accuracy: 0.31 - ETA: 0s - loss: 1.0945 - accuracy: 0.31 - ETA: 0s - loss: 1.0945 - accuracy: 0.31 - ETA: 0s - loss: 1.0946 - accuracy: 0.31 - ETA: 0s - loss: 1.0948 - accuracy: 0.31 - ETA: 0s - loss: 1.0955 - accuracy: 0.30 - ETA: 0s - loss: 1.0958 - accuracy: 0.30 - ETA: 0s - loss: 1.0958 - accuracy: 0.30 - ETA: 0s - loss: 1.0955 - accuracy: 0.30 - ETA: 0s - loss: 1.0954 - accuracy: 0.30 - 1s 57us/step - loss: 1.0955 - accuracy: 0.3066 - val_loss: 1.0933 - val_accuracy: 0.3971\n",
      "Epoch 6/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0913 - accuracy: 0.42 - ETA: 0s - loss: 1.0901 - accuracy: 0.41 - ETA: 0s - loss: 1.0939 - accuracy: 0.40 - ETA: 0s - loss: 1.0942 - accuracy: 0.40 - ETA: 0s - loss: 1.0949 - accuracy: 0.40 - ETA: 0s - loss: 1.0944 - accuracy: 0.40 - ETA: 0s - loss: 1.0943 - accuracy: 0.40 - ETA: 0s - loss: 1.0938 - accuracy: 0.40 - ETA: 0s - loss: 1.0938 - accuracy: 0.40 - ETA: 0s - loss: 1.0933 - accuracy: 0.40 - 1s 52us/step - loss: 1.0927 - accuracy: 0.4051 - val_loss: 1.0915 - val_accuracy: 0.3971\n",
      "Epoch 7/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0927 - accuracy: 0.41 - ETA: 0s - loss: 1.0939 - accuracy: 0.39 - ETA: 0s - loss: 1.0938 - accuracy: 0.39 - ETA: 0s - loss: 1.0943 - accuracy: 0.39 - ETA: 0s - loss: 1.0937 - accuracy: 0.39 - ETA: 0s - loss: 1.0921 - accuracy: 0.39 - ETA: 0s - loss: 1.0915 - accuracy: 0.40 - ETA: 0s - loss: 1.0914 - accuracy: 0.39 - ETA: 0s - loss: 1.0914 - accuracy: 0.40 - ETA: 0s - loss: 1.0911 - accuracy: 0.40 - ETA: 0s - loss: 1.0913 - accuracy: 0.40 - ETA: 0s - loss: 1.0912 - accuracy: 0.40 - ETA: 0s - loss: 1.0909 - accuracy: 0.40 - 1s 55us/step - loss: 1.0907 - accuracy: 0.4051 - val_loss: 1.0903 - val_accuracy: 0.3971\n",
      "Epoch 8/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0903 - accuracy: 0.39 - ETA: 0s - loss: 1.0912 - accuracy: 0.39 - ETA: 0s - loss: 1.0900 - accuracy: 0.40 - ETA: 0s - loss: 1.0895 - accuracy: 0.40 - ETA: 0s - loss: 1.0905 - accuracy: 0.40 - ETA: 0s - loss: 1.0893 - accuracy: 0.40 - ETA: 0s - loss: 1.0897 - accuracy: 0.40 - ETA: 0s - loss: 1.0892 - accuracy: 0.40 - 1s 49us/step - loss: 1.0894 - accuracy: 0.4051 - val_loss: 1.0896 - val_accuracy: 0.3971\n",
      "Epoch 9/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0880 - accuracy: 0.39 - ETA: 0s - loss: 1.0892 - accuracy: 0.40 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - ETA: 0s - loss: 1.0885 - accuracy: 0.40 - ETA: 0s - loss: 1.0883 - accuracy: 0.40 - ETA: 0s - loss: 1.0890 - accuracy: 0.40 - ETA: 0s - loss: 1.0893 - accuracy: 0.40 - ETA: 0s - loss: 1.0891 - accuracy: 0.40 - ETA: 0s - loss: 1.0888 - accuracy: 0.40 - 1s 51us/step - loss: 1.0885 - accuracy: 0.4051 - val_loss: 1.0892 - val_accuracy: 0.3971\n",
      "Epoch 10/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0904 - accuracy: 0.39 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - ETA: 0s - loss: 1.0888 - accuracy: 0.40 - ETA: 0s - loss: 1.0886 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0878 - accuracy: 0.40 - ETA: 0s - loss: 1.0878 - accuracy: 0.40 - 1s 49us/step - loss: 1.0879 - accuracy: 0.4051 - val_loss: 1.0890 - val_accuracy: 0.3971\n",
      "Epoch 11/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0884 - accuracy: 0.40 - ETA: 0s - loss: 1.0888 - accuracy: 0.40 - ETA: 0s - loss: 1.0885 - accuracy: 0.40 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - 1s 48us/step - loss: 1.0876 - accuracy: 0.4051 - val_loss: 1.0889 - val_accuracy: 0.3971\n",
      "Epoch 12/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0935 - accuracy: 0.37 - ETA: 0s - loss: 1.0897 - accuracy: 0.39 - ETA: 0s - loss: 1.0884 - accuracy: 0.40 - ETA: 0s - loss: 1.0889 - accuracy: 0.40 - ETA: 0s - loss: 1.0885 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0881 - accuracy: 0.40 - ETA: 0s - loss: 1.0874 - accuracy: 0.40 - 1s 46us/step - loss: 1.0874 - accuracy: 0.4051 - val_loss: 1.0889 - val_accuracy: 0.3971\n",
      "Epoch 13/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0844 - accuracy: 0.40 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - ETA: 0s - loss: 1.0876 - accuracy: 0.40 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - ETA: 0s - loss: 1.0877 - accuracy: 0.40 - 1s 46us/step - loss: 1.0873 - accuracy: 0.4051 - val_loss: 1.0889 - val_accuracy: 0.3971\n",
      "Epoch 14/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0814 - accuracy: 0.42 - ETA: 0s - loss: 1.0823 - accuracy: 0.42 - ETA: 0s - loss: 1.0859 - accuracy: 0.40 - ETA: 0s - loss: 1.0860 - accuracy: 0.40 - ETA: 0s - loss: 1.0868 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0874 - accuracy: 0.40 - 1s 45us/step - loss: 1.0872 - accuracy: 0.4051 - val_loss: 1.0890 - val_accuracy: 0.3971\n"
     ]
    }
   ],
   "source": [
    "hist4 = model4.fit(x_train, y_train, batch_size=1000, validation_data=(x_test, y_test), epochs=200,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model5.add(GlobalAveragePooling1D())\n",
    "model5.add(Dense(2, activation='softmax'))\n",
    "model5.add(Dense(3, activation='softmax'))\n",
    "model5.add(Dense(4, activation='softmax'))\n",
    "model5.add(Dense(5, activation='softmax'))\n",
    "model5.add(Dense(4, activation='softmax'))\n",
    "model5.add(Dense(3, activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/200\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 1.0895 - accuracy: 0.41 - ETA: 1s - loss: 1.0898 - accuracy: 0.40 - ETA: 0s - loss: 1.0877 - accuracy: 0.41 - ETA: 0s - loss: 1.0886 - accuracy: 0.40 - ETA: 0s - loss: 1.0892 - accuracy: 0.40 - ETA: 0s - loss: 1.0884 - accuracy: 0.40 - ETA: 0s - loss: 1.0890 - accuracy: 0.40 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - 1s 56us/step - loss: 1.0887 - accuracy: 0.4051 - val_loss: 1.0891 - val_accuracy: 0.3971\n",
      "Epoch 2/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0915 - accuracy: 0.39 - ETA: 0s - loss: 1.0925 - accuracy: 0.38 - ETA: 0s - loss: 1.0908 - accuracy: 0.39 - ETA: 0s - loss: 1.0904 - accuracy: 0.39 - ETA: 0s - loss: 1.0887 - accuracy: 0.40 - ETA: 0s - loss: 1.0886 - accuracy: 0.40 - ETA: 0s - loss: 1.0880 - accuracy: 0.40 - ETA: 0s - loss: 1.0872 - accuracy: 0.40 - 1s 47us/step - loss: 1.0873 - accuracy: 0.4051 - val_loss: 1.0890 - val_accuracy: 0.3971\n",
      "Epoch 3/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0923 - accuracy: 0.38 - ETA: 0s - loss: 1.0869 - accuracy: 0.40 - ETA: 0s - loss: 1.0877 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - ETA: 0s - loss: 1.0876 - accuracy: 0.40 - ETA: 0s - loss: 1.0874 - accuracy: 0.40 - 1s 47us/step - loss: 1.0871 - accuracy: 0.4051 - val_loss: 1.0891 - val_accuracy: 0.3971\n",
      "Epoch 4/200\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0953 - accuracy: 0.37 - ETA: 0s - loss: 1.0866 - accuracy: 0.40 - ETA: 0s - loss: 1.0882 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - ETA: 0s - loss: 1.0875 - accuracy: 0.40 - ETA: 0s - loss: 1.0867 - accuracy: 0.40 - ETA: 0s - loss: 1.0873 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0870 - accuracy: 0.40 - ETA: 0s - loss: 1.0871 - accuracy: 0.40 - 1s 51us/step - loss: 1.0871 - accuracy: 0.4051 - val_loss: 1.0892 - val_accuracy: 0.3971\n"
     ]
    }
   ],
   "source": [
    "hist5 = model5.fit(x_train, y_train, batch_size=1000, validation_data=(x_test, y_test), epochs=200,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3916/3916 [==============================] - ETA:  - ETA:  - 0s 19us/step\n",
      "3916/3916 [==============================] - ETA:  - ETA:  - 0s 17us/step\n",
      "3916/3916 [==============================] - ETA:  - ETA:  - 0s 17us/step\n",
      "3916/3916 [==============================] - ETA:  - ETA:  - 0s 18us/step\n",
      "3916/3916 [==============================] - ETA:  - ETA:  - 0s 18us/step\n",
      "3916/3916 [==============================] - ETA:  - ETA:  - 0s 19us/step\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_testset, Y_testset)\n",
    "accuracy1 = model1.evaluate(X_testset, Y_testset)\n",
    "accuracy2 = model2.evaluate(X_testset, Y_testset)\n",
    "accuracy3 = model3.evaluate(X_testset, Y_testset)\n",
    "accuracy4 = model4.evaluate(X_testset, Y_testset)\n",
    "accuracy5 = model5.evaluate(X_testset, Y_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 0 hidden layer: 68.44 %\n",
      "Loss for 0 hidden layer: 57.42 %\n",
      "Accuracy for 1 hidden layer: 96.68 %\n",
      "Loss for 1 hidden layer: 14.65 %\n",
      "Accuracy for 2 hidden layers: 68.34 %\n",
      "Loss for 2 hidden layers: 56.57 %\n",
      "Accuracy for 3 hidden layers: 39.79 %\n",
      "Loss for 3 hidden layers: 108.96 %\n",
      "Accuracy for 4 hidden layers: 39.79 %\n",
      "Loss for 4 hidden layers: 108.98 %\n",
      "Accuracy for 5 hidden layers: 39.79 %\n",
      "Loss for 5 hidden layers: 108.98 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for 0 hidden layer: %.2f' % (accuracy[1]*100),'%')\n",
    "print('Loss for 0 hidden layer: %.2f' % (accuracy[0]*100),'%')\n",
    "print('Accuracy for 1 hidden layer: %.2f' % (accuracy1[1]*100),'%')\n",
    "print('Loss for 1 hidden layer: %.2f' % (accuracy1[0]*100),'%')\n",
    "print('Accuracy for 2 hidden layers: %.2f' % (accuracy2[1]*100),'%')\n",
    "print('Loss for 2 hidden layers: %.2f' % (accuracy2[0]*100),'%')\n",
    "print('Accuracy for 3 hidden layers: %.2f' % (accuracy3[1]*100),'%')\n",
    "print('Loss for 3 hidden layers: %.2f' % (accuracy3[0]*100),'%')\n",
    "print('Accuracy for 4 hidden layers: %.2f' % (accuracy4[1]*100),'%')\n",
    "print('Loss for 4 hidden layers: %.2f' % (accuracy4[0]*100),'%')\n",
    "print('Accuracy for 5 hidden layers: %.2f' % (accuracy5[1]*100),'%')\n",
    "print('Loss for 5 hidden layers: %.2f' % (accuracy5[0]*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracies ={0:accuracy[1]*100, 1:accuracy1[1]*100, 2:accuracy2[1]*100, 3:accuracy3[1]*100, 4:accuracy4[1]*100,\n",
    "            5:accuracy5[1]*100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(Accuracies.items()) # sorted by key, return a list of tuples\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "plt.plot(x, y)\n",
    "plt.title('Accuracies for different depths')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Depth')\n",
    "plt.legend(['Accuracy'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop = Sequential()\n",
    "model_drop.add(Dense(12, input_dim=256, activation='sigmoid'))\n",
    "model_drop.add(Dropout(0.2))\n",
    "model_drop.add(Activation('sigmoid'))\n",
    "model_drop.add(Dense(8))\n",
    "model_drop.add(Dropout(0.2))\n",
    "model_drop.add(Activation('sigmoid'))\n",
    "model_drop.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop = Sequential()\n",
    "model_drop.add(Embedding(input_dim=input_dim, output_dim=20))\n",
    "model_drop.add(GlobalAveragePooling1D())\n",
    "model_drop.add(Dropout(0.2))\n",
    "model_drop.add(Dense(4, activation='softmax'))\n",
    "model_drop.add(Dropout(0.2))\n",
    "model_drop.add(Dense(3, activation='softmax'))\n",
    "model_drop.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0026 - accuracy: 0.48 - 0s 27us/step - loss: 1.0023 - accuracy: 0.4891 - val_loss: 1.0012 - val_accuracy: 0.5301\n",
      "Epoch 2/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0019 - accuracy: 0.49 - 0s 27us/step - loss: 1.0018 - accuracy: 0.4958 - val_loss: 1.0000 - val_accuracy: 0.5324\n",
      "Epoch 3/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9988 - accuracy: 0.49 - 0s 27us/step - loss: 0.9993 - accuracy: 0.4959 - val_loss: 0.9989 - val_accuracy: 0.5312\n",
      "Epoch 4/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 1.0007 - accuracy: 0.49 - 0s 27us/step - loss: 0.9999 - accuracy: 0.4925 - val_loss: 0.9978 - val_accuracy: 0.5286\n",
      "Epoch 5/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9942 - accuracy: 0.49 - 0s 27us/step - loss: 0.9954 - accuracy: 0.4964 - val_loss: 0.9968 - val_accuracy: 0.5271\n",
      "Epoch 6/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9994 - accuracy: 0.49 - 0s 27us/step - loss: 0.9958 - accuracy: 0.4974 - val_loss: 0.9958 - val_accuracy: 0.5258\n",
      "Epoch 7/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9955 - accuracy: 0.49 - 0s 27us/step - loss: 0.9949 - accuracy: 0.4961 - val_loss: 0.9947 - val_accuracy: 0.5243\n",
      "Epoch 8/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9927 - accuracy: 0.49 - 0s 28us/step - loss: 0.9930 - accuracy: 0.4948 - val_loss: 0.9936 - val_accuracy: 0.5260\n",
      "Epoch 9/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9879 - accuracy: 0.50 - 0s 27us/step - loss: 0.9902 - accuracy: 0.5002 - val_loss: 0.9924 - val_accuracy: 0.5286\n",
      "Epoch 10/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9947 - accuracy: 0.49 - 0s 27us/step - loss: 0.9927 - accuracy: 0.4966 - val_loss: 0.9911 - val_accuracy: 0.5329\n",
      "Epoch 11/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9893 - accuracy: 0.49 - 0s 28us/step - loss: 0.9900 - accuracy: 0.4985 - val_loss: 0.9897 - val_accuracy: 0.5365\n",
      "Epoch 12/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9890 - accuracy: 0.50 - 0s 30us/step - loss: 0.9896 - accuracy: 0.4983 - val_loss: 0.9885 - val_accuracy: 0.5380\n",
      "Epoch 13/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9884 - accuracy: 0.49 - 0s 27us/step - loss: 0.9878 - accuracy: 0.4981 - val_loss: 0.9873 - val_accuracy: 0.5398\n",
      "Epoch 14/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9920 - accuracy: 0.49 - 0s 27us/step - loss: 0.9896 - accuracy: 0.4979 - val_loss: 0.9861 - val_accuracy: 0.5414\n",
      "Epoch 15/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9891 - accuracy: 0.49 - 0s 30us/step - loss: 0.9881 - accuracy: 0.4981 - val_loss: 0.9849 - val_accuracy: 0.5432\n",
      "Epoch 16/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9789 - accuracy: 0.51 - 0s 27us/step - loss: 0.9804 - accuracy: 0.5073 - val_loss: 0.9837 - val_accuracy: 0.5444\n",
      "Epoch 17/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9866 - accuracy: 0.50 - 0s 27us/step - loss: 0.9857 - accuracy: 0.5018 - val_loss: 0.9826 - val_accuracy: 0.5434\n",
      "Epoch 18/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9845 - accuracy: 0.49 - 0s 27us/step - loss: 0.9813 - accuracy: 0.5048 - val_loss: 0.9816 - val_accuracy: 0.5414\n",
      "Epoch 19/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9800 - accuracy: 0.50 - 0s 27us/step - loss: 0.9798 - accuracy: 0.5038 - val_loss: 0.9805 - val_accuracy: 0.5411\n",
      "Epoch 20/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9826 - accuracy: 0.49 - 0s 27us/step - loss: 0.9831 - accuracy: 0.4975 - val_loss: 0.9793 - val_accuracy: 0.5424\n",
      "Epoch 21/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9773 - accuracy: 0.50 - 0s 26us/step - loss: 0.9780 - accuracy: 0.5052 - val_loss: 0.9780 - val_accuracy: 0.5460\n",
      "Epoch 22/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9760 - accuracy: 0.50 - 0s 27us/step - loss: 0.9759 - accuracy: 0.5050 - val_loss: 0.9767 - val_accuracy: 0.5501\n",
      "Epoch 23/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9788 - accuracy: 0.50 - 0s 27us/step - loss: 0.9782 - accuracy: 0.5070 - val_loss: 0.9754 - val_accuracy: 0.5544\n",
      "Epoch 24/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9754 - accuracy: 0.50 - 0s 28us/step - loss: 0.9747 - accuracy: 0.5094 - val_loss: 0.9741 - val_accuracy: 0.5557\n",
      "Epoch 25/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9675 - accuracy: 0.51 - 0s 26us/step - loss: 0.9706 - accuracy: 0.5121 - val_loss: 0.9730 - val_accuracy: 0.5569\n",
      "Epoch 26/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9724 - accuracy: 0.50 - 0s 27us/step - loss: 0.9728 - accuracy: 0.5047 - val_loss: 0.9718 - val_accuracy: 0.5569\n",
      "Epoch 27/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9713 - accuracy: 0.50 - 0s 27us/step - loss: 0.9717 - accuracy: 0.5066 - val_loss: 0.9706 - val_accuracy: 0.5575\n",
      "Epoch 28/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9716 - accuracy: 0.50 - 0s 28us/step - loss: 0.9691 - accuracy: 0.5118 - val_loss: 0.9695 - val_accuracy: 0.5575\n",
      "Epoch 29/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9657 - accuracy: 0.51 - 0s 28us/step - loss: 0.9689 - accuracy: 0.5095 - val_loss: 0.9684 - val_accuracy: 0.5577\n",
      "Epoch 30/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9695 - accuracy: 0.51 - 0s 28us/step - loss: 0.9674 - accuracy: 0.5124 - val_loss: 0.9672 - val_accuracy: 0.5580\n",
      "Epoch 31/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9685 - accuracy: 0.50 - 0s 28us/step - loss: 0.9659 - accuracy: 0.5121 - val_loss: 0.9659 - val_accuracy: 0.5600\n",
      "Epoch 32/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9643 - accuracy: 0.51 - 0s 28us/step - loss: 0.9635 - accuracy: 0.5140 - val_loss: 0.9645 - val_accuracy: 0.5633\n",
      "Epoch 33/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9650 - accuracy: 0.51 - 0s 28us/step - loss: 0.9675 - accuracy: 0.5116 - val_loss: 0.9632 - val_accuracy: 0.5659\n",
      "Epoch 34/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9581 - accuracy: 0.52 - 0s 28us/step - loss: 0.9600 - accuracy: 0.5222 - val_loss: 0.9620 - val_accuracy: 0.5664\n",
      "Epoch 35/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9632 - accuracy: 0.51 - 0s 29us/step - loss: 0.9604 - accuracy: 0.5137 - val_loss: 0.9609 - val_accuracy: 0.5666\n",
      "Epoch 36/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9621 - accuracy: 0.51 - 0s 27us/step - loss: 0.9618 - accuracy: 0.5137 - val_loss: 0.9597 - val_accuracy: 0.5669\n",
      "Epoch 37/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9611 - accuracy: 0.51 - 0s 28us/step - loss: 0.9604 - accuracy: 0.5124 - val_loss: 0.9586 - val_accuracy: 0.5664\n",
      "Epoch 38/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9514 - accuracy: 0.52 - 0s 28us/step - loss: 0.9568 - accuracy: 0.5162 - val_loss: 0.9575 - val_accuracy: 0.5661\n",
      "Epoch 39/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9577 - accuracy: 0.51 - 0s 27us/step - loss: 0.9570 - accuracy: 0.5157 - val_loss: 0.9563 - val_accuracy: 0.5666\n",
      "Epoch 40/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9556 - accuracy: 0.51 - 0s 27us/step - loss: 0.9566 - accuracy: 0.5152 - val_loss: 0.9550 - val_accuracy: 0.5689\n",
      "Epoch 41/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9521 - accuracy: 0.52 - 0s 27us/step - loss: 0.9532 - accuracy: 0.5159 - val_loss: 0.9537 - val_accuracy: 0.5712\n",
      "Epoch 42/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9499 - accuracy: 0.51 - 0s 26us/step - loss: 0.9511 - accuracy: 0.5217 - val_loss: 0.9524 - val_accuracy: 0.5735\n",
      "Epoch 43/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9474 - accuracy: 0.52 - 0s 28us/step - loss: 0.9479 - accuracy: 0.5268 - val_loss: 0.9511 - val_accuracy: 0.5751\n",
      "Epoch 44/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9483 - accuracy: 0.52 - 0s 27us/step - loss: 0.9510 - accuracy: 0.5200 - val_loss: 0.9499 - val_accuracy: 0.5758\n",
      "Epoch 45/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9508 - accuracy: 0.51 - 0s 28us/step - loss: 0.9509 - accuracy: 0.5171 - val_loss: 0.9487 - val_accuracy: 0.5769\n",
      "Epoch 46/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9458 - accuracy: 0.52 - 0s 27us/step - loss: 0.9460 - accuracy: 0.5261 - val_loss: 0.9476 - val_accuracy: 0.5764\n",
      "Epoch 47/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9472 - accuracy: 0.52 - 0s 28us/step - loss: 0.9443 - accuracy: 0.5233 - val_loss: 0.9465 - val_accuracy: 0.5761\n",
      "Epoch 48/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9442 - accuracy: 0.52 - 0s 27us/step - loss: 0.9456 - accuracy: 0.5203 - val_loss: 0.9453 - val_accuracy: 0.5766\n",
      "Epoch 49/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9407 - accuracy: 0.52 - 0s 28us/step - loss: 0.9400 - accuracy: 0.5274 - val_loss: 0.9440 - val_accuracy: 0.5781\n",
      "Epoch 50/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9401 - accuracy: 0.52 - 0s 27us/step - loss: 0.9413 - accuracy: 0.5251 - val_loss: 0.9427 - val_accuracy: 0.5792\n",
      "Epoch 51/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9396 - accuracy: 0.52 - 0s 30us/step - loss: 0.9388 - accuracy: 0.5293 - val_loss: 0.9415 - val_accuracy: 0.5797\n",
      "Epoch 52/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9395 - accuracy: 0.52 - 0s 28us/step - loss: 0.9402 - accuracy: 0.5239 - val_loss: 0.9404 - val_accuracy: 0.5794\n",
      "Epoch 53/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9400 - accuracy: 0.52 - 0s 28us/step - loss: 0.9398 - accuracy: 0.5235 - val_loss: 0.9392 - val_accuracy: 0.5804\n",
      "Epoch 54/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9371 - accuracy: 0.52 - 0s 28us/step - loss: 0.9364 - accuracy: 0.5271 - val_loss: 0.9378 - val_accuracy: 0.5804\n",
      "Epoch 55/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9367 - accuracy: 0.52 - 0s 27us/step - loss: 0.9364 - accuracy: 0.5275 - val_loss: 0.9366 - val_accuracy: 0.5815\n",
      "Epoch 56/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9370 - accuracy: 0.52 - 0s 27us/step - loss: 0.9323 - accuracy: 0.5322 - val_loss: 0.9355 - val_accuracy: 0.5815\n",
      "Epoch 57/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9387 - accuracy: 0.52 - 0s 28us/step - loss: 0.9385 - accuracy: 0.5224 - val_loss: 0.9344 - val_accuracy: 0.5802\n",
      "Epoch 58/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9370 - accuracy: 0.52 - 0s 27us/step - loss: 0.9350 - accuracy: 0.5254 - val_loss: 0.9333 - val_accuracy: 0.5804\n",
      "Epoch 59/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9272 - accuracy: 0.53 - 0s 27us/step - loss: 0.9298 - accuracy: 0.5288 - val_loss: 0.9321 - val_accuracy: 0.5827\n",
      "Epoch 60/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9318 - accuracy: 0.52 - 0s 28us/step - loss: 0.9304 - accuracy: 0.5274 - val_loss: 0.9308 - val_accuracy: 0.5845\n",
      "Epoch 61/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9230 - accuracy: 0.53 - 0s 29us/step - loss: 0.9228 - accuracy: 0.5380 - val_loss: 0.9295 - val_accuracy: 0.5853\n",
      "Epoch 62/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.53 - 0s 27us/step - loss: 0.9265 - accuracy: 0.5312 - val_loss: 0.9283 - val_accuracy: 0.5868\n",
      "Epoch 63/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9302 - accuracy: 0.52 - 0s 29us/step - loss: 0.9275 - accuracy: 0.5277 - val_loss: 0.9271 - val_accuracy: 0.5873\n",
      "Epoch 64/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.54 - 0s 28us/step - loss: 0.9212 - accuracy: 0.5342 - val_loss: 0.9261 - val_accuracy: 0.5850\n",
      "Epoch 65/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9216 - accuracy: 0.53 - 0s 27us/step - loss: 0.9218 - accuracy: 0.5317 - val_loss: 0.9251 - val_accuracy: 0.5855\n",
      "Epoch 66/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9237 - accuracy: 0.53 - 0s 30us/step - loss: 0.9211 - accuracy: 0.5363 - val_loss: 0.9240 - val_accuracy: 0.5845\n",
      "Epoch 67/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9164 - accuracy: 0.54 - 0s 27us/step - loss: 0.9178 - accuracy: 0.5378 - val_loss: 0.9226 - val_accuracy: 0.5868\n",
      "Epoch 68/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9128 - accuracy: 0.54 - 0s 28us/step - loss: 0.9159 - accuracy: 0.5402 - val_loss: 0.9211 - val_accuracy: 0.5896\n",
      "Epoch 69/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.53 - 0s 28us/step - loss: 0.9172 - accuracy: 0.5359 - val_loss: 0.9199 - val_accuracy: 0.5914\n",
      "Epoch 70/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9139 - accuracy: 0.54 - 0s 29us/step - loss: 0.9154 - accuracy: 0.5387 - val_loss: 0.9189 - val_accuracy: 0.5901\n",
      "Epoch 71/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9121 - accuracy: 0.53 - 0s 29us/step - loss: 0.9165 - accuracy: 0.5346 - val_loss: 0.9179 - val_accuracy: 0.5891\n",
      "Epoch 72/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9170 - accuracy: 0.53 - 0s 27us/step - loss: 0.9165 - accuracy: 0.5334 - val_loss: 0.9168 - val_accuracy: 0.5889\n",
      "Epoch 73/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.53 - 0s 27us/step - loss: 0.9149 - accuracy: 0.5320 - val_loss: 0.9156 - val_accuracy: 0.5899\n",
      "Epoch 74/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9090 - accuracy: 0.53 - 0s 27us/step - loss: 0.9101 - accuracy: 0.5376 - val_loss: 0.9143 - val_accuracy: 0.5927\n",
      "Epoch 75/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9129 - accuracy: 0.53 - 0s 27us/step - loss: 0.9109 - accuracy: 0.5376 - val_loss: 0.9131 - val_accuracy: 0.5937\n",
      "Epoch 76/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9103 - accuracy: 0.53 - 0s 28us/step - loss: 0.9073 - accuracy: 0.5413 - val_loss: 0.9121 - val_accuracy: 0.5932\n",
      "Epoch 77/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.52 - 0s 28us/step - loss: 0.9089 - accuracy: 0.5387 - val_loss: 0.9112 - val_accuracy: 0.5912\n",
      "Epoch 78/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9030 - accuracy: 0.54 - 0s 28us/step - loss: 0.9048 - accuracy: 0.5385 - val_loss: 0.9102 - val_accuracy: 0.5901\n",
      "Epoch 79/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9052 - accuracy: 0.53 - 0s 26us/step - loss: 0.9050 - accuracy: 0.5384 - val_loss: 0.9090 - val_accuracy: 0.5914\n",
      "Epoch 80/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8997 - accuracy: 0.54 - 0s 29us/step - loss: 0.9014 - accuracy: 0.5449 - val_loss: 0.9076 - val_accuracy: 0.5937\n",
      "Epoch 81/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9005 - accuracy: 0.54 - 0s 28us/step - loss: 0.9015 - accuracy: 0.5400 - val_loss: 0.9063 - val_accuracy: 0.5945\n",
      "Epoch 82/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9033 - accuracy: 0.54 - 0s 27us/step - loss: 0.9005 - accuracy: 0.5431 - val_loss: 0.9051 - val_accuracy: 0.5953\n",
      "Epoch 83/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.54 - 0s 28us/step - loss: 0.8987 - accuracy: 0.5411 - val_loss: 0.9040 - val_accuracy: 0.5955\n",
      "Epoch 84/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8971 - accuracy: 0.54 - 0s 27us/step - loss: 0.8996 - accuracy: 0.5428 - val_loss: 0.9029 - val_accuracy: 0.5965\n",
      "Epoch 85/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8961 - accuracy: 0.54 - 0s 28us/step - loss: 0.8957 - accuracy: 0.5423 - val_loss: 0.9019 - val_accuracy: 0.5963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8893 - accuracy: 0.55 - 0s 27us/step - loss: 0.8949 - accuracy: 0.5437 - val_loss: 0.9006 - val_accuracy: 0.5975\n",
      "Epoch 87/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.9010 - accuracy: 0.53 - 0s 30us/step - loss: 0.8961 - accuracy: 0.5417 - val_loss: 0.8994 - val_accuracy: 0.5993\n",
      "Epoch 88/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8942 - accuracy: 0.54 - 0s 29us/step - loss: 0.8936 - accuracy: 0.5441 - val_loss: 0.8983 - val_accuracy: 0.6001\n",
      "Epoch 89/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8921 - accuracy: 0.54 - 0s 29us/step - loss: 0.8936 - accuracy: 0.5423 - val_loss: 0.8973 - val_accuracy: 0.6001\n",
      "Epoch 90/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8944 - accuracy: 0.54 - 0s 28us/step - loss: 0.8931 - accuracy: 0.5436 - val_loss: 0.8962 - val_accuracy: 0.6004\n",
      "Epoch 91/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8946 - accuracy: 0.54 - 0s 29us/step - loss: 0.8958 - accuracy: 0.5401 - val_loss: 0.8951 - val_accuracy: 0.6006\n",
      "Epoch 92/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8922 - accuracy: 0.53 - 0s 29us/step - loss: 0.8888 - accuracy: 0.5464 - val_loss: 0.8941 - val_accuracy: 0.6006\n",
      "Epoch 93/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8926 - accuracy: 0.54 - 0s 29us/step - loss: 0.8906 - accuracy: 0.5446 - val_loss: 0.8931 - val_accuracy: 0.5996\n",
      "Epoch 94/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8827 - accuracy: 0.55 - 0s 31us/step - loss: 0.8866 - accuracy: 0.5482 - val_loss: 0.8919 - val_accuracy: 0.6016\n",
      "Epoch 95/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8861 - accuracy: 0.54 - 0s 30us/step - loss: 0.8879 - accuracy: 0.5423 - val_loss: 0.8908 - val_accuracy: 0.6024\n",
      "Epoch 96/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8790 - accuracy: 0.55 - 0s 28us/step - loss: 0.8837 - accuracy: 0.5458 - val_loss: 0.8897 - val_accuracy: 0.6027\n",
      "Epoch 97/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8851 - accuracy: 0.54 - 0s 28us/step - loss: 0.8846 - accuracy: 0.5478 - val_loss: 0.8888 - val_accuracy: 0.6029\n",
      "Epoch 98/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8793 - accuracy: 0.55 - 0s 27us/step - loss: 0.8770 - accuracy: 0.5544 - val_loss: 0.8877 - val_accuracy: 0.6029\n",
      "Epoch 99/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8817 - accuracy: 0.54 - 0s 28us/step - loss: 0.8829 - accuracy: 0.5462 - val_loss: 0.8864 - val_accuracy: 0.6055\n",
      "Epoch 100/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8777 - accuracy: 0.55 - 0s 29us/step - loss: 0.8788 - accuracy: 0.5501 - val_loss: 0.8852 - val_accuracy: 0.6065\n",
      "Epoch 101/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.54 - 0s 28us/step - loss: 0.8771 - accuracy: 0.5484 - val_loss: 0.8840 - val_accuracy: 0.6067\n",
      "Epoch 102/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8778 - accuracy: 0.55 - 0s 28us/step - loss: 0.8761 - accuracy: 0.5508 - val_loss: 0.8830 - val_accuracy: 0.6070\n",
      "Epoch 103/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8782 - accuracy: 0.54 - 0s 28us/step - loss: 0.8763 - accuracy: 0.5482 - val_loss: 0.8823 - val_accuracy: 0.6070\n",
      "Epoch 104/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8787 - accuracy: 0.54 - 0s 29us/step - loss: 0.8755 - accuracy: 0.5489 - val_loss: 0.8817 - val_accuracy: 0.6047\n",
      "Epoch 105/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.54 - 0s 31us/step - loss: 0.8745 - accuracy: 0.5502 - val_loss: 0.8808 - val_accuracy: 0.6044\n",
      "Epoch 106/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8679 - accuracy: 0.55 - 0s 31us/step - loss: 0.8723 - accuracy: 0.5497 - val_loss: 0.8793 - val_accuracy: 0.6070\n",
      "Epoch 107/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8708 - accuracy: 0.55 - 0s 30us/step - loss: 0.8703 - accuracy: 0.5537 - val_loss: 0.8779 - val_accuracy: 0.6083\n",
      "Epoch 108/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8726 - accuracy: 0.55 - 0s 28us/step - loss: 0.8698 - accuracy: 0.5539 - val_loss: 0.8767 - val_accuracy: 0.6088\n",
      "Epoch 109/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8680 - accuracy: 0.55 - 0s 28us/step - loss: 0.8685 - accuracy: 0.5565 - val_loss: 0.8757 - val_accuracy: 0.6085\n",
      "Epoch 110/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8670 - accuracy: 0.55 - 0s 28us/step - loss: 0.8672 - accuracy: 0.5541 - val_loss: 0.8749 - val_accuracy: 0.6090\n",
      "Epoch 111/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8660 - accuracy: 0.55 - 0s 27us/step - loss: 0.8665 - accuracy: 0.5530 - val_loss: 0.8742 - val_accuracy: 0.6085\n",
      "Epoch 112/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8618 - accuracy: 0.55 - 0s 27us/step - loss: 0.8629 - accuracy: 0.5537 - val_loss: 0.8731 - val_accuracy: 0.6083\n",
      "Epoch 113/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8704 - accuracy: 0.54 - 0s 26us/step - loss: 0.8713 - accuracy: 0.5453 - val_loss: 0.8721 - val_accuracy: 0.6085\n",
      "Epoch 114/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8666 - accuracy: 0.54 - 0s 27us/step - loss: 0.8647 - accuracy: 0.5495 - val_loss: 0.8708 - val_accuracy: 0.6096\n",
      "Epoch 115/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.55 - 0s 27us/step - loss: 0.8592 - accuracy: 0.5565 - val_loss: 0.8696 - val_accuracy: 0.6101\n",
      "Epoch 116/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8613 - accuracy: 0.55 - 0s 26us/step - loss: 0.8628 - accuracy: 0.5512 - val_loss: 0.8687 - val_accuracy: 0.6101\n",
      "Epoch 117/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8572 - accuracy: 0.56 - 0s 26us/step - loss: 0.8597 - accuracy: 0.5564 - val_loss: 0.8680 - val_accuracy: 0.6108\n",
      "Epoch 118/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8613 - accuracy: 0.55 - 0s 27us/step - loss: 0.8574 - accuracy: 0.5561 - val_loss: 0.8674 - val_accuracy: 0.6101\n",
      "Epoch 119/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8607 - accuracy: 0.54 - 0s 27us/step - loss: 0.8593 - accuracy: 0.5533 - val_loss: 0.8665 - val_accuracy: 0.6098\n",
      "Epoch 120/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8567 - accuracy: 0.55 - 0s 27us/step - loss: 0.8589 - accuracy: 0.5537 - val_loss: 0.8652 - val_accuracy: 0.6106\n",
      "Epoch 121/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8533 - accuracy: 0.55 - 0s 27us/step - loss: 0.8564 - accuracy: 0.5539 - val_loss: 0.8641 - val_accuracy: 0.6103\n",
      "Epoch 122/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8509 - accuracy: 0.55 - 0s 27us/step - loss: 0.8518 - accuracy: 0.5592 - val_loss: 0.8631 - val_accuracy: 0.6108\n",
      "Epoch 123/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8539 - accuracy: 0.55 - 0s 27us/step - loss: 0.8543 - accuracy: 0.5526 - val_loss: 0.8622 - val_accuracy: 0.6108\n",
      "Epoch 124/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8507 - accuracy: 0.56 - 0s 28us/step - loss: 0.8490 - accuracy: 0.5622 - val_loss: 0.8616 - val_accuracy: 0.6111\n",
      "Epoch 125/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8486 - accuracy: 0.56 - 0s 27us/step - loss: 0.8487 - accuracy: 0.5620 - val_loss: 0.8607 - val_accuracy: 0.6108\n",
      "Epoch 126/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8478 - accuracy: 0.56 - 0s 26us/step - loss: 0.8491 - accuracy: 0.5591 - val_loss: 0.8593 - val_accuracy: 0.6131\n",
      "Epoch 127/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8611 - accuracy: 0.54 - 0s 26us/step - loss: 0.8576 - accuracy: 0.5507 - val_loss: 0.8582 - val_accuracy: 0.6147\n",
      "Epoch 128/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8503 - accuracy: 0.55 - 0s 27us/step - loss: 0.8499 - accuracy: 0.5549 - val_loss: 0.8572 - val_accuracy: 0.6154\n",
      "Epoch 129/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8477 - accuracy: 0.55 - 0s 27us/step - loss: 0.8481 - accuracy: 0.5559 - val_loss: 0.8563 - val_accuracy: 0.6157\n",
      "Epoch 130/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8468 - accuracy: 0.55 - 0s 27us/step - loss: 0.8507 - accuracy: 0.5530 - val_loss: 0.8556 - val_accuracy: 0.6147\n",
      "Epoch 131/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8428 - accuracy: 0.56 - 0s 27us/step - loss: 0.8424 - accuracy: 0.5637 - val_loss: 0.8550 - val_accuracy: 0.6141\n",
      "Epoch 132/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8483 - accuracy: 0.55 - 0s 27us/step - loss: 0.8457 - accuracy: 0.5555 - val_loss: 0.8541 - val_accuracy: 0.6152\n",
      "Epoch 133/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8484 - accuracy: 0.55 - 0s 28us/step - loss: 0.8462 - accuracy: 0.5545 - val_loss: 0.8528 - val_accuracy: 0.6167\n",
      "Epoch 134/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8423 - accuracy: 0.56 - 0s 27us/step - loss: 0.8414 - accuracy: 0.5627 - val_loss: 0.8516 - val_accuracy: 0.6182\n",
      "Epoch 135/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8437 - accuracy: 0.55 - 0s 28us/step - loss: 0.8396 - accuracy: 0.5609 - val_loss: 0.8509 - val_accuracy: 0.6175\n",
      "Epoch 136/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8401 - accuracy: 0.56 - 0s 29us/step - loss: 0.8386 - accuracy: 0.5641 - val_loss: 0.8502 - val_accuracy: 0.6177\n",
      "Epoch 137/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8373 - accuracy: 0.56 - 0s 29us/step - loss: 0.8393 - accuracy: 0.5611 - val_loss: 0.8496 - val_accuracy: 0.6159\n",
      "Epoch 138/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8429 - accuracy: 0.55 - 0s 28us/step - loss: 0.8385 - accuracy: 0.5586 - val_loss: 0.8487 - val_accuracy: 0.6167\n",
      "Epoch 139/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8423 - accuracy: 0.55 - 0s 29us/step - loss: 0.8367 - accuracy: 0.5617 - val_loss: 0.8475 - val_accuracy: 0.6182\n",
      "Epoch 140/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8323 - accuracy: 0.56 - 0s 28us/step - loss: 0.8326 - accuracy: 0.5643 - val_loss: 0.8463 - val_accuracy: 0.6190\n",
      "Epoch 141/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.56 - 0s 30us/step - loss: 0.8318 - accuracy: 0.5636 - val_loss: 0.8452 - val_accuracy: 0.6193\n",
      "Epoch 142/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8257 - accuracy: 0.56 - 0s 29us/step - loss: 0.8280 - accuracy: 0.5696 - val_loss: 0.8447 - val_accuracy: 0.6198\n",
      "Epoch 143/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8370 - accuracy: 0.55 - 0s 30us/step - loss: 0.8303 - accuracy: 0.5662 - val_loss: 0.8441 - val_accuracy: 0.6190\n",
      "Epoch 144/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8313 - accuracy: 0.56 - 0s 29us/step - loss: 0.8326 - accuracy: 0.5611 - val_loss: 0.8432 - val_accuracy: 0.6193\n",
      "Epoch 145/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8275 - accuracy: 0.56 - 0s 29us/step - loss: 0.8314 - accuracy: 0.5614 - val_loss: 0.8420 - val_accuracy: 0.6198\n",
      "Epoch 146/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8339 - accuracy: 0.55 - 0s 27us/step - loss: 0.8307 - accuracy: 0.5615 - val_loss: 0.8408 - val_accuracy: 0.6218\n",
      "Epoch 147/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8294 - accuracy: 0.56 - 0s 30us/step - loss: 0.8292 - accuracy: 0.5632 - val_loss: 0.8400 - val_accuracy: 0.6218\n",
      "Epoch 148/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8238 - accuracy: 0.56 - 0s 30us/step - loss: 0.8246 - accuracy: 0.5666 - val_loss: 0.8394 - val_accuracy: 0.6205\n",
      "Epoch 149/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8220 - accuracy: 0.57 - 0s 27us/step - loss: 0.8246 - accuracy: 0.5673 - val_loss: 0.8389 - val_accuracy: 0.6195\n",
      "Epoch 150/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8243 - accuracy: 0.57 - 0s 27us/step - loss: 0.8240 - accuracy: 0.5685 - val_loss: 0.8382 - val_accuracy: 0.6205\n",
      "Epoch 151/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.56 - 0s 28us/step - loss: 0.8260 - accuracy: 0.5638 - val_loss: 0.8370 - val_accuracy: 0.6213\n",
      "Epoch 152/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.56 - 0s 31us/step - loss: 0.8228 - accuracy: 0.5656 - val_loss: 0.8358 - val_accuracy: 0.6216\n",
      "Epoch 153/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8206 - accuracy: 0.56 - 0s 30us/step - loss: 0.8205 - accuracy: 0.5681 - val_loss: 0.8349 - val_accuracy: 0.6221\n",
      "Epoch 154/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8198 - accuracy: 0.56 - 0s 29us/step - loss: 0.8216 - accuracy: 0.5667 - val_loss: 0.8342 - val_accuracy: 0.6216\n",
      "Epoch 155/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8179 - accuracy: 0.56 - 0s 29us/step - loss: 0.8182 - accuracy: 0.5666 - val_loss: 0.8334 - val_accuracy: 0.6216\n",
      "Epoch 156/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8203 - accuracy: 0.56 - 0s 28us/step - loss: 0.8200 - accuracy: 0.5662 - val_loss: 0.8327 - val_accuracy: 0.6218\n",
      "Epoch 157/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8210 - accuracy: 0.56 - 0s 27us/step - loss: 0.8200 - accuracy: 0.5645 - val_loss: 0.8322 - val_accuracy: 0.6218\n",
      "Epoch 158/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8053 - accuracy: 0.57 - 0s 26us/step - loss: 0.8092 - accuracy: 0.5763 - val_loss: 0.8313 - val_accuracy: 0.6221\n",
      "Epoch 159/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.57 - 0s 29us/step - loss: 0.8144 - accuracy: 0.5684 - val_loss: 0.8303 - val_accuracy: 0.6223\n",
      "Epoch 160/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8115 - accuracy: 0.57 - 0s 27us/step - loss: 0.8125 - accuracy: 0.5726 - val_loss: 0.8291 - val_accuracy: 0.6246\n",
      "Epoch 161/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8111 - accuracy: 0.57 - 0s 28us/step - loss: 0.8092 - accuracy: 0.5722 - val_loss: 0.8278 - val_accuracy: 0.6254\n",
      "Epoch 162/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8125 - accuracy: 0.57 - 0s 30us/step - loss: 0.8088 - accuracy: 0.5730 - val_loss: 0.8271 - val_accuracy: 0.6251\n",
      "Epoch 163/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8048 - accuracy: 0.57 - 0s 29us/step - loss: 0.8078 - accuracy: 0.5750 - val_loss: 0.8266 - val_accuracy: 0.6244\n",
      "Epoch 164/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8156 - accuracy: 0.56 - 0s 28us/step - loss: 0.8126 - accuracy: 0.5659 - val_loss: 0.8260 - val_accuracy: 0.6228\n",
      "Epoch 165/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.57 - 0s 28us/step - loss: 0.8084 - accuracy: 0.5709 - val_loss: 0.8253 - val_accuracy: 0.6236\n",
      "Epoch 166/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8083 - accuracy: 0.57 - 0s 28us/step - loss: 0.8110 - accuracy: 0.5717 - val_loss: 0.8245 - val_accuracy: 0.6236\n",
      "Epoch 167/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.57 - 0s 28us/step - loss: 0.8067 - accuracy: 0.5726 - val_loss: 0.8235 - val_accuracy: 0.6233\n",
      "Epoch 168/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7995 - accuracy: 0.57 - 0s 29us/step - loss: 0.8030 - accuracy: 0.5730 - val_loss: 0.8225 - val_accuracy: 0.6241\n",
      "Epoch 169/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8062 - accuracy: 0.57 - 0s 29us/step - loss: 0.8102 - accuracy: 0.5687 - val_loss: 0.8216 - val_accuracy: 0.6249\n",
      "Epoch 170/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8017 - accuracy: 0.57 - 0s 29us/step - loss: 0.8053 - accuracy: 0.5745 - val_loss: 0.8210 - val_accuracy: 0.6239\n",
      "Epoch 171/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8011 - accuracy: 0.57 - 0s 28us/step - loss: 0.8012 - accuracy: 0.5746 - val_loss: 0.8205 - val_accuracy: 0.6241\n",
      "Epoch 172/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8031 - accuracy: 0.57 - 0s 28us/step - loss: 0.8058 - accuracy: 0.5709 - val_loss: 0.8194 - val_accuracy: 0.6241\n",
      "Epoch 173/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8041 - accuracy: 0.57 - 0s 29us/step - loss: 0.8019 - accuracy: 0.5736 - val_loss: 0.8187 - val_accuracy: 0.6246\n",
      "Epoch 174/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7980 - accuracy: 0.57 - 0s 31us/step - loss: 0.7971 - accuracy: 0.5790 - val_loss: 0.8181 - val_accuracy: 0.6244\n",
      "Epoch 175/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7995 - accuracy: 0.57 - 0s 31us/step - loss: 0.8000 - accuracy: 0.5748 - val_loss: 0.8174 - val_accuracy: 0.6244\n",
      "Epoch 176/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7969 - accuracy: 0.57 - 0s 29us/step - loss: 0.7957 - accuracy: 0.5793 - val_loss: 0.8165 - val_accuracy: 0.6254\n",
      "Epoch 177/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.8030 - accuracy: 0.57 - 0s 28us/step - loss: 0.8036 - accuracy: 0.5757 - val_loss: 0.8156 - val_accuracy: 0.6254\n",
      "Epoch 178/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7968 - accuracy: 0.57 - 0s 28us/step - loss: 0.7959 - accuracy: 0.5772 - val_loss: 0.8150 - val_accuracy: 0.6256\n",
      "Epoch 179/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7973 - accuracy: 0.57 - 0s 28us/step - loss: 0.7969 - accuracy: 0.5748 - val_loss: 0.8142 - val_accuracy: 0.6256\n",
      "Epoch 180/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7982 - accuracy: 0.57 - 0s 28us/step - loss: 0.7963 - accuracy: 0.5781 - val_loss: 0.8137 - val_accuracy: 0.6259\n",
      "Epoch 181/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7904 - accuracy: 0.58 - 0s 28us/step - loss: 0.7924 - accuracy: 0.5822 - val_loss: 0.8130 - val_accuracy: 0.6267\n",
      "Epoch 182/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7859 - accuracy: 0.58 - 0s 27us/step - loss: 0.7895 - accuracy: 0.5826 - val_loss: 0.8120 - val_accuracy: 0.6267\n",
      "Epoch 183/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7915 - accuracy: 0.57 - 1s 33us/step - loss: 0.7898 - accuracy: 0.5805 - val_loss: 0.8110 - val_accuracy: 0.6272\n",
      "Epoch 184/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.57 - 1s 33us/step - loss: 0.7913 - accuracy: 0.5796 - val_loss: 0.8102 - val_accuracy: 0.6272\n",
      "Epoch 185/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7936 - accuracy: 0.57 - 0s 28us/step - loss: 0.7912 - accuracy: 0.5811 - val_loss: 0.8097 - val_accuracy: 0.6274\n",
      "Epoch 186/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7873 - accuracy: 0.58 - 0s 26us/step - loss: 0.7848 - accuracy: 0.5874 - val_loss: 0.8090 - val_accuracy: 0.6277\n",
      "Epoch 187/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7908 - accuracy: 0.57 - 0s 30us/step - loss: 0.7916 - accuracy: 0.5779 - val_loss: 0.8080 - val_accuracy: 0.6279\n",
      "Epoch 188/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7922 - accuracy: 0.58 - 0s 28us/step - loss: 0.7902 - accuracy: 0.5794 - val_loss: 0.8071 - val_accuracy: 0.6292\n",
      "Epoch 189/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7810 - accuracy: 0.59 - 0s 27us/step - loss: 0.7824 - accuracy: 0.5874 - val_loss: 0.8063 - val_accuracy: 0.6300\n",
      "Epoch 190/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7809 - accuracy: 0.58 - 0s 27us/step - loss: 0.7848 - accuracy: 0.5835 - val_loss: 0.8061 - val_accuracy: 0.6295\n",
      "Epoch 191/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7877 - accuracy: 0.57 - 0s 26us/step - loss: 0.7859 - accuracy: 0.5791 - val_loss: 0.8060 - val_accuracy: 0.6287\n",
      "Epoch 192/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7888 - accuracy: 0.58 - 0s 25us/step - loss: 0.7859 - accuracy: 0.5837 - val_loss: 0.8049 - val_accuracy: 0.6300\n",
      "Epoch 193/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7813 - accuracy: 0.58 - 0s 25us/step - loss: 0.7815 - accuracy: 0.5862 - val_loss: 0.8035 - val_accuracy: 0.6307\n",
      "Epoch 194/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7855 - accuracy: 0.58 - 0s 26us/step - loss: 0.7847 - accuracy: 0.5842 - val_loss: 0.8024 - val_accuracy: 0.6310\n",
      "Epoch 195/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7816 - accuracy: 0.58 - 0s 26us/step - loss: 0.7782 - accuracy: 0.5876 - val_loss: 0.8017 - val_accuracy: 0.6313\n",
      "Epoch 196/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7855 - accuracy: 0.57 - 0s 25us/step - loss: 0.7833 - accuracy: 0.5814 - val_loss: 0.8012 - val_accuracy: 0.6310\n",
      "Epoch 197/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7857 - accuracy: 0.58 - 0s 26us/step - loss: 0.7805 - accuracy: 0.5882 - val_loss: 0.8008 - val_accuracy: 0.6320\n",
      "Epoch 198/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.59 - 0s 26us/step - loss: 0.7752 - accuracy: 0.5923 - val_loss: 0.7999 - val_accuracy: 0.6323\n",
      "Epoch 199/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7771 - accuracy: 0.59 - 0s 26us/step - loss: 0.7760 - accuracy: 0.5899 - val_loss: 0.7988 - val_accuracy: 0.6323\n",
      "Epoch 200/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7741 - accuracy: 0.59 - 0s 27us/step - loss: 0.7746 - accuracy: 0.5918 - val_loss: 0.7979 - val_accuracy: 0.6338\n",
      "Epoch 201/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7798 - accuracy: 0.58 - 0s 26us/step - loss: 0.7788 - accuracy: 0.5869 - val_loss: 0.7975 - val_accuracy: 0.6330\n",
      "Epoch 202/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7767 - accuracy: 0.59 - 0s 27us/step - loss: 0.7750 - accuracy: 0.5904 - val_loss: 0.7972 - val_accuracy: 0.6330\n",
      "Epoch 203/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7712 - accuracy: 0.59 - 0s 28us/step - loss: 0.7728 - accuracy: 0.5932 - val_loss: 0.7965 - val_accuracy: 0.6338\n",
      "Epoch 204/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7672 - accuracy: 0.59 - 0s 28us/step - loss: 0.7695 - accuracy: 0.5934 - val_loss: 0.7957 - val_accuracy: 0.6338\n",
      "Epoch 205/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7718 - accuracy: 0.59 - 0s 27us/step - loss: 0.7713 - accuracy: 0.5916 - val_loss: 0.7946 - val_accuracy: 0.6341\n",
      "Epoch 206/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7705 - accuracy: 0.59 - 0s 26us/step - loss: 0.7678 - accuracy: 0.5977 - val_loss: 0.7940 - val_accuracy: 0.6348\n",
      "Epoch 207/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7684 - accuracy: 0.59 - 0s 26us/step - loss: 0.7686 - accuracy: 0.5928 - val_loss: 0.7935 - val_accuracy: 0.6356\n",
      "Epoch 208/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7664 - accuracy: 0.59 - 0s 26us/step - loss: 0.7697 - accuracy: 0.5938 - val_loss: 0.7927 - val_accuracy: 0.6361\n",
      "Epoch 209/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7713 - accuracy: 0.58 - 0s 26us/step - loss: 0.7707 - accuracy: 0.5909 - val_loss: 0.7918 - val_accuracy: 0.6361\n",
      "Epoch 210/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7646 - accuracy: 0.60 - 0s 26us/step - loss: 0.7663 - accuracy: 0.5998 - val_loss: 0.7908 - val_accuracy: 0.6359\n",
      "Epoch 211/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7660 - accuracy: 0.60 - 0s 28us/step - loss: 0.7638 - accuracy: 0.6004 - val_loss: 0.7898 - val_accuracy: 0.6374\n",
      "Epoch 212/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7650 - accuracy: 0.59 - 0s 29us/step - loss: 0.7643 - accuracy: 0.5999 - val_loss: 0.7894 - val_accuracy: 0.6369\n",
      "Epoch 213/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7644 - accuracy: 0.60 - 0s 29us/step - loss: 0.7628 - accuracy: 0.6023 - val_loss: 0.7888 - val_accuracy: 0.6376\n",
      "Epoch 214/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7615 - accuracy: 0.59 - 0s 29us/step - loss: 0.7634 - accuracy: 0.5996 - val_loss: 0.7880 - val_accuracy: 0.6387\n",
      "Epoch 215/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7598 - accuracy: 0.60 - 0s 29us/step - loss: 0.7611 - accuracy: 0.6035 - val_loss: 0.7871 - val_accuracy: 0.6392\n",
      "Epoch 216/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7584 - accuracy: 0.60 - 0s 30us/step - loss: 0.7613 - accuracy: 0.6029 - val_loss: 0.7863 - val_accuracy: 0.6394\n",
      "Epoch 217/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7626 - accuracy: 0.59 - 0s 29us/step - loss: 0.7595 - accuracy: 0.6032 - val_loss: 0.7857 - val_accuracy: 0.6399\n",
      "Epoch 218/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7531 - accuracy: 0.60 - 0s 28us/step - loss: 0.7556 - accuracy: 0.6095 - val_loss: 0.7848 - val_accuracy: 0.6407\n",
      "Epoch 219/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7565 - accuracy: 0.60 - 0s 27us/step - loss: 0.7563 - accuracy: 0.6045 - val_loss: 0.7837 - val_accuracy: 0.6425\n",
      "Epoch 220/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7513 - accuracy: 0.61 - 0s 27us/step - loss: 0.7552 - accuracy: 0.6109 - val_loss: 0.7828 - val_accuracy: 0.6443\n",
      "Epoch 221/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7504 - accuracy: 0.61 - 0s 27us/step - loss: 0.7526 - accuracy: 0.6112 - val_loss: 0.7824 - val_accuracy: 0.6438\n",
      "Epoch 222/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7581 - accuracy: 0.60 - 0s 28us/step - loss: 0.7550 - accuracy: 0.6086 - val_loss: 0.7820 - val_accuracy: 0.6438\n",
      "Epoch 223/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7506 - accuracy: 0.61 - 0s 26us/step - loss: 0.7522 - accuracy: 0.6112 - val_loss: 0.7815 - val_accuracy: 0.6453\n",
      "Epoch 224/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7456 - accuracy: 0.61 - 0s 27us/step - loss: 0.7488 - accuracy: 0.6151 - val_loss: 0.7800 - val_accuracy: 0.6468\n",
      "Epoch 225/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7439 - accuracy: 0.61 - 0s 27us/step - loss: 0.7457 - accuracy: 0.6161 - val_loss: 0.7787 - val_accuracy: 0.6484\n",
      "Epoch 226/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7503 - accuracy: 0.61 - 0s 28us/step - loss: 0.7492 - accuracy: 0.6160 - val_loss: 0.7779 - val_accuracy: 0.6504\n",
      "Epoch 227/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7523 - accuracy: 0.61 - 0s 27us/step - loss: 0.7507 - accuracy: 0.6164 - val_loss: 0.7776 - val_accuracy: 0.6512\n",
      "Epoch 228/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7509 - accuracy: 0.61 - 0s 28us/step - loss: 0.7478 - accuracy: 0.6193 - val_loss: 0.7769 - val_accuracy: 0.6522\n",
      "Epoch 229/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7419 - accuracy: 0.62 - 0s 26us/step - loss: 0.7448 - accuracy: 0.6208 - val_loss: 0.7758 - val_accuracy: 0.6537\n",
      "Epoch 230/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7461 - accuracy: 0.62 - 0s 27us/step - loss: 0.7430 - accuracy: 0.6221 - val_loss: 0.7746 - val_accuracy: 0.6550\n",
      "Epoch 231/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7459 - accuracy: 0.62 - 0s 26us/step - loss: 0.7410 - accuracy: 0.6284 - val_loss: 0.7734 - val_accuracy: 0.6565\n",
      "Epoch 232/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7348 - accuracy: 0.63 - 0s 26us/step - loss: 0.7384 - accuracy: 0.6303 - val_loss: 0.7725 - val_accuracy: 0.6565\n",
      "Epoch 233/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7483 - accuracy: 0.62 - 0s 27us/step - loss: 0.7443 - accuracy: 0.6238 - val_loss: 0.7721 - val_accuracy: 0.6586\n",
      "Epoch 234/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7378 - accuracy: 0.62 - 0s 27us/step - loss: 0.7414 - accuracy: 0.6251 - val_loss: 0.7715 - val_accuracy: 0.6596\n",
      "Epoch 235/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7399 - accuracy: 0.62 - 0s 26us/step - loss: 0.7391 - accuracy: 0.6296 - val_loss: 0.7708 - val_accuracy: 0.6619\n",
      "Epoch 236/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7380 - accuracy: 0.62 - 0s 27us/step - loss: 0.7387 - accuracy: 0.6294 - val_loss: 0.7696 - val_accuracy: 0.6647\n",
      "Epoch 237/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7369 - accuracy: 0.63 - 0s 26us/step - loss: 0.7375 - accuracy: 0.6321 - val_loss: 0.7682 - val_accuracy: 0.6683\n",
      "Epoch 238/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7318 - accuracy: 0.64 - 0s 27us/step - loss: 0.7352 - accuracy: 0.6364 - val_loss: 0.7671 - val_accuracy: 0.6719\n",
      "Epoch 239/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.63 - 0s 26us/step - loss: 0.7336 - accuracy: 0.6386 - val_loss: 0.7667 - val_accuracy: 0.6736\n",
      "Epoch 240/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7352 - accuracy: 0.63 - 0s 26us/step - loss: 0.7363 - accuracy: 0.6327 - val_loss: 0.7665 - val_accuracy: 0.6747\n",
      "Epoch 241/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.64 - 0s 26us/step - loss: 0.7323 - accuracy: 0.6399 - val_loss: 0.7658 - val_accuracy: 0.6757\n",
      "Epoch 242/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.64 - 0s 26us/step - loss: 0.7314 - accuracy: 0.6418 - val_loss: 0.7643 - val_accuracy: 0.6808\n",
      "Epoch 243/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7233 - accuracy: 0.64 - 0s 27us/step - loss: 0.7293 - accuracy: 0.6437 - val_loss: 0.7631 - val_accuracy: 0.6836\n",
      "Epoch 244/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7254 - accuracy: 0.65 - 0s 26us/step - loss: 0.7265 - accuracy: 0.6511 - val_loss: 0.7622 - val_accuracy: 0.6864\n",
      "Epoch 245/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7237 - accuracy: 0.65 - 0s 26us/step - loss: 0.7222 - accuracy: 0.6575 - val_loss: 0.7615 - val_accuracy: 0.6869\n",
      "Epoch 246/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7264 - accuracy: 0.65 - 0s 27us/step - loss: 0.7257 - accuracy: 0.6525 - val_loss: 0.7605 - val_accuracy: 0.6872\n",
      "Epoch 247/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7267 - accuracy: 0.65 - 0s 26us/step - loss: 0.7257 - accuracy: 0.6511 - val_loss: 0.7592 - val_accuracy: 0.6895\n",
      "Epoch 248/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.66 - 0s 26us/step - loss: 0.7270 - accuracy: 0.6552 - val_loss: 0.7582 - val_accuracy: 0.6931\n",
      "Epoch 249/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7220 - accuracy: 0.65 - 0s 28us/step - loss: 0.7223 - accuracy: 0.6605 - val_loss: 0.7572 - val_accuracy: 0.6941\n",
      "Epoch 250/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7137 - accuracy: 0.66 - 0s 28us/step - loss: 0.7187 - accuracy: 0.6592 - val_loss: 0.7562 - val_accuracy: 0.6961\n",
      "Epoch 251/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7181 - accuracy: 0.66 - 0s 28us/step - loss: 0.7146 - accuracy: 0.6672 - val_loss: 0.7552 - val_accuracy: 0.6971\n",
      "Epoch 252/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7139 - accuracy: 0.66 - 0s 30us/step - loss: 0.7146 - accuracy: 0.6647 - val_loss: 0.7540 - val_accuracy: 0.6987\n",
      "Epoch 253/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7188 - accuracy: 0.66 - 0s 27us/step - loss: 0.7156 - accuracy: 0.6639 - val_loss: 0.7530 - val_accuracy: 0.6984\n",
      "Epoch 254/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7148 - accuracy: 0.66 - 0s 28us/step - loss: 0.7174 - accuracy: 0.6629 - val_loss: 0.7521 - val_accuracy: 0.6997\n",
      "Epoch 255/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7067 - accuracy: 0.67 - 0s 26us/step - loss: 0.7105 - accuracy: 0.6693 - val_loss: 0.7510 - val_accuracy: 0.7010\n",
      "Epoch 256/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7150 - accuracy: 0.66 - 0s 26us/step - loss: 0.7137 - accuracy: 0.6659 - val_loss: 0.7496 - val_accuracy: 0.7058\n",
      "Epoch 257/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7199 - accuracy: 0.65 - 0s 27us/step - loss: 0.7135 - accuracy: 0.6665 - val_loss: 0.7484 - val_accuracy: 0.7094\n",
      "Epoch 258/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7070 - accuracy: 0.67 - 0s 25us/step - loss: 0.7058 - accuracy: 0.6755 - val_loss: 0.7473 - val_accuracy: 0.7114\n",
      "Epoch 259/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7082 - accuracy: 0.67 - 0s 27us/step - loss: 0.7106 - accuracy: 0.6731 - val_loss: 0.7462 - val_accuracy: 0.7132\n",
      "Epoch 260/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7066 - accuracy: 0.68 - 0s 26us/step - loss: 0.7098 - accuracy: 0.6755 - val_loss: 0.7454 - val_accuracy: 0.7155\n",
      "Epoch 261/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.67 - 0s 26us/step - loss: 0.7064 - accuracy: 0.6776 - val_loss: 0.7445 - val_accuracy: 0.7186\n",
      "Epoch 262/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7072 - accuracy: 0.67 - 0s 26us/step - loss: 0.7044 - accuracy: 0.6812 - val_loss: 0.7434 - val_accuracy: 0.7209\n",
      "Epoch 263/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7039 - accuracy: 0.67 - 0s 26us/step - loss: 0.7008 - accuracy: 0.6830 - val_loss: 0.7420 - val_accuracy: 0.7250\n",
      "Epoch 264/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7025 - accuracy: 0.68 - 0s 26us/step - loss: 0.7013 - accuracy: 0.6893 - val_loss: 0.7408 - val_accuracy: 0.7273\n",
      "Epoch 265/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.68 - 0s 26us/step - loss: 0.7001 - accuracy: 0.6874 - val_loss: 0.7397 - val_accuracy: 0.7311\n",
      "Epoch 266/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.7024 - accuracy: 0.68 - 0s 27us/step - loss: 0.7019 - accuracy: 0.6883 - val_loss: 0.7384 - val_accuracy: 0.7339\n",
      "Epoch 267/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6981 - accuracy: 0.69 - 0s 27us/step - loss: 0.6973 - accuracy: 0.6962 - val_loss: 0.7374 - val_accuracy: 0.7352\n",
      "Epoch 268/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.69 - 0s 26us/step - loss: 0.6914 - accuracy: 0.6994 - val_loss: 0.7362 - val_accuracy: 0.7367\n",
      "Epoch 269/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6966 - accuracy: 0.69 - 0s 26us/step - loss: 0.6945 - accuracy: 0.6962 - val_loss: 0.7348 - val_accuracy: 0.7372\n",
      "Epoch 270/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6982 - accuracy: 0.68 - 0s 26us/step - loss: 0.6974 - accuracy: 0.6901 - val_loss: 0.7335 - val_accuracy: 0.7398\n",
      "Epoch 271/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.69 - 0s 26us/step - loss: 0.6941 - accuracy: 0.6966 - val_loss: 0.7323 - val_accuracy: 0.7423\n",
      "Epoch 272/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.69 - 0s 27us/step - loss: 0.6919 - accuracy: 0.6989 - val_loss: 0.7313 - val_accuracy: 0.7434\n",
      "Epoch 273/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.70 - 0s 27us/step - loss: 0.6897 - accuracy: 0.6994 - val_loss: 0.7301 - val_accuracy: 0.7457\n",
      "Epoch 274/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.70 - 0s 26us/step - loss: 0.6860 - accuracy: 0.7032 - val_loss: 0.7288 - val_accuracy: 0.7469\n",
      "Epoch 275/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.70 - 0s 26us/step - loss: 0.6802 - accuracy: 0.7117 - val_loss: 0.7276 - val_accuracy: 0.7490\n",
      "Epoch 276/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.70 - 0s 26us/step - loss: 0.6865 - accuracy: 0.7050 - val_loss: 0.7263 - val_accuracy: 0.7505\n",
      "Epoch 277/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.71 - 0s 29us/step - loss: 0.6817 - accuracy: 0.7088 - val_loss: 0.7250 - val_accuracy: 0.7515\n",
      "Epoch 278/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.71 - 0s 26us/step - loss: 0.6767 - accuracy: 0.7126 - val_loss: 0.7236 - val_accuracy: 0.7520\n",
      "Epoch 279/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6778 - accuracy: 0.71 - 0s 26us/step - loss: 0.6785 - accuracy: 0.7103 - val_loss: 0.7221 - val_accuracy: 0.7541\n",
      "Epoch 280/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.71 - 0s 26us/step - loss: 0.6745 - accuracy: 0.7142 - val_loss: 0.7207 - val_accuracy: 0.7566\n",
      "Epoch 281/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6748 - accuracy: 0.71 - 0s 26us/step - loss: 0.6760 - accuracy: 0.7138 - val_loss: 0.7196 - val_accuracy: 0.7582\n",
      "Epoch 282/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6750 - accuracy: 0.71 - 0s 28us/step - loss: 0.6770 - accuracy: 0.7126 - val_loss: 0.7185 - val_accuracy: 0.7592\n",
      "Epoch 283/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.72 - 0s 27us/step - loss: 0.6694 - accuracy: 0.7190 - val_loss: 0.7171 - val_accuracy: 0.7617\n",
      "Epoch 284/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6632 - accuracy: 0.72 - 0s 28us/step - loss: 0.6649 - accuracy: 0.7255 - val_loss: 0.7159 - val_accuracy: 0.7638\n",
      "Epoch 285/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6705 - accuracy: 0.72 - 0s 28us/step - loss: 0.6697 - accuracy: 0.7213 - val_loss: 0.7146 - val_accuracy: 0.7656\n",
      "Epoch 286/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6637 - accuracy: 0.72 - 0s 28us/step - loss: 0.6641 - accuracy: 0.7252 - val_loss: 0.7132 - val_accuracy: 0.7669\n",
      "Epoch 287/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6719 - accuracy: 0.71 - 0s 28us/step - loss: 0.6696 - accuracy: 0.7174 - val_loss: 0.7118 - val_accuracy: 0.7669\n",
      "Epoch 288/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6592 - accuracy: 0.72 - 0s 29us/step - loss: 0.6593 - accuracy: 0.7302 - val_loss: 0.7105 - val_accuracy: 0.7684\n",
      "Epoch 289/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.72 - 0s 28us/step - loss: 0.6647 - accuracy: 0.7239 - val_loss: 0.7090 - val_accuracy: 0.7704\n",
      "Epoch 290/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.72 - 0s 29us/step - loss: 0.6635 - accuracy: 0.7267 - val_loss: 0.7076 - val_accuracy: 0.7712\n",
      "Epoch 291/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.73 - 0s 29us/step - loss: 0.6577 - accuracy: 0.7297 - val_loss: 0.7065 - val_accuracy: 0.7720\n",
      "Epoch 292/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6587 - accuracy: 0.72 - 0s 28us/step - loss: 0.6553 - accuracy: 0.7287 - val_loss: 0.7057 - val_accuracy: 0.7735\n",
      "Epoch 293/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6533 - accuracy: 0.73 - 0s 26us/step - loss: 0.6565 - accuracy: 0.7294 - val_loss: 0.7043 - val_accuracy: 0.7753\n",
      "Epoch 294/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.72 - 0s 28us/step - loss: 0.6576 - accuracy: 0.7268 - val_loss: 0.7024 - val_accuracy: 0.7771\n",
      "Epoch 295/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.72 - 0s 26us/step - loss: 0.6530 - accuracy: 0.7331 - val_loss: 0.7007 - val_accuracy: 0.7786\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.72 - 0s 26us/step - loss: 0.6514 - accuracy: 0.7317 - val_loss: 0.6994 - val_accuracy: 0.7768\n",
      "Epoch 297/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.72 - 0s 25us/step - loss: 0.6539 - accuracy: 0.7310 - val_loss: 0.6985 - val_accuracy: 0.7766\n",
      "Epoch 298/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.72 - 0s 27us/step - loss: 0.6551 - accuracy: 0.7290 - val_loss: 0.6977 - val_accuracy: 0.7763\n",
      "Epoch 299/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.73 - 0s 27us/step - loss: 0.6506 - accuracy: 0.7298 - val_loss: 0.6960 - val_accuracy: 0.7791\n",
      "Epoch 300/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.73 - 0s 29us/step - loss: 0.6482 - accuracy: 0.7339 - val_loss: 0.6941 - val_accuracy: 0.7829\n",
      "Epoch 301/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6440 - accuracy: 0.74 - 0s 26us/step - loss: 0.6444 - accuracy: 0.7414 - val_loss: 0.6927 - val_accuracy: 0.7863\n",
      "Epoch 302/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.73 - 0s 26us/step - loss: 0.6423 - accuracy: 0.7398 - val_loss: 0.6916 - val_accuracy: 0.7875\n",
      "Epoch 303/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.74 - 0s 26us/step - loss: 0.6421 - accuracy: 0.7421 - val_loss: 0.6906 - val_accuracy: 0.7855\n",
      "Epoch 304/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6429 - accuracy: 0.74 - 0s 26us/step - loss: 0.6436 - accuracy: 0.7391 - val_loss: 0.6895 - val_accuracy: 0.7847\n",
      "Epoch 305/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6410 - accuracy: 0.74 - 0s 27us/step - loss: 0.6389 - accuracy: 0.7428 - val_loss: 0.6878 - val_accuracy: 0.7845\n",
      "Epoch 306/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6398 - accuracy: 0.73 - 0s 26us/step - loss: 0.6401 - accuracy: 0.7386 - val_loss: 0.6859 - val_accuracy: 0.7878\n",
      "Epoch 307/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6423 - accuracy: 0.73 - 0s 26us/step - loss: 0.6427 - accuracy: 0.7350 - val_loss: 0.6844 - val_accuracy: 0.7886\n",
      "Epoch 308/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.74 - 0s 26us/step - loss: 0.6297 - accuracy: 0.7459 - val_loss: 0.6830 - val_accuracy: 0.7891\n",
      "Epoch 309/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.74 - 0s 27us/step - loss: 0.6303 - accuracy: 0.7456 - val_loss: 0.6819 - val_accuracy: 0.7896\n",
      "Epoch 310/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.74 - 0s 26us/step - loss: 0.6343 - accuracy: 0.7408 - val_loss: 0.6809 - val_accuracy: 0.7903\n",
      "Epoch 311/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.75 - 0s 27us/step - loss: 0.6280 - accuracy: 0.7513 - val_loss: 0.6797 - val_accuracy: 0.7903\n",
      "Epoch 312/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6247 - accuracy: 0.74 - 0s 26us/step - loss: 0.6278 - accuracy: 0.7467 - val_loss: 0.6780 - val_accuracy: 0.7911\n",
      "Epoch 313/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.74 - 0s 27us/step - loss: 0.6299 - accuracy: 0.7443 - val_loss: 0.6766 - val_accuracy: 0.7921\n",
      "Epoch 314/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.74 - 0s 28us/step - loss: 0.6266 - accuracy: 0.7474 - val_loss: 0.6753 - val_accuracy: 0.7934\n",
      "Epoch 315/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.75 - 0s 27us/step - loss: 0.6244 - accuracy: 0.7473 - val_loss: 0.6744 - val_accuracy: 0.7942\n",
      "Epoch 316/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6187 - accuracy: 0.75 - 0s 26us/step - loss: 0.6190 - accuracy: 0.7548 - val_loss: 0.6727 - val_accuracy: 0.7962\n",
      "Epoch 317/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.76 - 0s 26us/step - loss: 0.6158 - accuracy: 0.7601 - val_loss: 0.6709 - val_accuracy: 0.7965\n",
      "Epoch 318/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.75 - 0s 27us/step - loss: 0.6184 - accuracy: 0.7541 - val_loss: 0.6694 - val_accuracy: 0.7962\n",
      "Epoch 319/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.74 - 0s 27us/step - loss: 0.6164 - accuracy: 0.7541 - val_loss: 0.6686 - val_accuracy: 0.7942\n",
      "Epoch 320/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.76 - 0s 26us/step - loss: 0.6093 - accuracy: 0.7589 - val_loss: 0.6674 - val_accuracy: 0.7949\n",
      "Epoch 321/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.75 - 0s 27us/step - loss: 0.6143 - accuracy: 0.7521 - val_loss: 0.6656 - val_accuracy: 0.7972\n",
      "Epoch 322/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6105 - accuracy: 0.75 - 0s 26us/step - loss: 0.6124 - accuracy: 0.7548 - val_loss: 0.6642 - val_accuracy: 0.8003\n",
      "Epoch 323/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6107 - accuracy: 0.75 - 0s 26us/step - loss: 0.6117 - accuracy: 0.7552 - val_loss: 0.6633 - val_accuracy: 0.8026\n",
      "Epoch 324/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.76 - 0s 27us/step - loss: 0.6083 - accuracy: 0.7585 - val_loss: 0.6623 - val_accuracy: 0.8031\n",
      "Epoch 325/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6036 - accuracy: 0.75 - 0s 28us/step - loss: 0.6026 - accuracy: 0.7621 - val_loss: 0.6610 - val_accuracy: 0.8036\n",
      "Epoch 326/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6053 - accuracy: 0.75 - 0s 28us/step - loss: 0.6058 - accuracy: 0.7585 - val_loss: 0.6596 - val_accuracy: 0.8039\n",
      "Epoch 327/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6103 - accuracy: 0.75 - 0s 29us/step - loss: 0.6053 - accuracy: 0.7595 - val_loss: 0.6577 - val_accuracy: 0.8046\n",
      "Epoch 328/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.75 - 0s 29us/step - loss: 0.6033 - accuracy: 0.7606 - val_loss: 0.6559 - val_accuracy: 0.8054\n",
      "Epoch 329/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5947 - accuracy: 0.76 - 0s 30us/step - loss: 0.5972 - accuracy: 0.7652 - val_loss: 0.6547 - val_accuracy: 0.8044\n",
      "Epoch 330/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.76 - 0s 30us/step - loss: 0.5939 - accuracy: 0.7663 - val_loss: 0.6538 - val_accuracy: 0.8049\n",
      "Epoch 331/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5978 - accuracy: 0.76 - 0s 28us/step - loss: 0.5983 - accuracy: 0.7654 - val_loss: 0.6529 - val_accuracy: 0.8064\n",
      "Epoch 332/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.77 - 0s 28us/step - loss: 0.5941 - accuracy: 0.7654 - val_loss: 0.6516 - val_accuracy: 0.8087\n",
      "Epoch 333/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.76 - 0s 28us/step - loss: 0.5949 - accuracy: 0.7647 - val_loss: 0.6499 - val_accuracy: 0.8098\n",
      "Epoch 334/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.76 - 0s 26us/step - loss: 0.5955 - accuracy: 0.7620 - val_loss: 0.6483 - val_accuracy: 0.8105\n",
      "Epoch 335/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.76 - 0s 27us/step - loss: 0.5902 - accuracy: 0.7689 - val_loss: 0.6473 - val_accuracy: 0.8100\n",
      "Epoch 336/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5885 - accuracy: 0.76 - 0s 27us/step - loss: 0.5897 - accuracy: 0.7642 - val_loss: 0.6461 - val_accuracy: 0.8080\n",
      "Epoch 337/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5931 - accuracy: 0.76 - 0s 27us/step - loss: 0.5894 - accuracy: 0.7670 - val_loss: 0.6445 - val_accuracy: 0.8077\n",
      "Epoch 338/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.76 - 0s 27us/step - loss: 0.5892 - accuracy: 0.7629 - val_loss: 0.6431 - val_accuracy: 0.8087\n",
      "Epoch 339/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.76 - 0s 28us/step - loss: 0.5901 - accuracy: 0.7608 - val_loss: 0.6415 - val_accuracy: 0.8108\n",
      "Epoch 340/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.76 - 0s 26us/step - loss: 0.5870 - accuracy: 0.7663 - val_loss: 0.6402 - val_accuracy: 0.8118\n",
      "Epoch 341/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.77 - 0s 27us/step - loss: 0.5851 - accuracy: 0.7702 - val_loss: 0.6395 - val_accuracy: 0.8118\n",
      "Epoch 342/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5844 - accuracy: 0.76 - 0s 26us/step - loss: 0.5816 - accuracy: 0.7681 - val_loss: 0.6384 - val_accuracy: 0.8115\n",
      "Epoch 343/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.77 - 0s 27us/step - loss: 0.5804 - accuracy: 0.7686 - val_loss: 0.6366 - val_accuracy: 0.8123\n",
      "Epoch 344/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5818 - accuracy: 0.76 - 0s 27us/step - loss: 0.5827 - accuracy: 0.7693 - val_loss: 0.6349 - val_accuracy: 0.8128\n",
      "Epoch 345/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5806 - accuracy: 0.76 - 0s 27us/step - loss: 0.5788 - accuracy: 0.7705 - val_loss: 0.6338 - val_accuracy: 0.8136\n",
      "Epoch 346/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.75 - 0s 27us/step - loss: 0.5820 - accuracy: 0.7651 - val_loss: 0.6328 - val_accuracy: 0.8136\n",
      "Epoch 347/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5816 - accuracy: 0.77 - 0s 27us/step - loss: 0.5807 - accuracy: 0.7684 - val_loss: 0.6317 - val_accuracy: 0.8136\n",
      "Epoch 348/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.77 - 0s 26us/step - loss: 0.5719 - accuracy: 0.7749 - val_loss: 0.6305 - val_accuracy: 0.8133\n",
      "Epoch 349/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.77 - 0s 27us/step - loss: 0.5727 - accuracy: 0.7696 - val_loss: 0.6291 - val_accuracy: 0.8138\n",
      "Epoch 350/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.77 - 0s 26us/step - loss: 0.5759 - accuracy: 0.7686 - val_loss: 0.6276 - val_accuracy: 0.8141\n",
      "Epoch 351/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.78 - 0s 27us/step - loss: 0.5662 - accuracy: 0.7802 - val_loss: 0.6263 - val_accuracy: 0.8141\n",
      "Epoch 352/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.77 - 0s 27us/step - loss: 0.5686 - accuracy: 0.7749 - val_loss: 0.6254 - val_accuracy: 0.8156\n",
      "Epoch 353/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.77 - 0s 27us/step - loss: 0.5654 - accuracy: 0.7755 - val_loss: 0.6242 - val_accuracy: 0.8154\n",
      "Epoch 354/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.77 - 0s 27us/step - loss: 0.5661 - accuracy: 0.7749 - val_loss: 0.6228 - val_accuracy: 0.8164\n",
      "Epoch 355/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.77 - 0s 28us/step - loss: 0.5688 - accuracy: 0.7727 - val_loss: 0.6212 - val_accuracy: 0.8161\n",
      "Epoch 356/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.77 - 0s 28us/step - loss: 0.5641 - accuracy: 0.7744 - val_loss: 0.6201 - val_accuracy: 0.8164\n",
      "Epoch 357/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.77 - 0s 28us/step - loss: 0.5664 - accuracy: 0.7722 - val_loss: 0.6192 - val_accuracy: 0.8169\n",
      "Epoch 358/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.77 - 0s 28us/step - loss: 0.5589 - accuracy: 0.7765 - val_loss: 0.6182 - val_accuracy: 0.8166\n",
      "Epoch 359/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5570 - accuracy: 0.77 - 0s 28us/step - loss: 0.5596 - accuracy: 0.7768 - val_loss: 0.6164 - val_accuracy: 0.8169\n",
      "Epoch 360/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.77 - 0s 28us/step - loss: 0.5583 - accuracy: 0.7769 - val_loss: 0.6150 - val_accuracy: 0.8177\n",
      "Epoch 361/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.76 - 0s 28us/step - loss: 0.5572 - accuracy: 0.7781 - val_loss: 0.6142 - val_accuracy: 0.8184\n",
      "Epoch 362/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.78 - 0s 30us/step - loss: 0.5538 - accuracy: 0.7785 - val_loss: 0.6136 - val_accuracy: 0.8179\n",
      "Epoch 363/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.77 - 0s 31us/step - loss: 0.5618 - accuracy: 0.7758 - val_loss: 0.6123 - val_accuracy: 0.8184\n",
      "Epoch 364/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5530 - accuracy: 0.78 - 0s 30us/step - loss: 0.5546 - accuracy: 0.7788 - val_loss: 0.6105 - val_accuracy: 0.8192\n",
      "Epoch 365/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.78 - 0s 29us/step - loss: 0.5518 - accuracy: 0.7811 - val_loss: 0.6090 - val_accuracy: 0.8210\n",
      "Epoch 366/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5445 - accuracy: 0.78 - 0s 28us/step - loss: 0.5501 - accuracy: 0.7820 - val_loss: 0.6076 - val_accuracy: 0.8223\n",
      "Epoch 367/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.78 - 0s 28us/step - loss: 0.5487 - accuracy: 0.7823 - val_loss: 0.6066 - val_accuracy: 0.8220\n",
      "Epoch 368/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.77 - 0s 27us/step - loss: 0.5444 - accuracy: 0.7818 - val_loss: 0.6063 - val_accuracy: 0.8212\n",
      "Epoch 369/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.78 - 0s 28us/step - loss: 0.5436 - accuracy: 0.7831 - val_loss: 0.6055 - val_accuracy: 0.8223\n",
      "Epoch 370/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.78 - 0s 28us/step - loss: 0.5461 - accuracy: 0.7823 - val_loss: 0.6035 - val_accuracy: 0.8230\n",
      "Epoch 371/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.78 - 0s 27us/step - loss: 0.5382 - accuracy: 0.7865 - val_loss: 0.6017 - val_accuracy: 0.8253\n",
      "Epoch 372/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5367 - accuracy: 0.78 - 0s 27us/step - loss: 0.5392 - accuracy: 0.7847 - val_loss: 0.6010 - val_accuracy: 0.8243\n",
      "Epoch 373/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.78 - 0s 26us/step - loss: 0.5373 - accuracy: 0.7877 - val_loss: 0.6007 - val_accuracy: 0.8248\n",
      "Epoch 374/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.78 - 0s 26us/step - loss: 0.5380 - accuracy: 0.7878 - val_loss: 0.5997 - val_accuracy: 0.8251\n",
      "Epoch 375/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.78 - 0s 26us/step - loss: 0.5376 - accuracy: 0.7845 - val_loss: 0.5983 - val_accuracy: 0.8261\n",
      "Epoch 376/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.78 - 0s 26us/step - loss: 0.5344 - accuracy: 0.7871 - val_loss: 0.5964 - val_accuracy: 0.8261\n",
      "Epoch 377/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.78 - 0s 26us/step - loss: 0.5353 - accuracy: 0.7837 - val_loss: 0.5949 - val_accuracy: 0.8274\n",
      "Epoch 378/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.79 - 0s 25us/step - loss: 0.5274 - accuracy: 0.7900 - val_loss: 0.5943 - val_accuracy: 0.8266\n",
      "Epoch 379/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.78 - 0s 25us/step - loss: 0.5321 - accuracy: 0.7887 - val_loss: 0.5937 - val_accuracy: 0.8264\n",
      "Epoch 380/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.79 - 0s 26us/step - loss: 0.5278 - accuracy: 0.7901 - val_loss: 0.5927 - val_accuracy: 0.8261\n",
      "Epoch 381/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5254 - accuracy: 0.79 - 0s 25us/step - loss: 0.5311 - accuracy: 0.7852 - val_loss: 0.5912 - val_accuracy: 0.8266\n",
      "Epoch 382/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.78 - 0s 26us/step - loss: 0.5303 - accuracy: 0.7867 - val_loss: 0.5896 - val_accuracy: 0.8274\n",
      "Epoch 383/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.79 - 0s 27us/step - loss: 0.5241 - accuracy: 0.7914 - val_loss: 0.5882 - val_accuracy: 0.8312\n",
      "Epoch 384/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5215 - accuracy: 0.79 - 0s 27us/step - loss: 0.5242 - accuracy: 0.7938 - val_loss: 0.5873 - val_accuracy: 0.8320\n",
      "Epoch 385/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.79 - 0s 27us/step - loss: 0.5224 - accuracy: 0.7944 - val_loss: 0.5868 - val_accuracy: 0.8297\n",
      "Epoch 386/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.79 - 0s 26us/step - loss: 0.5189 - accuracy: 0.7945 - val_loss: 0.5862 - val_accuracy: 0.8289\n",
      "Epoch 387/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5195 - accuracy: 0.79 - 0s 25us/step - loss: 0.5256 - accuracy: 0.7874 - val_loss: 0.5847 - val_accuracy: 0.8289\n",
      "Epoch 388/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.79 - 0s 26us/step - loss: 0.5200 - accuracy: 0.7947 - val_loss: 0.5831 - val_accuracy: 0.8287\n",
      "Epoch 389/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.79 - 0s 26us/step - loss: 0.5225 - accuracy: 0.7904 - val_loss: 0.5822 - val_accuracy: 0.8287\n",
      "Epoch 390/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.79 - 0s 26us/step - loss: 0.5173 - accuracy: 0.7935 - val_loss: 0.5816 - val_accuracy: 0.8312\n",
      "Epoch 391/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5145 - accuracy: 0.79 - 0s 26us/step - loss: 0.5149 - accuracy: 0.7961 - val_loss: 0.5804 - val_accuracy: 0.8320\n",
      "Epoch 392/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.78 - 0s 27us/step - loss: 0.5202 - accuracy: 0.7889 - val_loss: 0.5790 - val_accuracy: 0.8330\n",
      "Epoch 393/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5155 - accuracy: 0.79 - 0s 27us/step - loss: 0.5186 - accuracy: 0.7908 - val_loss: 0.5779 - val_accuracy: 0.8325\n",
      "Epoch 394/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.79 - 0s 26us/step - loss: 0.5130 - accuracy: 0.7951 - val_loss: 0.5772 - val_accuracy: 0.8330\n",
      "Epoch 395/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.79 - 0s 26us/step - loss: 0.5124 - accuracy: 0.7930 - val_loss: 0.5761 - val_accuracy: 0.8332\n",
      "Epoch 396/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5152 - accuracy: 0.79 - 0s 26us/step - loss: 0.5138 - accuracy: 0.7917 - val_loss: 0.5752 - val_accuracy: 0.8320\n",
      "Epoch 397/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.79 - 0s 27us/step - loss: 0.5108 - accuracy: 0.7945 - val_loss: 0.5743 - val_accuracy: 0.8315\n",
      "Epoch 398/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.79 - 0s 28us/step - loss: 0.5112 - accuracy: 0.7921 - val_loss: 0.5731 - val_accuracy: 0.8307\n",
      "Epoch 399/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.79 - 0s 28us/step - loss: 0.5084 - accuracy: 0.7944 - val_loss: 0.5717 - val_accuracy: 0.8322\n",
      "Epoch 400/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.79 - 0s 28us/step - loss: 0.5030 - accuracy: 0.7964 - val_loss: 0.5705 - val_accuracy: 0.8330\n",
      "Epoch 401/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.79 - 0s 29us/step - loss: 0.5071 - accuracy: 0.7926 - val_loss: 0.5695 - val_accuracy: 0.8348\n",
      "Epoch 402/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.79 - 0s 29us/step - loss: 0.5000 - accuracy: 0.7985 - val_loss: 0.5690 - val_accuracy: 0.8350\n",
      "Epoch 403/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.80 - 0s 29us/step - loss: 0.5020 - accuracy: 0.8000 - val_loss: 0.5686 - val_accuracy: 0.8355\n",
      "Epoch 404/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5017 - accuracy: 0.79 - 0s 29us/step - loss: 0.5016 - accuracy: 0.7984 - val_loss: 0.5677 - val_accuracy: 0.8345\n",
      "Epoch 405/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5046 - accuracy: 0.79 - 0s 27us/step - loss: 0.5037 - accuracy: 0.7963 - val_loss: 0.5662 - val_accuracy: 0.8332\n",
      "Epoch 406/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.79 - 0s 26us/step - loss: 0.5011 - accuracy: 0.7970 - val_loss: 0.5648 - val_accuracy: 0.8340\n",
      "Epoch 407/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.79 - 0s 28us/step - loss: 0.5030 - accuracy: 0.7952 - val_loss: 0.5638 - val_accuracy: 0.8353\n",
      "Epoch 408/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.79 - 0s 26us/step - loss: 0.5056 - accuracy: 0.7940 - val_loss: 0.5633 - val_accuracy: 0.8353\n",
      "Epoch 409/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.79 - 0s 27us/step - loss: 0.5011 - accuracy: 0.7972 - val_loss: 0.5624 - val_accuracy: 0.8361\n",
      "Epoch 410/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5048 - accuracy: 0.79 - 0s 27us/step - loss: 0.4983 - accuracy: 0.7971 - val_loss: 0.5615 - val_accuracy: 0.8366\n",
      "Epoch 411/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.80 - 0s 27us/step - loss: 0.4964 - accuracy: 0.7992 - val_loss: 0.5604 - val_accuracy: 0.8363\n",
      "Epoch 412/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4904 - accuracy: 0.80 - 0s 28us/step - loss: 0.4912 - accuracy: 0.8023 - val_loss: 0.5593 - val_accuracy: 0.8363\n",
      "Epoch 413/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5052 - accuracy: 0.79 - 0s 26us/step - loss: 0.4998 - accuracy: 0.7954 - val_loss: 0.5584 - val_accuracy: 0.8350\n",
      "Epoch 414/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.79 - 0s 25us/step - loss: 0.4951 - accuracy: 0.7970 - val_loss: 0.5571 - val_accuracy: 0.8345\n",
      "Epoch 415/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.79 - 0s 28us/step - loss: 0.4909 - accuracy: 0.7986 - val_loss: 0.5561 - val_accuracy: 0.8350\n",
      "Epoch 416/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4901 - accuracy: 0.80 - 0s 26us/step - loss: 0.4886 - accuracy: 0.8032 - val_loss: 0.5553 - val_accuracy: 0.8355\n",
      "Epoch 417/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4892 - accuracy: 0.80 - 0s 27us/step - loss: 0.4889 - accuracy: 0.8018 - val_loss: 0.5549 - val_accuracy: 0.8378\n",
      "Epoch 418/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.79 - 0s 28us/step - loss: 0.4890 - accuracy: 0.7993 - val_loss: 0.5544 - val_accuracy: 0.8386\n",
      "Epoch 419/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.79 - 0s 26us/step - loss: 0.4877 - accuracy: 0.8001 - val_loss: 0.5532 - val_accuracy: 0.8399\n",
      "Epoch 420/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.79 - 0s 26us/step - loss: 0.4866 - accuracy: 0.8003 - val_loss: 0.5515 - val_accuracy: 0.8384\n",
      "Epoch 421/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4994 - accuracy: 0.79 - 0s 27us/step - loss: 0.4891 - accuracy: 0.7982 - val_loss: 0.5503 - val_accuracy: 0.8373\n",
      "Epoch 422/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4838 - accuracy: 0.80 - 0s 28us/step - loss: 0.4850 - accuracy: 0.8016 - val_loss: 0.5495 - val_accuracy: 0.8373\n",
      "Epoch 423/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.80 - 0s 28us/step - loss: 0.4810 - accuracy: 0.8027 - val_loss: 0.5494 - val_accuracy: 0.8378\n",
      "Epoch 424/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4899 - accuracy: 0.79 - 0s 28us/step - loss: 0.4869 - accuracy: 0.8003 - val_loss: 0.5489 - val_accuracy: 0.8389\n",
      "Epoch 425/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4809 - accuracy: 0.80 - 0s 27us/step - loss: 0.4807 - accuracy: 0.8041 - val_loss: 0.5477 - val_accuracy: 0.8396\n",
      "Epoch 426/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4809 - accuracy: 0.80 - 0s 27us/step - loss: 0.4783 - accuracy: 0.8045 - val_loss: 0.5461 - val_accuracy: 0.8394\n",
      "Epoch 427/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4828 - accuracy: 0.80 - 0s 26us/step - loss: 0.4824 - accuracy: 0.8024 - val_loss: 0.5451 - val_accuracy: 0.8407\n",
      "Epoch 428/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.81 - 0s 27us/step - loss: 0.4715 - accuracy: 0.8097 - val_loss: 0.5448 - val_accuracy: 0.8407\n",
      "Epoch 429/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4771 - accuracy: 0.80 - 0s 27us/step - loss: 0.4763 - accuracy: 0.8019 - val_loss: 0.5444 - val_accuracy: 0.8404\n",
      "Epoch 430/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.80 - 0s 26us/step - loss: 0.4764 - accuracy: 0.8073 - val_loss: 0.5434 - val_accuracy: 0.8409\n",
      "Epoch 431/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4858 - accuracy: 0.79 - 0s 26us/step - loss: 0.4834 - accuracy: 0.8004 - val_loss: 0.5417 - val_accuracy: 0.8401\n",
      "Epoch 432/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.80 - 0s 27us/step - loss: 0.4749 - accuracy: 0.8044 - val_loss: 0.5405 - val_accuracy: 0.8407\n",
      "Epoch 433/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4771 - accuracy: 0.80 - 0s 26us/step - loss: 0.4732 - accuracy: 0.8036 - val_loss: 0.5399 - val_accuracy: 0.8407\n",
      "Epoch 434/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4797 - accuracy: 0.79 - 0s 27us/step - loss: 0.4774 - accuracy: 0.8016 - val_loss: 0.5398 - val_accuracy: 0.8424\n",
      "Epoch 435/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.81 - 0s 26us/step - loss: 0.4646 - accuracy: 0.8103 - val_loss: 0.5389 - val_accuracy: 0.8437\n",
      "Epoch 436/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.81 - 0s 27us/step - loss: 0.4636 - accuracy: 0.8115 - val_loss: 0.5377 - val_accuracy: 0.8440\n",
      "Epoch 437/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.81 - 0s 28us/step - loss: 0.4625 - accuracy: 0.8114 - val_loss: 0.5367 - val_accuracy: 0.8442\n",
      "Epoch 438/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4701 - accuracy: 0.80 - 0s 29us/step - loss: 0.4684 - accuracy: 0.8062 - val_loss: 0.5361 - val_accuracy: 0.8440\n",
      "Epoch 439/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4724 - accuracy: 0.80 - 0s 28us/step - loss: 0.4719 - accuracy: 0.8020 - val_loss: 0.5354 - val_accuracy: 0.8440\n",
      "Epoch 440/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.80 - 0s 28us/step - loss: 0.4652 - accuracy: 0.8088 - val_loss: 0.5344 - val_accuracy: 0.8435\n",
      "Epoch 441/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4655 - accuracy: 0.80 - 0s 30us/step - loss: 0.4642 - accuracy: 0.8083 - val_loss: 0.5334 - val_accuracy: 0.8430\n",
      "Epoch 442/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.80 - 0s 28us/step - loss: 0.4678 - accuracy: 0.8072 - val_loss: 0.5325 - val_accuracy: 0.8432\n",
      "Epoch 443/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.80 - 0s 27us/step - loss: 0.4683 - accuracy: 0.8037 - val_loss: 0.5320 - val_accuracy: 0.8437\n",
      "Epoch 444/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.80 - 0s 26us/step - loss: 0.4600 - accuracy: 0.8082 - val_loss: 0.5317 - val_accuracy: 0.8437\n",
      "Epoch 445/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.80 - 0s 26us/step - loss: 0.4673 - accuracy: 0.8052 - val_loss: 0.5305 - val_accuracy: 0.8440\n",
      "Epoch 446/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4601 - accuracy: 0.81 - 0s 27us/step - loss: 0.4569 - accuracy: 0.8121 - val_loss: 0.5290 - val_accuracy: 0.8447\n",
      "Epoch 447/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.80 - 0s 27us/step - loss: 0.4632 - accuracy: 0.8051 - val_loss: 0.5281 - val_accuracy: 0.8445\n",
      "Epoch 448/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.80 - 0s 27us/step - loss: 0.4587 - accuracy: 0.8098 - val_loss: 0.5279 - val_accuracy: 0.8453\n",
      "Epoch 449/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.80 - 0s 27us/step - loss: 0.4636 - accuracy: 0.8037 - val_loss: 0.5280 - val_accuracy: 0.8437\n",
      "Epoch 450/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.80 - 0s 26us/step - loss: 0.4594 - accuracy: 0.8063 - val_loss: 0.5272 - val_accuracy: 0.8440\n",
      "Epoch 451/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4588 - accuracy: 0.80 - 0s 27us/step - loss: 0.4584 - accuracy: 0.8080 - val_loss: 0.5256 - val_accuracy: 0.8453\n",
      "Epoch 452/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4606 - accuracy: 0.80 - 0s 27us/step - loss: 0.4619 - accuracy: 0.8038 - val_loss: 0.5241 - val_accuracy: 0.8450\n",
      "Epoch 453/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.80 - 0s 26us/step - loss: 0.4569 - accuracy: 0.8082 - val_loss: 0.5234 - val_accuracy: 0.8450\n",
      "Epoch 454/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.81 - 0s 26us/step - loss: 0.4505 - accuracy: 0.8125 - val_loss: 0.5234 - val_accuracy: 0.8442\n",
      "Epoch 455/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.81 - 0s 27us/step - loss: 0.4509 - accuracy: 0.8136 - val_loss: 0.5233 - val_accuracy: 0.8440\n",
      "Epoch 456/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.81 - 0s 27us/step - loss: 0.4481 - accuracy: 0.8129 - val_loss: 0.5226 - val_accuracy: 0.8445\n",
      "Epoch 457/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.81 - 0s 26us/step - loss: 0.4531 - accuracy: 0.8094 - val_loss: 0.5213 - val_accuracy: 0.8447\n",
      "Epoch 458/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.81 - 0s 26us/step - loss: 0.4523 - accuracy: 0.8107 - val_loss: 0.5198 - val_accuracy: 0.8463\n",
      "Epoch 459/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4463 - accuracy: 0.81 - 0s 27us/step - loss: 0.4488 - accuracy: 0.8117 - val_loss: 0.5192 - val_accuracy: 0.8463\n",
      "Epoch 460/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4456 - accuracy: 0.81 - 0s 26us/step - loss: 0.4423 - accuracy: 0.8182 - val_loss: 0.5191 - val_accuracy: 0.8463\n",
      "Epoch 461/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4490 - accuracy: 0.81 - 0s 26us/step - loss: 0.4487 - accuracy: 0.8121 - val_loss: 0.5189 - val_accuracy: 0.8463\n",
      "Epoch 462/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4564 - accuracy: 0.80 - 0s 25us/step - loss: 0.4499 - accuracy: 0.8121 - val_loss: 0.5177 - val_accuracy: 0.8455\n",
      "Epoch 463/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4548 - accuracy: 0.80 - 0s 25us/step - loss: 0.4516 - accuracy: 0.8090 - val_loss: 0.5162 - val_accuracy: 0.8450\n",
      "Epoch 464/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.80 - 0s 27us/step - loss: 0.4497 - accuracy: 0.8077 - val_loss: 0.5154 - val_accuracy: 0.8453\n",
      "Epoch 465/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.81 - 0s 27us/step - loss: 0.4423 - accuracy: 0.8138 - val_loss: 0.5151 - val_accuracy: 0.8447\n",
      "Epoch 466/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.80 - 0s 26us/step - loss: 0.4517 - accuracy: 0.8071 - val_loss: 0.5145 - val_accuracy: 0.8470\n",
      "Epoch 467/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.81 - 0s 26us/step - loss: 0.4451 - accuracy: 0.8133 - val_loss: 0.5138 - val_accuracy: 0.8475\n",
      "Epoch 468/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4421 - accuracy: 0.81 - 0s 25us/step - loss: 0.4431 - accuracy: 0.8155 - val_loss: 0.5130 - val_accuracy: 0.8478\n",
      "Epoch 469/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.81 - 0s 25us/step - loss: 0.4371 - accuracy: 0.8180 - val_loss: 0.5121 - val_accuracy: 0.8478\n",
      "Epoch 470/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4410 - accuracy: 0.81 - 0s 27us/step - loss: 0.4414 - accuracy: 0.8142 - val_loss: 0.5113 - val_accuracy: 0.8473\n",
      "Epoch 471/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.81 - 0s 27us/step - loss: 0.4437 - accuracy: 0.8130 - val_loss: 0.5108 - val_accuracy: 0.8460\n",
      "Epoch 472/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.81 - 0s 27us/step - loss: 0.4446 - accuracy: 0.8117 - val_loss: 0.5100 - val_accuracy: 0.8460\n",
      "Epoch 473/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4359 - accuracy: 0.81 - 0s 26us/step - loss: 0.4343 - accuracy: 0.8178 - val_loss: 0.5093 - val_accuracy: 0.8473\n",
      "Epoch 474/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.82 - 0s 27us/step - loss: 0.4291 - accuracy: 0.8216 - val_loss: 0.5085 - val_accuracy: 0.8491\n",
      "Epoch 475/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.81 - 0s 28us/step - loss: 0.4452 - accuracy: 0.8103 - val_loss: 0.5080 - val_accuracy: 0.8483\n",
      "Epoch 476/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4401 - accuracy: 0.81 - 0s 29us/step - loss: 0.4384 - accuracy: 0.8149 - val_loss: 0.5078 - val_accuracy: 0.8478\n",
      "Epoch 477/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.81 - 0s 30us/step - loss: 0.4394 - accuracy: 0.8152 - val_loss: 0.5072 - val_accuracy: 0.8481\n",
      "Epoch 478/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.81 - 0s 31us/step - loss: 0.4344 - accuracy: 0.8170 - val_loss: 0.5063 - val_accuracy: 0.8475\n",
      "Epoch 479/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.82 - 0s 28us/step - loss: 0.4291 - accuracy: 0.8188 - val_loss: 0.5050 - val_accuracy: 0.8491\n",
      "Epoch 480/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4381 - accuracy: 0.81 - 0s 28us/step - loss: 0.4377 - accuracy: 0.8118 - val_loss: 0.5040 - val_accuracy: 0.8498\n",
      "Epoch 481/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.82 - 0s 28us/step - loss: 0.4303 - accuracy: 0.8177 - val_loss: 0.5036 - val_accuracy: 0.8496\n",
      "Epoch 482/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4355 - accuracy: 0.81 - 0s 26us/step - loss: 0.4309 - accuracy: 0.8169 - val_loss: 0.5035 - val_accuracy: 0.8491\n",
      "Epoch 483/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.81 - 0s 26us/step - loss: 0.4294 - accuracy: 0.8176 - val_loss: 0.5032 - val_accuracy: 0.8486\n",
      "Epoch 484/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4317 - accuracy: 0.81 - 0s 28us/step - loss: 0.4320 - accuracy: 0.8165 - val_loss: 0.5022 - val_accuracy: 0.8491\n",
      "Epoch 485/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4388 - accuracy: 0.81 - 0s 28us/step - loss: 0.4338 - accuracy: 0.8127 - val_loss: 0.5013 - val_accuracy: 0.8498\n",
      "Epoch 486/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.81 - 0s 28us/step - loss: 0.4318 - accuracy: 0.8136 - val_loss: 0.5006 - val_accuracy: 0.8498\n",
      "Epoch 487/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.81 - 0s 27us/step - loss: 0.4271 - accuracy: 0.8193 - val_loss: 0.5003 - val_accuracy: 0.8488\n",
      "Epoch 488/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4210 - accuracy: 0.82 - 0s 25us/step - loss: 0.4219 - accuracy: 0.8211 - val_loss: 0.4998 - val_accuracy: 0.8488\n",
      "Epoch 489/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4174 - accuracy: 0.82 - 0s 26us/step - loss: 0.4190 - accuracy: 0.8232 - val_loss: 0.4987 - val_accuracy: 0.8493\n",
      "Epoch 490/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.81 - 0s 25us/step - loss: 0.4285 - accuracy: 0.8166 - val_loss: 0.4976 - val_accuracy: 0.8506\n",
      "Epoch 491/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.81 - 0s 25us/step - loss: 0.4261 - accuracy: 0.8169 - val_loss: 0.4968 - val_accuracy: 0.8506\n",
      "Epoch 492/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4358 - accuracy: 0.81 - 0s 25us/step - loss: 0.4330 - accuracy: 0.8117 - val_loss: 0.4969 - val_accuracy: 0.8504\n",
      "Epoch 493/1000\n",
      "15663/15663 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.81 - 0s 27us/step - loss: 0.4279 - accuracy: 0.8146 - val_loss: 0.4973 - val_accuracy: 0.8478\n"
     ]
    }
   ],
   "source": [
    "history_dropout = model_drop.fit(x_train, y_train, batch_size=10000, validation_data=(x_test, y_test), epochs=1000,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Cross-Validation</h3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    #Define Keras Model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train, epochs=17, batch_size=25, verbose=0)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(X_testset, Y_testset)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100),'%')\n",
    "print('Loss: %.2f' % (accuracy[0]*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(0, 493))\n",
    "y1 = history_dropout.history['accuracy'] \n",
    "y2 = history_dropout.history['val_accuracy'] \n",
    "yhat1 = savgol_filter(y1, 51, 5) # window size 51, polynomial order 3\n",
    "yhat2 = savgol_filter(y2, 51, 5) # window size 51, polynomial order 3\n",
    "plt.plot(x,y1, color='honeydew')\n",
    "plt.plot(x,y2, color='azure')\n",
    "plt.plot(x,yhat1, color='green')\n",
    "plt.plot(x,yhat2, color='blue')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['', '', 'Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = hist.history['accuracy']\n",
    "Y = hist.history['val_accuracy'] \n",
    "x_y = [x - y for x, y in zip(X, Y)]\n",
    "\n",
    "x = list(range(0, 25))\n",
    "y = x_y\n",
    "yhat = savgol_filter(y, 21, 5) # window size 51, polynomial order 3\n",
    "plt.plot(x,y, color='mistyrose')\n",
    "plt.plot(x,yhat, color='red')\n",
    "plt.title('Accuracy difference betwen train and test data')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['', '', 'train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(0, 25))\n",
    "y1 = hist.history['loss'] \n",
    "y2 = hist.history['val_loss'] \n",
    "yhat1 = savgol_filter(y1, 21, 2) # window size 501, polynomial order 3\n",
    "yhat2 = savgol_filter(y2, 21, 2) # window size 501, polynomial order 3\n",
    "plt.plot(x,y1, color='honeydew')\n",
    "plt.plot(x,y2, color='azure')\n",
    "plt.plot(x,yhat1, color='green')\n",
    "plt.plot(x,yhat2, color='blue')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['', '', 'train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = hist.history['val_loss']\n",
    "Y = hist.history['loss'] \n",
    "x_y = [x - y for x, y in zip(X, Y)]\n",
    "\n",
    "x = list(range(0, 25))\n",
    "y = x_y\n",
    "yhat = savgol_filter(y, 21, 3) # window size 51, polynomial order 3\n",
    "plt.plot(x,y, color='mistyrose')\n",
    "plt.plot(x,yhat, color='red')\n",
    "plt.title('Loss difference betwen train and test data')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['', '', 'train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Predictions</h3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_trainset)\n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('%d (expected %d)' % (predictions[i], Y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Plot the ANN</h3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'None'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-7a0041453d47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mann_viz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Auhtor_Prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ann_visualizer\\visualize.py\u001b[0m in \u001b[0;36mann_viz\u001b[1;34m(model, view, filename, title)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0minput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mhidden_layers_nr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'None'"
     ]
    }
   ],
   "source": [
    "ann_viz(model, title=\"Auhtor_Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question bonus\n",
    "- Embedding repreent mots avec des vecteurs, plusoeurs dimensions, vector de valeurs (re faire comme ancien homework). Compare vector to see if word are close or not each other\n",
    "- Embeding une facon de representer mots into data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise into the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1046</td>\n",
       "      <td>11510</td>\n",
       "      <td>3666</td>\n",
       "      <td>13</td>\n",
       "      <td>2368</td>\n",
       "      <td>1313</td>\n",
       "      <td>31894</td>\n",
       "      <td>20049</td>\n",
       "      <td>1907</td>\n",
       "      <td>5078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>31895</td>\n",
       "      <td>5359</td>\n",
       "      <td>211</td>\n",
       "      <td>1065</td>\n",
       "      <td>118</td>\n",
       "      <td>771</td>\n",
       "      <td>1086</td>\n",
       "      <td>3119</td>\n",
       "      <td>20050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>4045</td>\n",
       "      <td>873</td>\n",
       "      <td>3007</td>\n",
       "      <td>1375</td>\n",
       "      <td>13</td>\n",
       "      <td>1793</td>\n",
       "      <td>31897</td>\n",
       "      <td>31898</td>\n",
       "      <td>24547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>711</td>\n",
       "      <td>24552</td>\n",
       "      <td>45023</td>\n",
       "      <td>2907</td>\n",
       "      <td>31901</td>\n",
       "      <td>45024</td>\n",
       "      <td>2011</td>\n",
       "      <td>16912</td>\n",
       "      <td>24553</td>\n",
       "      <td>20054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31903</td>\n",
       "      <td>108</td>\n",
       "      <td>966</td>\n",
       "      <td>45026</td>\n",
       "      <td>4604</td>\n",
       "      <td>3121</td>\n",
       "      <td>442</td>\n",
       "      <td>1754</td>\n",
       "      <td>31904</td>\n",
       "      <td>45027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...    246    247    248  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...   1046  11510   3666   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...      4  31895   5359   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    150   4045    873   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    711  24552  45023   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...  31903    108    966   \n",
       "\n",
       "     249    250    251    252    253    254    255  \n",
       "0     13   2368   1313  31894  20049   1907   5078  \n",
       "1    211   1065    118    771   1086   3119  20050  \n",
       "2   3007   1375     13   1793  31897  31898  24547  \n",
       "3   2907  31901  45024   2011  16912  24553  20054  \n",
       "4  45026   4604   3121    442   1754  31904  45027  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_noise = pd.DataFrame(docs)\n",
    "docs_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [0]*9789 + [1]*9790\n",
    "my_list1 = [0]*9789 + [1]*9790\n",
    "my_list2 = [0]*9789 + [1]*9790\n",
    "my_list3 = [0]*9789 + [1]*9790\n",
    "my_list4 = [0]*9789 + [1]*9790\n",
    "random.shuffle(my_list)\n",
    "random.shuffle(my_list1)\n",
    "random.shuffle(my_list2)\n",
    "random.shuffle(my_list3)\n",
    "random.shuffle(my_list4)\n",
    "my_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_noise[51] = docs_noise[51] + my_list\n",
    "docs_noise[102] = docs_noise[102] + my_list1\n",
    "docs_noise[153] = docs_noise[153] + my_list2\n",
    "docs_noise[204] = docs_noise[204] + my_list3\n",
    "docs_noise[255] = docs_noise[255] + my_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_noise = docs_noise.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 20049,  1907,  5079],\n",
       "       [    0,     0,     0, ...,  1086,  3119, 20051],\n",
       "       [    0,     0,     0, ..., 31897, 31898, 24547],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   697,   152,  9350],\n",
       "       [    0,     0,     0, ...,  1574, 21725,    95],\n",
       "       [    0,     0,     0, ...,  4295,   444, 50417]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 20049,  1907,  5079],\n",
       "       [    0,     0,     0, ...,  1086,  3119, 20051],\n",
       "       [    0,     0,     0, ..., 31897, 31898, 24547],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   697,   152,  9350],\n",
       "       [    0,     0,     0, ...,  1574, 21725,    95],\n",
       "       [    0,     0,     0, ...,  4295,   444, 50417]])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs_noise) + 3\n",
    "embedding_dims = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [19579, 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-348-c99b0f496ce9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [19579, 6]"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-333-944537ef624b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m hist = model.fit(x_train, y_train,\n\u001b[0;32m      3\u001b[0m                  \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-331-254bf1eca3b7>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(embedding_dims, optimizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 463\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             dtype=self.dtype)\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         weight = K.variable(initializer(shape, dtype=dtype),\n\u001b[0m\u001b[0;32m    280\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         x = K.random_uniform(shape, self.minval, self.maxval,\n\u001b[1;32m--> 115\u001b[1;33m                              dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[0;32m   4355\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         return tf_keras_backend.random_uniform(\n\u001b[1;32m-> 4357\u001b[1;33m             shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[0;32m   5596\u001b[0m     \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5597\u001b[0m   return random_ops.random_uniform(\n\u001b[1;32m-> 5598\u001b[1;33m       shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n\u001b[0m\u001b[0;32m   5599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[1;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"random_uniform\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[0mminval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"min\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mshape_tensor\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    962\u001b[0m       \u001b[1;31m# not convertible to Tensors becasue of mixed content.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[0;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[0;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[1;32m-> 1184\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    284\u001b[0m                                          as_ref=False):\n\u001b[0;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    225\u001b[0m   \"\"\"\n\u001b[0;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 227\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=1600,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
