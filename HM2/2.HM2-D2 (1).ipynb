{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://admissions.ntust.edu.tw/front_index/images/logo.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<h1><center>MACHINE LEARNING : HOMEWORK 2</center></h1>\n",
    "<h1><center>Data 2 : Authors prediction</center></h1>\n",
    "<h2>Hector LANDES - M10601810</h2>\n",
    "<h2>Machine Learning - CS5087701</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "import scipy.interpolate as interp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nltk.corpus import stopwords\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from ann_visualizer.visualize import ann_viz;\n",
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Users\\Hector Landes\\graphviz-2.38\\release\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv (r'C:\\Users\\Hector Landes\\Desktop\\Data Science Projects\\20191005 Homework_1_ML\\2. Prepared Data\\test.csv')\n",
    "train = pd.read_csv (r'C:\\Users\\Hector Landes\\Desktop\\Data Science Projects\\20191005 Homework_1_ML\\2. Prepared Data\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create boolean value if author is EAP, HPL or MWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in train.author])\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Preprocessing from the HM1</3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def RemoveStopwords(word_list):\n",
    "    return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "def GetWordnetPos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def LemmatizeTokens(word_list):\n",
    "    return [lemmatizer.lemmatize(word, GetWordnetPos(word)) for word in word_list]\n",
    "   \n",
    "def CountPartOfSpeech(word_list):\n",
    "    tagged = nltk.pos_tag(nltk.Text(word_list))\n",
    "    counts = Counter(tag for word, tag in tagged)\n",
    "    total = sum(counts.values())\n",
    "    ret = dict((word, float(count) / total) for word, count in counts.items())\n",
    "    return ret\n",
    "\n",
    "def ExtractPartOfSpeech(pos_dict, pos_to_extract):\n",
    "    if pos_to_extract in pos_dict.keys():\n",
    "\n",
    "\n",
    "        return pos_dict[pos_to_extract]\n",
    "   \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiki = train.iloc[0:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"tokens\"] = kiki.text.apply(lambda x: LemmatizeTokens(RemoveStopwords(Tokenize(x))))\n",
    "\n",
    "#train[\"tokens\"] = train.text.apply(lambda x: LemmatizeTokens(RemoveStopwords(Tokenize(x))))\n",
    "#test[\"tokens\"] = test.text.apply(lambda x: LemmatizeTokens(RemoveStopwords(Tokenize(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"pos\"] = kiki.tokens.apply(CountPartOfSpeech)\n",
    "\n",
    "#train[\"pos\"] = train.tokens.apply(CountPartOfSpeech)\n",
    "#test[\"pos\"] = test.tokens.apply(CountPartOfSpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"prop_noun\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"NN\"))\n",
    "kiki[\"prop_verb\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"VB\"))\n",
    "kiki[\"prop_adj\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"JJ\"))\n",
    "kiki[\"prop_adv\"] = kiki.pos.apply(lambda x: ExtractPartOfSpeech(x, \"RB\"))\n",
    "\n",
    "#train[\"prop_noun\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"NN\"))\n",
    "#train[\"prop_verb\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"VB\"))\n",
    "#train[\"prop_adj\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"JJ\"))\n",
    "#train[\"prop_adv\"] = train.pos.apply(lambda x: ExtractPartOfSpeech(x, \"RB\"))\n",
    "#test[\"prop_noun\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"NN\"))\n",
    "#test[\"prop_verb\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"VB\"))\n",
    "#test[\"prop_adj\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"JJ\"))\n",
    "#test[\"prop_adv\"] = test.pos.apply(lambda x: ExtractPartOfSpeech(x, \"RB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_words\"] = kiki[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "#test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"token_num_words\"] = kiki[\"tokens\"].apply(lambda x: len(str(x).split()))\n",
    "#test[\"token_num_words\"] = test[\"tokens\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_unique_words\"] = kiki[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "#test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_chars\"] = kiki[\"text\"].apply(lambda x: len(str(x)))\n",
    "#test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_punctuations\"] = kiki['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "#test[\"num_punctuations\"] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_words_upper\"] = kiki[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "#test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"num_words_title\"] = kiki[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "#test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"mean_word_len\"] = kiki[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "#test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"mean_word_len\"] = kiki[\"tokens\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "#test[\"mean_word_len\"] = test[\"tokens\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"std_word_len\"] = kiki[\"text\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))\n",
    "#test[\"std_word_len\"] = test[\"text\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki[\"std_word_len\"] = kiki[\"tokens\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))\n",
    "#test[\"std_word_len\"] = test[\"tokens\"].apply(lambda x: np.std([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kiki['label_author'] = LabelEncoder().fit_transform(kiki['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'text',\n",
       " 'author',\n",
       " 'num_stopwords',\n",
       " 'tokens',\n",
       " 'pos',\n",
       " 'prop_noun',\n",
       " 'prop_verb',\n",
       " 'prop_adj',\n",
       " 'prop_adv',\n",
       " 'num_words',\n",
       " 'token_num_words',\n",
       " 'num_unique_words',\n",
       " 'num_chars',\n",
       " 'num_punctuations',\n",
       " 'num_words_upper',\n",
       " 'num_words_title',\n",
       " 'mean_word_len',\n",
       " 'std_word_len',\n",
       " 'label_author']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = kiki.columns.tolist()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiki = kiki[['id',\n",
    " 'text',\n",
    " 'author',\n",
    " 'tokens',\n",
    " 'pos',\n",
    "'num_stopwords',             \n",
    " 'prop_noun',\n",
    " 'prop_verb',\n",
    " 'prop_adj',\n",
    " 'prop_adv',\n",
    " 'num_words',\n",
    " 'token_num_words',\n",
    " 'num_unique_words',\n",
    " 'num_chars',\n",
    " 'num_punctuations',\n",
    " 'num_words_upper',\n",
    " 'num_words_title',\n",
    " 'mean_word_len',\n",
    " 'std_word_len',\n",
    " 'label_author']]\n",
    "#test = test[['id',\n",
    "# 'text',\n",
    "# 'tokens',\n",
    "# 'pos',\n",
    "#'num_stopwords',             \n",
    "# 'prop_noun',\n",
    "# 'prop_verb',\n",
    "# 'prop_adj',\n",
    "# 'prop_adv',\n",
    "# 'num_words',\n",
    "# 'token_num_words',\n",
    "# 'num_unique_words',\n",
    "# 'num_chars',\n",
    "# 'num_punctuations',\n",
    "# 'num_words_upper',\n",
    "# 'num_words_title',\n",
    "# 'mean_word_len',\n",
    "# 'std_word_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<b> X </b> as the <b> Feature Matrix and normalization </b> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>prop_noun</th>\n",
       "      <th>prop_verb</th>\n",
       "      <th>prop_adj</th>\n",
       "      <th>prop_adv</th>\n",
       "      <th>num_words</th>\n",
       "      <th>token_num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>std_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.119497</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.101604</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.096374</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.327177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050314</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>0.050459</td>\n",
       "      <td>0.022946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.174078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_stopwords  prop_noun  prop_verb  prop_adj  prop_adv  num_words  \\\n",
       "0       0.119497    0.52381   0.047619  0.142857  0.095238   0.101604   \n",
       "1       0.050314    0.00000   0.333333  0.166667  0.166667   0.029412   \n",
       "\n",
       "   token_num_words  num_unique_words  num_chars  num_punctuations  \\\n",
       "0         0.093458          0.146789   0.096374          0.103448   \n",
       "1         0.023364          0.050459   0.022946          0.000000   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  std_word_len  \n",
       "0         0.181818         0.107143       0.500000      0.327177  \n",
       "1         0.000000         0.035714       0.464286      0.174078  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = kiki[kiki.columns[5:19]]\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "X[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> <b> Y </b> as the <b> Target Value </b> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    2\n",
       "4    1\n",
       "Name: label_author, dtype: int32"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = kiki['label_author']\n",
    "Y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data in training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainset, X_testset, Y_trainset, Y_testset = train_test_split(X, Y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix ->   Training: (3200, 14)    Testing: (800, 14)\n",
      "Target value   ->   Training: (3200,)       Testing: (800,)\n"
     ]
    }
   ],
   "source": [
    "print('Feature matrix ->   Training:',X_trainset.shape,'   Testing:',X_testset.shape)\n",
    "print('Target value   ->   Training:',Y_trainset.shape,'      Testing:',Y_testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(train)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "fri = pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1046</td>\n",
       "      <td>11510</td>\n",
       "      <td>3666</td>\n",
       "      <td>13</td>\n",
       "      <td>2368</td>\n",
       "      <td>1313</td>\n",
       "      <td>31894</td>\n",
       "      <td>20049</td>\n",
       "      <td>1907</td>\n",
       "      <td>5078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>31895</td>\n",
       "      <td>5359</td>\n",
       "      <td>211</td>\n",
       "      <td>1065</td>\n",
       "      <td>118</td>\n",
       "      <td>771</td>\n",
       "      <td>1086</td>\n",
       "      <td>3119</td>\n",
       "      <td>20050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>150</td>\n",
       "      <td>4045</td>\n",
       "      <td>873</td>\n",
       "      <td>3007</td>\n",
       "      <td>1375</td>\n",
       "      <td>13</td>\n",
       "      <td>1793</td>\n",
       "      <td>31897</td>\n",
       "      <td>31898</td>\n",
       "      <td>24547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>711</td>\n",
       "      <td>24552</td>\n",
       "      <td>45023</td>\n",
       "      <td>2907</td>\n",
       "      <td>31901</td>\n",
       "      <td>45024</td>\n",
       "      <td>2011</td>\n",
       "      <td>16912</td>\n",
       "      <td>24553</td>\n",
       "      <td>20054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31903</td>\n",
       "      <td>108</td>\n",
       "      <td>966</td>\n",
       "      <td>45026</td>\n",
       "      <td>4604</td>\n",
       "      <td>3121</td>\n",
       "      <td>442</td>\n",
       "      <td>1754</td>\n",
       "      <td>31904</td>\n",
       "      <td>45027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>480</td>\n",
       "      <td>20060</td>\n",
       "      <td>45041</td>\n",
       "      <td>9413</td>\n",
       "      <td>9414</td>\n",
       "      <td>1549</td>\n",
       "      <td>11514</td>\n",
       "      <td>45042</td>\n",
       "      <td>24555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16918</td>\n",
       "      <td>32</td>\n",
       "      <td>24558</td>\n",
       "      <td>7355</td>\n",
       "      <td>45043</td>\n",
       "      <td>65</td>\n",
       "      <td>4208</td>\n",
       "      <td>10372</td>\n",
       "      <td>16919</td>\n",
       "      <td>12881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>343</td>\n",
       "      <td>4</td>\n",
       "      <td>45044</td>\n",
       "      <td>31910</td>\n",
       "      <td>630</td>\n",
       "      <td>9415</td>\n",
       "      <td>8590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1021</td>\n",
       "      <td>24562</td>\n",
       "      <td>10</td>\n",
       "      <td>125</td>\n",
       "      <td>31915</td>\n",
       "      <td>911</td>\n",
       "      <td>3241</td>\n",
       "      <td>14626</td>\n",
       "      <td>7901</td>\n",
       "      <td>4391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>652</td>\n",
       "      <td>3526</td>\n",
       "      <td>45057</td>\n",
       "      <td>31918</td>\n",
       "      <td>45058</td>\n",
       "      <td>652</td>\n",
       "      <td>3526</td>\n",
       "      <td>24564</td>\n",
       "      <td>1301</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10376</td>\n",
       "      <td>16925</td>\n",
       "      <td>24565</td>\n",
       "      <td>20066</td>\n",
       "      <td>4048</td>\n",
       "      <td>6013</td>\n",
       "      <td>6014</td>\n",
       "      <td>14627</td>\n",
       "      <td>11522</td>\n",
       "      <td>20067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3009</td>\n",
       "      <td>4</td>\n",
       "      <td>20068</td>\n",
       "      <td>8592</td>\n",
       "      <td>10</td>\n",
       "      <td>226</td>\n",
       "      <td>130</td>\n",
       "      <td>113</td>\n",
       "      <td>1527</td>\n",
       "      <td>31920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45061</td>\n",
       "      <td>45062</td>\n",
       "      <td>5080</td>\n",
       "      <td>45063</td>\n",
       "      <td>12885</td>\n",
       "      <td>221</td>\n",
       "      <td>45064</td>\n",
       "      <td>13</td>\n",
       "      <td>1829</td>\n",
       "      <td>3011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>835</td>\n",
       "      <td>4</td>\n",
       "      <td>45065</td>\n",
       "      <td>785</td>\n",
       "      <td>3120</td>\n",
       "      <td>8584</td>\n",
       "      <td>4392</td>\n",
       "      <td>14628</td>\n",
       "      <td>5081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>11523</td>\n",
       "      <td>12886</td>\n",
       "      <td>31921</td>\n",
       "      <td>24567</td>\n",
       "      <td>45067</td>\n",
       "      <td>45068</td>\n",
       "      <td>248</td>\n",
       "      <td>6016</td>\n",
       "      <td>5672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14629</td>\n",
       "      <td>70</td>\n",
       "      <td>45071</td>\n",
       "      <td>31924</td>\n",
       "      <td>1963</td>\n",
       "      <td>213</td>\n",
       "      <td>16929</td>\n",
       "      <td>7359</td>\n",
       "      <td>31925</td>\n",
       "      <td>45072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31930</td>\n",
       "      <td>48</td>\n",
       "      <td>3528</td>\n",
       "      <td>16931</td>\n",
       "      <td>3529</td>\n",
       "      <td>45081</td>\n",
       "      <td>24577</td>\n",
       "      <td>6410</td>\n",
       "      <td>1223</td>\n",
       "      <td>12892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>237</td>\n",
       "      <td>8595</td>\n",
       "      <td>3125</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>4608</td>\n",
       "      <td>3013</td>\n",
       "      <td>31931</td>\n",
       "      <td>45082</td>\n",
       "      <td>3242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45086</td>\n",
       "      <td>7903</td>\n",
       "      <td>212</td>\n",
       "      <td>13</td>\n",
       "      <td>58</td>\n",
       "      <td>24579</td>\n",
       "      <td>9419</td>\n",
       "      <td>90</td>\n",
       "      <td>670</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7906</td>\n",
       "      <td>1675</td>\n",
       "      <td>11525</td>\n",
       "      <td>20075</td>\n",
       "      <td>3007</td>\n",
       "      <td>1375</td>\n",
       "      <td>45089</td>\n",
       "      <td>7360</td>\n",
       "      <td>24580</td>\n",
       "      <td>14638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>24584</td>\n",
       "      <td>42</td>\n",
       "      <td>12896</td>\n",
       "      <td>45091</td>\n",
       "      <td>58</td>\n",
       "      <td>45092</td>\n",
       "      <td>313</td>\n",
       "      <td>5086</td>\n",
       "      <td>2908</td>\n",
       "      <td>31936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4212</td>\n",
       "      <td>24586</td>\n",
       "      <td>10381</td>\n",
       "      <td>24587</td>\n",
       "      <td>175</td>\n",
       "      <td>187</td>\n",
       "      <td>7907</td>\n",
       "      <td>10382</td>\n",
       "      <td>2123</td>\n",
       "      <td>45094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8598</td>\n",
       "      <td>10383</td>\n",
       "      <td>45096</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>45097</td>\n",
       "      <td>212</td>\n",
       "      <td>10</td>\n",
       "      <td>31941</td>\n",
       "      <td>45098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10386</td>\n",
       "      <td>129</td>\n",
       "      <td>10</td>\n",
       "      <td>329</td>\n",
       "      <td>6020</td>\n",
       "      <td>985</td>\n",
       "      <td>47</td>\n",
       "      <td>5676</td>\n",
       "      <td>13</td>\n",
       "      <td>20085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>812</td>\n",
       "      <td>29</td>\n",
       "      <td>617</td>\n",
       "      <td>3377</td>\n",
       "      <td>6021</td>\n",
       "      <td>4</td>\n",
       "      <td>9421</td>\n",
       "      <td>9422</td>\n",
       "      <td>20086</td>\n",
       "      <td>31946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45108</td>\n",
       "      <td>8600</td>\n",
       "      <td>31947</td>\n",
       "      <td>4830</td>\n",
       "      <td>47</td>\n",
       "      <td>2176</td>\n",
       "      <td>14650</td>\n",
       "      <td>13</td>\n",
       "      <td>45109</td>\n",
       "      <td>31948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5361</td>\n",
       "      <td>65</td>\n",
       "      <td>1432</td>\n",
       "      <td>24597</td>\n",
       "      <td>4615</td>\n",
       "      <td>45121</td>\n",
       "      <td>31956</td>\n",
       "      <td>24598</td>\n",
       "      <td>31957</td>\n",
       "      <td>14652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11510</td>\n",
       "      <td>9414</td>\n",
       "      <td>20090</td>\n",
       "      <td>72</td>\n",
       "      <td>32</td>\n",
       "      <td>928</td>\n",
       "      <td>4215</td>\n",
       "      <td>119</td>\n",
       "      <td>8604</td>\n",
       "      <td>8605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3017</td>\n",
       "      <td>5365</td>\n",
       "      <td>45130</td>\n",
       "      <td>326</td>\n",
       "      <td>45131</td>\n",
       "      <td>31963</td>\n",
       "      <td>67</td>\n",
       "      <td>31964</td>\n",
       "      <td>31965</td>\n",
       "      <td>31966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>407</td>\n",
       "      <td>16943</td>\n",
       "      <td>14656</td>\n",
       "      <td>14657</td>\n",
       "      <td>31968</td>\n",
       "      <td>10</td>\n",
       "      <td>243</td>\n",
       "      <td>45134</td>\n",
       "      <td>7368</td>\n",
       "      <td>31969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19549</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60163</td>\n",
       "      <td>7986</td>\n",
       "      <td>72</td>\n",
       "      <td>44660</td>\n",
       "      <td>15665</td>\n",
       "      <td>43410</td>\n",
       "      <td>12240</td>\n",
       "      <td>72</td>\n",
       "      <td>235</td>\n",
       "      <td>18453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33204</td>\n",
       "      <td>4004</td>\n",
       "      <td>5233</td>\n",
       "      <td>1516</td>\n",
       "      <td>10</td>\n",
       "      <td>11396</td>\n",
       "      <td>44602</td>\n",
       "      <td>1097</td>\n",
       "      <td>45558</td>\n",
       "      <td>9931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19551</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9479</td>\n",
       "      <td>532</td>\n",
       "      <td>64467</td>\n",
       "      <td>46020</td>\n",
       "      <td>92</td>\n",
       "      <td>29235</td>\n",
       "      <td>13888</td>\n",
       "      <td>16075</td>\n",
       "      <td>1368</td>\n",
       "      <td>10043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19552</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>100</td>\n",
       "      <td>40410</td>\n",
       "      <td>73506</td>\n",
       "      <td>2331</td>\n",
       "      <td>66173</td>\n",
       "      <td>66510</td>\n",
       "      <td>11112</td>\n",
       "      <td>1103</td>\n",
       "      <td>5672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19553</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16578</td>\n",
       "      <td>212</td>\n",
       "      <td>42227</td>\n",
       "      <td>9281</td>\n",
       "      <td>212</td>\n",
       "      <td>13613</td>\n",
       "      <td>12164</td>\n",
       "      <td>133</td>\n",
       "      <td>3434</td>\n",
       "      <td>45708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19554</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1130</td>\n",
       "      <td>117</td>\n",
       "      <td>55423</td>\n",
       "      <td>34043</td>\n",
       "      <td>58</td>\n",
       "      <td>1130</td>\n",
       "      <td>1724</td>\n",
       "      <td>19966</td>\n",
       "      <td>11753</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19555</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1405</td>\n",
       "      <td>5651</td>\n",
       "      <td>17701</td>\n",
       "      <td>5189</td>\n",
       "      <td>70</td>\n",
       "      <td>1766</td>\n",
       "      <td>76566</td>\n",
       "      <td>70955</td>\n",
       "      <td>62242</td>\n",
       "      <td>18098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9097</td>\n",
       "      <td>98</td>\n",
       "      <td>16664</td>\n",
       "      <td>37907</td>\n",
       "      <td>75364</td>\n",
       "      <td>406</td>\n",
       "      <td>4797</td>\n",
       "      <td>5041</td>\n",
       "      <td>13</td>\n",
       "      <td>26915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19557</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7200</td>\n",
       "      <td>1429</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>7599</td>\n",
       "      <td>8188</td>\n",
       "      <td>764</td>\n",
       "      <td>22353</td>\n",
       "      <td>24515</td>\n",
       "      <td>24461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19558</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45265</td>\n",
       "      <td>45266</td>\n",
       "      <td>75387</td>\n",
       "      <td>25674</td>\n",
       "      <td>47973</td>\n",
       "      <td>599</td>\n",
       "      <td>19266</td>\n",
       "      <td>147</td>\n",
       "      <td>49327</td>\n",
       "      <td>59859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19559</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>23000</td>\n",
       "      <td>42</td>\n",
       "      <td>4117</td>\n",
       "      <td>1430</td>\n",
       "      <td>1202</td>\n",
       "      <td>1907</td>\n",
       "      <td>5071</td>\n",
       "      <td>6655</td>\n",
       "      <td>38911</td>\n",
       "      <td>7686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19560</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9685</td>\n",
       "      <td>642</td>\n",
       "      <td>465</td>\n",
       "      <td>6642</td>\n",
       "      <td>133</td>\n",
       "      <td>68099</td>\n",
       "      <td>124</td>\n",
       "      <td>2699</td>\n",
       "      <td>56735</td>\n",
       "      <td>10283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19561</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1614</td>\n",
       "      <td>12655</td>\n",
       "      <td>43636</td>\n",
       "      <td>30551</td>\n",
       "      <td>41734</td>\n",
       "      <td>4571</td>\n",
       "      <td>2053</td>\n",
       "      <td>8378</td>\n",
       "      <td>66786</td>\n",
       "      <td>31436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19562</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1042</td>\n",
       "      <td>41268</td>\n",
       "      <td>30410</td>\n",
       "      <td>32</td>\n",
       "      <td>8142</td>\n",
       "      <td>6198</td>\n",
       "      <td>13</td>\n",
       "      <td>23689</td>\n",
       "      <td>29903</td>\n",
       "      <td>23378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7829</td>\n",
       "      <td>267</td>\n",
       "      <td>68</td>\n",
       "      <td>655</td>\n",
       "      <td>7362</td>\n",
       "      <td>64</td>\n",
       "      <td>49156</td>\n",
       "      <td>19146</td>\n",
       "      <td>6916</td>\n",
       "      <td>3532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19564</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>28284</td>\n",
       "      <td>2706</td>\n",
       "      <td>689</td>\n",
       "      <td>1534</td>\n",
       "      <td>10</td>\n",
       "      <td>1432</td>\n",
       "      <td>2706</td>\n",
       "      <td>1598</td>\n",
       "      <td>14792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76546</td>\n",
       "      <td>3133</td>\n",
       "      <td>6511</td>\n",
       "      <td>19953</td>\n",
       "      <td>108</td>\n",
       "      <td>4908</td>\n",
       "      <td>7871</td>\n",
       "      <td>103</td>\n",
       "      <td>895</td>\n",
       "      <td>56414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1966</td>\n",
       "      <td>12657</td>\n",
       "      <td>5626</td>\n",
       "      <td>77</td>\n",
       "      <td>9037</td>\n",
       "      <td>94</td>\n",
       "      <td>16298</td>\n",
       "      <td>9306</td>\n",
       "      <td>44479</td>\n",
       "      <td>17922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19567</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>424</td>\n",
       "      <td>712</td>\n",
       "      <td>317</td>\n",
       "      <td>4</td>\n",
       "      <td>912</td>\n",
       "      <td>13259</td>\n",
       "      <td>12299</td>\n",
       "      <td>30176</td>\n",
       "      <td>16834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19568</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>75478</td>\n",
       "      <td>62502</td>\n",
       "      <td>22338</td>\n",
       "      <td>33070</td>\n",
       "      <td>3806</td>\n",
       "      <td>175</td>\n",
       "      <td>187</td>\n",
       "      <td>2940</td>\n",
       "      <td>68478</td>\n",
       "      <td>15972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19569</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3520</td>\n",
       "      <td>37598</td>\n",
       "      <td>19064</td>\n",
       "      <td>212</td>\n",
       "      <td>26433</td>\n",
       "      <td>17562</td>\n",
       "      <td>10</td>\n",
       "      <td>513</td>\n",
       "      <td>73286</td>\n",
       "      <td>20020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5341</td>\n",
       "      <td>2661</td>\n",
       "      <td>25777</td>\n",
       "      <td>95</td>\n",
       "      <td>415</td>\n",
       "      <td>1933</td>\n",
       "      <td>5583</td>\n",
       "      <td>50607</td>\n",
       "      <td>1631</td>\n",
       "      <td>12872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>71873</td>\n",
       "      <td>10</td>\n",
       "      <td>6629</td>\n",
       "      <td>2008</td>\n",
       "      <td>1065</td>\n",
       "      <td>118</td>\n",
       "      <td>2670</td>\n",
       "      <td>26297</td>\n",
       "      <td>11581</td>\n",
       "      <td>37308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19572</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>24314</td>\n",
       "      <td>16174</td>\n",
       "      <td>42</td>\n",
       "      <td>67792</td>\n",
       "      <td>67793</td>\n",
       "      <td>67794</td>\n",
       "      <td>1150</td>\n",
       "      <td>797</td>\n",
       "      <td>4450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19573</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>43346</td>\n",
       "      <td>14782</td>\n",
       "      <td>9836</td>\n",
       "      <td>300</td>\n",
       "      <td>18278</td>\n",
       "      <td>24557</td>\n",
       "      <td>16918</td>\n",
       "      <td>32</td>\n",
       "      <td>37906</td>\n",
       "      <td>44969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3465</td>\n",
       "      <td>7758</td>\n",
       "      <td>262</td>\n",
       "      <td>103</td>\n",
       "      <td>3877</td>\n",
       "      <td>60120</td>\n",
       "      <td>43218</td>\n",
       "      <td>2267</td>\n",
       "      <td>702</td>\n",
       "      <td>68702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>20671</td>\n",
       "      <td>4</td>\n",
       "      <td>30877</td>\n",
       "      <td>352</td>\n",
       "      <td>2980</td>\n",
       "      <td>97</td>\n",
       "      <td>27419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>75491</td>\n",
       "      <td>72470</td>\n",
       "      <td>4</td>\n",
       "      <td>75603</td>\n",
       "      <td>895</td>\n",
       "      <td>908</td>\n",
       "      <td>541</td>\n",
       "      <td>697</td>\n",
       "      <td>152</td>\n",
       "      <td>9350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>31848</td>\n",
       "      <td>65603</td>\n",
       "      <td>11306</td>\n",
       "      <td>918</td>\n",
       "      <td>198</td>\n",
       "      <td>128</td>\n",
       "      <td>1574</td>\n",
       "      <td>21725</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>784</td>\n",
       "      <td>2783</td>\n",
       "      <td>280</td>\n",
       "      <td>211</td>\n",
       "      <td>1065</td>\n",
       "      <td>6809</td>\n",
       "      <td>252</td>\n",
       "      <td>4295</td>\n",
       "      <td>444</td>\n",
       "      <td>50416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19579 rows  256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...    246    247  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...   1046  11510   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...      4  31895   \n",
       "2        0    0    0    0    0    0    0    0    0    0  ...    150   4045   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    711  24552   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...  31903    108   \n",
       "5        0    0    0    0    0    0    0    0    0    0  ...     48    480   \n",
       "6        0    0    0    0    0    0    0    0    0    0  ...  16918     32   \n",
       "7        0    0    0    0    0    0    0    0    0    0  ...      9     28   \n",
       "8        0    0    0    0    0    0    0    0    0    0  ...   1021  24562   \n",
       "9        0    0    0    0    0    0    0    0    0    0  ...    652   3526   \n",
       "10       0    0    0    0    0    0    0    0    0    0  ...  10376  16925   \n",
       "11       0    0    0    0    0    0    0    0    0    0  ...   3009      4   \n",
       "12       0    0    0    0    0    0    0    0    0    0  ...  45061  45062   \n",
       "13       0    0    0    0    0    0    0    0    0    0  ...      6    835   \n",
       "14       0    0    0    0    0    0    0    0    0    0  ...      4  11523   \n",
       "15       0    0    0    0    0    0    0    0    0    0  ...  14629     70   \n",
       "16       0    0    0    0    0    0    0    0    0    0  ...  31930     48   \n",
       "17       0    0    0    0    0    0    0    0    0    0  ...    237   8595   \n",
       "18       0    0    0    0    0    0    0    0    0    0  ...  45086   7903   \n",
       "19       0    0    0    0    0    0    0    0    0    0  ...   7906   1675   \n",
       "20       0    0    0    0    0    0    0    0    0    0  ...  24584     42   \n",
       "21       0    0    0    0    0    0    0    0    0    0  ...   4212  24586   \n",
       "22       0    0    0    0    0    0    0    0    0    0  ...   8598  10383   \n",
       "23       0    0    0    0    0    0    0    0    0    0  ...  10386    129   \n",
       "24       0    0    0    0    0    0    0    0    0    0  ...    812     29   \n",
       "25       0    0    0    0    0    0    0    0    0    0  ...  45108   8600   \n",
       "26       0    0    0    0    0    0    0    0    0    0  ...   5361     65   \n",
       "27       0    0    0    0    0    0    0    0    0    0  ...  11510   9414   \n",
       "28       0    0    0    0    0    0    0    0    0    0  ...   3017   5365   \n",
       "29       0    0    0    0    0    0    0    0    0    0  ...    407  16943   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "19549    0    0    0    0    0    0    0    0    0    0  ...  60163   7986   \n",
       "19550    0    0    0    0    0    0    0    0    0    0  ...  33204   4004   \n",
       "19551    0    0    0    0    0    0    0    0    0    0  ...   9479    532   \n",
       "19552    0    0    0    0    0    0    0    0    0    0  ...    107    100   \n",
       "19553    0    0    0    0    0    0    0    0    0    0  ...  16578    212   \n",
       "19554    0    0    0    0    0    0    0    0    0    0  ...   1130    117   \n",
       "19555    0    0    0    0    0    0    0    0    0    0  ...   1405   5651   \n",
       "19556    0    0    0    0    0    0    0    0    0    0  ...   9097     98   \n",
       "19557    0    0    0    0    0    0    0    0    0    0  ...   7200   1429   \n",
       "19558    0    0    0    0    0    0    0    0    0    0  ...  45265  45266   \n",
       "19559    0    0    0    0    0    0    0    0    0    0  ...  23000     42   \n",
       "19560    0    0    0    0    0    0    0    0    0    0  ...   9685    642   \n",
       "19561    0    0    0    0    0    0    0    0    0    0  ...   1614  12655   \n",
       "19562    0    0    0    0    0    0    0    0    0    0  ...   1042  41268   \n",
       "19563    0    0    0    0    0    0    0    0    0    0  ...   7829    267   \n",
       "19564    0    0    0    0    0    0    0    0    0    0  ...     90  28284   \n",
       "19565    0    0    0    0    0    0    0    0    0    0  ...  76546   3133   \n",
       "19566    0    0    0    0    0    0    0    0    0    0  ...   1966  12657   \n",
       "19567    0    0    0    0    0    0    0    0    0    0  ...      3    424   \n",
       "19568    0    0    0    0    0    0    0    0    0    0  ...  75478  62502   \n",
       "19569    0    0    0    0    0    0    0    0    0    0  ...   3520  37598   \n",
       "19570    0    0    0    0    0    0    0    0    0    0  ...   5341   2661   \n",
       "19571    0    0    0    0    0    0    0    0    0    0  ...  71873     10   \n",
       "19572    0    0    0    0    0    0    0    0    0    0  ...    235  24314   \n",
       "19573    0    0    0    0    0    0    0    0    0    0  ...  43346  14782   \n",
       "19574    0    0    0    0    0    0    0    0    0    0  ...   3465   7758   \n",
       "19575    0    0    0    0    0    0    0    0    0    0  ...     91      9   \n",
       "19576    0    0    0    0    0    0    0    0    0    0  ...  75491  72470   \n",
       "19577    0    0    0    0    0    0    0    0    0    0  ...     16  31848   \n",
       "19578    0    0    0    0    0    0    0    0    0    0  ...    784   2783   \n",
       "\n",
       "         248    249    250    251    252    253    254    255  \n",
       "0       3666     13   2368   1313  31894  20049   1907   5078  \n",
       "1       5359    211   1065    118    771   1086   3119  20050  \n",
       "2        873   3007   1375     13   1793  31897  31898  24547  \n",
       "3      45023   2907  31901  45024   2011  16912  24553  20054  \n",
       "4        966  45026   4604   3121    442   1754  31904  45027  \n",
       "5      20060  45041   9413   9414   1549  11514  45042  24555  \n",
       "6      24558   7355  45043     65   4208  10372  16919  12881  \n",
       "7         15    343      4  45044  31910    630   9415   8590  \n",
       "8         10    125  31915    911   3241  14626   7901   4391  \n",
       "9      45057  31918  45058    652   3526  24564   1301    241  \n",
       "10     24565  20066   4048   6013   6014  14627  11522  20067  \n",
       "11     20068   8592     10    226    130    113   1527  31920  \n",
       "12      5080  45063  12885    221  45064     13   1829   3011  \n",
       "13         4  45065    785   3120   8584   4392  14628   5081  \n",
       "14     12886  31921  24567  45067  45068    248   6016   5672  \n",
       "15     45071  31924   1963    213  16929   7359  31925  45072  \n",
       "16      3528  16931   3529  45081  24577   6410   1223  12892  \n",
       "17      3125     10     54   4608   3013  31931  45082   3242  \n",
       "18       212     13     58  24579   9419     90    670   2014  \n",
       "19     11525  20075   3007   1375  45089   7360  24580  14638  \n",
       "20     12896  45091     58  45092    313   5086   2908  31936  \n",
       "21     10381  24587    175    187   7907  10382   2123  45094  \n",
       "22     45096    212    212  45097    212     10  31941  45098  \n",
       "23        10    329   6020    985     47   5676     13  20085  \n",
       "24       617   3377   6021      4   9421   9422  20086  31946  \n",
       "25     31947   4830     47   2176  14650     13  45109  31948  \n",
       "26      1432  24597   4615  45121  31956  24598  31957  14652  \n",
       "27     20090     72     32    928   4215    119   8604   8605  \n",
       "28     45130    326  45131  31963     67  31964  31965  31966  \n",
       "29     14656  14657  31968     10    243  45134   7368  31969  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "19549     72  44660  15665  43410  12240     72    235  18453  \n",
       "19550   5233   1516     10  11396  44602   1097  45558   9931  \n",
       "19551  64467  46020     92  29235  13888  16075   1368  10043  \n",
       "19552  40410  73506   2331  66173  66510  11112   1103   5672  \n",
       "19553  42227   9281    212  13613  12164    133   3434  45708  \n",
       "19554  55423  34043     58   1130   1724  19966  11753   1306  \n",
       "19555  17701   5189     70   1766  76566  70955  62242  18098  \n",
       "19556  16664  37907  75364    406   4797   5041     13  26915  \n",
       "19557      4     13   7599   8188    764  22353  24515  24461  \n",
       "19558  75387  25674  47973    599  19266    147  49327  59859  \n",
       "19559   4117   1430   1202   1907   5071   6655  38911   7686  \n",
       "19560    465   6642    133  68099    124   2699  56735  10283  \n",
       "19561  43636  30551  41734   4571   2053   8378  66786  31436  \n",
       "19562  30410     32   8142   6198     13  23689  29903  23378  \n",
       "19563     68    655   7362     64  49156  19146   6916   3532  \n",
       "19564   2706    689   1534     10   1432   2706   1598  14792  \n",
       "19565   6511  19953    108   4908   7871    103    895  56414  \n",
       "19566   5626     77   9037     94  16298   9306  44479  17922  \n",
       "19567    712    317      4    912  13259  12299  30176  16834  \n",
       "19568  22338  33070   3806    175    187   2940  68478  15972  \n",
       "19569  19064    212  26433  17562     10    513  73286  20020  \n",
       "19570  25777     95    415   1933   5583  50607   1631  12872  \n",
       "19571   6629   2008   1065    118   2670  26297  11581  37308  \n",
       "19572  16174     42  67792  67793  67794   1150    797   4450  \n",
       "19573   9836    300  18278  24557  16918     32  37906  44969  \n",
       "19574    262    103   3877  60120  43218   2267    702  68702  \n",
       "19575      8  20671      4  30877    352   2980     97  27419  \n",
       "19576      4  75603    895    908    541    697    152   9350  \n",
       "19577  65603  11306    918    198    128   1574  21725     95  \n",
       "19578    280    211   1065   6809    252   4295    444  50416  \n",
       "\n",
       "[19579 rows x 256 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - ETA: 7s - loss: 1.0992 - accuracy: 0.31 - ETA: 3s - loss: 1.1007 - accuracy: 0.27 - ETA: 2s - loss: 1.1003 - accuracy: 0.27 - ETA: 2s - loss: 1.0989 - accuracy: 0.32 - ETA: 2s - loss: 1.0985 - accuracy: 0.33 - ETA: 2s - loss: 1.0980 - accuracy: 0.34 - ETA: 2s - loss: 1.0977 - accuracy: 0.34 - ETA: 2s - loss: 1.0971 - accuracy: 0.35 - ETA: 2s - loss: 1.0970 - accuracy: 0.35 - ETA: 2s - loss: 1.0965 - accuracy: 0.35 - ETA: 2s - loss: 1.0958 - accuracy: 0.36 - ETA: 1s - loss: 1.0951 - accuracy: 0.36 - ETA: 1s - loss: 1.0946 - accuracy: 0.37 - ETA: 1s - loss: 1.0942 - accuracy: 0.37 - ETA: 1s - loss: 1.0942 - accuracy: 0.37 - ETA: 1s - loss: 1.0933 - accuracy: 0.37 - ETA: 1s - loss: 1.0929 - accuracy: 0.38 - ETA: 1s - loss: 1.0927 - accuracy: 0.38 - ETA: 1s - loss: 1.0917 - accuracy: 0.38 - ETA: 1s - loss: 1.0913 - accuracy: 0.38 - ETA: 1s - loss: 1.0919 - accuracy: 0.38 - ETA: 1s - loss: 1.0917 - accuracy: 0.38 - ETA: 1s - loss: 1.0916 - accuracy: 0.38 - ETA: 0s - loss: 1.0916 - accuracy: 0.38 - ETA: 0s - loss: 1.0913 - accuracy: 0.38 - ETA: 0s - loss: 1.0914 - accuracy: 0.38 - ETA: 0s - loss: 1.0913 - accuracy: 0.38 - ETA: 0s - loss: 1.0907 - accuracy: 0.38 - ETA: 0s - loss: 1.0902 - accuracy: 0.38 - ETA: 0s - loss: 1.0898 - accuracy: 0.38 - ETA: 0s - loss: 1.0899 - accuracy: 0.38 - ETA: 0s - loss: 1.0895 - accuracy: 0.38 - ETA: 0s - loss: 1.0891 - accuracy: 0.39 - ETA: 0s - loss: 1.0884 - accuracy: 0.39 - ETA: 0s - loss: 1.0882 - accuracy: 0.39 - ETA: 0s - loss: 1.0879 - accuracy: 0.39 - ETA: 0s - loss: 1.0880 - accuracy: 0.39 - ETA: 0s - loss: 1.0876 - accuracy: 0.39 - 3s 160us/step - loss: 1.0874 - accuracy: 0.3942 - val_loss: 1.0798 - val_accuracy: 0.4099\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 1.0936 - accuracy: 0.36 - ETA: 2s - loss: 1.0890 - accuracy: 0.37 - ETA: 2s - loss: 1.0860 - accuracy: 0.38 - ETA: 2s - loss: 1.0822 - accuracy: 0.39 - ETA: 2s - loss: 1.0778 - accuracy: 0.41 - ETA: 2s - loss: 1.0773 - accuracy: 0.41 - ETA: 2s - loss: 1.0766 - accuracy: 0.41 - ETA: 2s - loss: 1.0760 - accuracy: 0.41 - ETA: 1s - loss: 1.0767 - accuracy: 0.41 - ETA: 1s - loss: 1.0764 - accuracy: 0.41 - ETA: 1s - loss: 1.0773 - accuracy: 0.41 - ETA: 1s - loss: 1.0774 - accuracy: 0.41 - ETA: 1s - loss: 1.0781 - accuracy: 0.40 - ETA: 1s - loss: 1.0788 - accuracy: 0.40 - ETA: 1s - loss: 1.0790 - accuracy: 0.40 - ETA: 1s - loss: 1.0788 - accuracy: 0.40 - ETA: 1s - loss: 1.0782 - accuracy: 0.40 - ETA: 1s - loss: 1.0781 - accuracy: 0.40 - ETA: 1s - loss: 1.0782 - accuracy: 0.40 - ETA: 1s - loss: 1.0789 - accuracy: 0.40 - ETA: 1s - loss: 1.0793 - accuracy: 0.40 - ETA: 1s - loss: 1.0791 - accuracy: 0.40 - ETA: 1s - loss: 1.0790 - accuracy: 0.40 - ETA: 1s - loss: 1.0792 - accuracy: 0.39 - ETA: 1s - loss: 1.0793 - accuracy: 0.39 - ETA: 1s - loss: 1.0791 - accuracy: 0.39 - ETA: 1s - loss: 1.0793 - accuracy: 0.39 - ETA: 0s - loss: 1.0799 - accuracy: 0.39 - ETA: 0s - loss: 1.0793 - accuracy: 0.39 - ETA: 0s - loss: 1.0786 - accuracy: 0.39 - ETA: 0s - loss: 1.0785 - accuracy: 0.39 - ETA: 0s - loss: 1.0784 - accuracy: 0.39 - ETA: 0s - loss: 1.0785 - accuracy: 0.39 - ETA: 0s - loss: 1.0784 - accuracy: 0.39 - ETA: 0s - loss: 1.0781 - accuracy: 0.39 - ETA: 0s - loss: 1.0777 - accuracy: 0.40 - ETA: 0s - loss: 1.0775 - accuracy: 0.40 - ETA: 0s - loss: 1.0774 - accuracy: 0.40 - ETA: 0s - loss: 1.0770 - accuracy: 0.40 - ETA: 0s - loss: 1.0769 - accuracy: 0.40 - ETA: 0s - loss: 1.0764 - accuracy: 0.40 - ETA: 0s - loss: 1.0765 - accuracy: 0.40 - ETA: 0s - loss: 1.0766 - accuracy: 0.40 - ETA: 0s - loss: 1.0765 - accuracy: 0.40 - ETA: 0s - loss: 1.0763 - accuracy: 0.40 - 3s 165us/step - loss: 1.0761 - accuracy: 0.4022 - val_loss: 1.0695 - val_accuracy: 0.4104\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 1.0796 - accuracy: 0.33 - ETA: 2s - loss: 1.0715 - accuracy: 0.38 - ETA: 2s - loss: 1.0674 - accuracy: 0.40 - ETA: 2s - loss: 1.0674 - accuracy: 0.40 - ETA: 2s - loss: 1.0689 - accuracy: 0.39 - ETA: 2s - loss: 1.0720 - accuracy: 0.38 - ETA: 2s - loss: 1.0706 - accuracy: 0.39 - ETA: 2s - loss: 1.0708 - accuracy: 0.38 - ETA: 2s - loss: 1.0695 - accuracy: 0.39 - ETA: 1s - loss: 1.0691 - accuracy: 0.39 - ETA: 1s - loss: 1.0690 - accuracy: 0.39 - ETA: 1s - loss: 1.0687 - accuracy: 0.39 - ETA: 1s - loss: 1.0687 - accuracy: 0.39 - ETA: 1s - loss: 1.0684 - accuracy: 0.39 - ETA: 1s - loss: 1.0683 - accuracy: 0.39 - ETA: 1s - loss: 1.0683 - accuracy: 0.39 - ETA: 1s - loss: 1.0676 - accuracy: 0.39 - ETA: 1s - loss: 1.0673 - accuracy: 0.39 - ETA: 1s - loss: 1.0671 - accuracy: 0.39 - ETA: 1s - loss: 1.0666 - accuracy: 0.39 - ETA: 1s - loss: 1.0667 - accuracy: 0.39 - ETA: 1s - loss: 1.0665 - accuracy: 0.39 - ETA: 1s - loss: 1.0667 - accuracy: 0.39 - ETA: 1s - loss: 1.0660 - accuracy: 0.39 - ETA: 0s - loss: 1.0653 - accuracy: 0.39 - ETA: 0s - loss: 1.0650 - accuracy: 0.39 - ETA: 0s - loss: 1.0647 - accuracy: 0.39 - ETA: 0s - loss: 1.0641 - accuracy: 0.40 - ETA: 0s - loss: 1.0637 - accuracy: 0.40 - ETA: 0s - loss: 1.0636 - accuracy: 0.40 - ETA: 0s - loss: 1.0636 - accuracy: 0.40 - ETA: 0s - loss: 1.0634 - accuracy: 0.40 - ETA: 0s - loss: 1.0633 - accuracy: 0.40 - ETA: 0s - loss: 1.0631 - accuracy: 0.40 - ETA: 0s - loss: 1.0628 - accuracy: 0.40 - ETA: 0s - loss: 1.0619 - accuracy: 0.40 - ETA: 0s - loss: 1.0621 - accuracy: 0.40 - ETA: 0s - loss: 1.0619 - accuracy: 0.40 - ETA: 0s - loss: 1.0617 - accuracy: 0.40 - ETA: 0s - loss: 1.0613 - accuracy: 0.40 - ETA: 0s - loss: 1.0608 - accuracy: 0.40 - ETA: 0s - loss: 1.0605 - accuracy: 0.40 - 3s 161us/step - loss: 1.0606 - accuracy: 0.4040 - val_loss: 1.0525 - val_accuracy: 0.4137\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 1.0667 - accuracy: 0.37 - ETA: 2s - loss: 1.0642 - accuracy: 0.37 - ETA: 2s - loss: 1.0574 - accuracy: 0.38 - ETA: 2s - loss: 1.0541 - accuracy: 0.39 - ETA: 2s - loss: 1.0514 - accuracy: 0.39 - ETA: 2s - loss: 1.0480 - accuracy: 0.40 - ETA: 1s - loss: 1.0478 - accuracy: 0.40 - ETA: 1s - loss: 1.0485 - accuracy: 0.40 - ETA: 1s - loss: 1.0480 - accuracy: 0.40 - ETA: 1s - loss: 1.0475 - accuracy: 0.40 - ETA: 1s - loss: 1.0469 - accuracy: 0.40 - ETA: 1s - loss: 1.0455 - accuracy: 0.40 - ETA: 1s - loss: 1.0455 - accuracy: 0.40 - ETA: 1s - loss: 1.0450 - accuracy: 0.40 - ETA: 1s - loss: 1.0450 - accuracy: 0.40 - ETA: 1s - loss: 1.0445 - accuracy: 0.40 - ETA: 1s - loss: 1.0432 - accuracy: 0.41 - ETA: 1s - loss: 1.0432 - accuracy: 0.41 - ETA: 1s - loss: 1.0426 - accuracy: 0.41 - ETA: 1s - loss: 1.0423 - accuracy: 0.41 - ETA: 1s - loss: 1.0419 - accuracy: 0.41 - ETA: 1s - loss: 1.0419 - accuracy: 0.41 - ETA: 1s - loss: 1.0417 - accuracy: 0.41 - ETA: 0s - loss: 1.0414 - accuracy: 0.41 - ETA: 0s - loss: 1.0418 - accuracy: 0.41 - ETA: 0s - loss: 1.0418 - accuracy: 0.41 - ETA: 0s - loss: 1.0418 - accuracy: 0.41 - ETA: 0s - loss: 1.0413 - accuracy: 0.41 - ETA: 0s - loss: 1.0409 - accuracy: 0.41 - ETA: 0s - loss: 1.0405 - accuracy: 0.41 - ETA: 0s - loss: 1.0403 - accuracy: 0.41 - ETA: 0s - loss: 1.0401 - accuracy: 0.41 - ETA: 0s - loss: 1.0399 - accuracy: 0.41 - ETA: 0s - loss: 1.0393 - accuracy: 0.41 - ETA: 0s - loss: 1.0388 - accuracy: 0.41 - ETA: 0s - loss: 1.0382 - accuracy: 0.41 - ETA: 0s - loss: 1.0377 - accuracy: 0.41 - ETA: 0s - loss: 1.0375 - accuracy: 0.41 - ETA: 0s - loss: 1.0371 - accuracy: 0.41 - ETA: 0s - loss: 1.0366 - accuracy: 0.42 - 3s 160us/step - loss: 1.0363 - accuracy: 0.4210 - val_loss: 1.0275 - val_accuracy: 0.4385\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.9967 - accuracy: 0.50 - ETA: 2s - loss: 1.0078 - accuracy: 0.46 - ETA: 2s - loss: 1.0130 - accuracy: 0.44 - ETA: 2s - loss: 1.0176 - accuracy: 0.44 - ETA: 2s - loss: 1.0183 - accuracy: 0.43 - ETA: 2s - loss: 1.0199 - accuracy: 0.43 - ETA: 2s - loss: 1.0197 - accuracy: 0.43 - ETA: 2s - loss: 1.0191 - accuracy: 0.43 - ETA: 2s - loss: 1.0189 - accuracy: 0.43 - ETA: 2s - loss: 1.0180 - accuracy: 0.43 - ETA: 2s - loss: 1.0181 - accuracy: 0.43 - ETA: 1s - loss: 1.0171 - accuracy: 0.44 - ETA: 1s - loss: 1.0164 - accuracy: 0.44 - ETA: 1s - loss: 1.0160 - accuracy: 0.44 - ETA: 1s - loss: 1.0147 - accuracy: 0.45 - ETA: 1s - loss: 1.0144 - accuracy: 0.45 - ETA: 1s - loss: 1.0139 - accuracy: 0.45 - ETA: 1s - loss: 1.0131 - accuracy: 0.45 - ETA: 1s - loss: 1.0123 - accuracy: 0.46 - ETA: 1s - loss: 1.0127 - accuracy: 0.45 - ETA: 1s - loss: 1.0117 - accuracy: 0.46 - ETA: 1s - loss: 1.0109 - accuracy: 0.46 - ETA: 1s - loss: 1.0104 - accuracy: 0.46 - ETA: 1s - loss: 1.0101 - accuracy: 0.46 - ETA: 1s - loss: 1.0089 - accuracy: 0.46 - ETA: 0s - loss: 1.0086 - accuracy: 0.46 - ETA: 0s - loss: 1.0086 - accuracy: 0.46 - ETA: 0s - loss: 1.0081 - accuracy: 0.46 - ETA: 0s - loss: 1.0080 - accuracy: 0.46 - ETA: 0s - loss: 1.0069 - accuracy: 0.46 - ETA: 0s - loss: 1.0065 - accuracy: 0.46 - ETA: 0s - loss: 1.0061 - accuracy: 0.46 - ETA: 0s - loss: 1.0060 - accuracy: 0.46 - ETA: 0s - loss: 1.0058 - accuracy: 0.46 - ETA: 0s - loss: 1.0056 - accuracy: 0.46 - ETA: 0s - loss: 1.0052 - accuracy: 0.46 - ETA: 0s - loss: 1.0051 - accuracy: 0.47 - ETA: 0s - loss: 1.0046 - accuracy: 0.47 - ETA: 0s - loss: 1.0045 - accuracy: 0.47 - ETA: 0s - loss: 1.0040 - accuracy: 0.47 - ETA: 0s - loss: 1.0036 - accuracy: 0.47 - ETA: 0s - loss: 1.0031 - accuracy: 0.47 - ETA: 0s - loss: 1.0027 - accuracy: 0.47 - 3s 165us/step - loss: 1.0021 - accuracy: 0.4789 - val_loss: 0.9944 - val_accuracy: 0.4872\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.9842 - accuracy: 0.48 - ETA: 2s - loss: 0.9757 - accuracy: 0.50 - ETA: 2s - loss: 0.9785 - accuracy: 0.50 - ETA: 2s - loss: 0.9793 - accuracy: 0.49 - ETA: 2s - loss: 0.9754 - accuracy: 0.49 - ETA: 2s - loss: 0.9772 - accuracy: 0.49 - ETA: 2s - loss: 0.9732 - accuracy: 0.50 - ETA: 2s - loss: 0.9726 - accuracy: 0.50 - ETA: 2s - loss: 0.9728 - accuracy: 0.50 - ETA: 2s - loss: 0.9730 - accuracy: 0.50 - ETA: 1s - loss: 0.9728 - accuracy: 0.50 - ETA: 1s - loss: 0.9729 - accuracy: 0.50 - ETA: 1s - loss: 0.9735 - accuracy: 0.50 - ETA: 1s - loss: 0.9731 - accuracy: 0.51 - ETA: 1s - loss: 0.9724 - accuracy: 0.51 - ETA: 1s - loss: 0.9725 - accuracy: 0.51 - ETA: 1s - loss: 0.9732 - accuracy: 0.52 - ETA: 1s - loss: 0.9719 - accuracy: 0.52 - ETA: 1s - loss: 0.9706 - accuracy: 0.53 - ETA: 1s - loss: 0.9697 - accuracy: 0.53 - ETA: 1s - loss: 0.9694 - accuracy: 0.54 - ETA: 1s - loss: 0.9693 - accuracy: 0.54 - ETA: 1s - loss: 0.9696 - accuracy: 0.54 - ETA: 1s - loss: 0.9689 - accuracy: 0.54 - ETA: 1s - loss: 0.9688 - accuracy: 0.54 - ETA: 1s - loss: 0.9681 - accuracy: 0.54 - ETA: 1s - loss: 0.9674 - accuracy: 0.54 - ETA: 1s - loss: 0.9676 - accuracy: 0.54 - ETA: 1s - loss: 0.9667 - accuracy: 0.54 - ETA: 0s - loss: 0.9665 - accuracy: 0.54 - ETA: 0s - loss: 0.9670 - accuracy: 0.54 - ETA: 0s - loss: 0.9668 - accuracy: 0.54 - ETA: 0s - loss: 0.9661 - accuracy: 0.54 - ETA: 0s - loss: 0.9656 - accuracy: 0.55 - ETA: 0s - loss: 0.9648 - accuracy: 0.55 - ETA: 0s - loss: 0.9644 - accuracy: 0.55 - ETA: 0s - loss: 0.9637 - accuracy: 0.56 - ETA: 0s - loss: 0.9631 - accuracy: 0.56 - ETA: 0s - loss: 0.9627 - accuracy: 0.56 - ETA: 0s - loss: 0.9627 - accuracy: 0.56 - ETA: 0s - loss: 0.9622 - accuracy: 0.57 - ETA: 0s - loss: 0.9622 - accuracy: 0.57 - ETA: 0s - loss: 0.9618 - accuracy: 0.57 - ETA: 0s - loss: 0.9609 - accuracy: 0.57 - ETA: 0s - loss: 0.9605 - accuracy: 0.57 - ETA: 0s - loss: 0.9604 - accuracy: 0.57 - ETA: 0s - loss: 0.9604 - accuracy: 0.57 - ETA: 0s - loss: 0.9599 - accuracy: 0.57 - 3s 170us/step - loss: 0.9598 - accuracy: 0.5726 - val_loss: 0.9557 - val_accuracy: 0.5659\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.9428 - accuracy: 0.54 - ETA: 2s - loss: 0.9280 - accuracy: 0.62 - ETA: 2s - loss: 0.9315 - accuracy: 0.60 - ETA: 2s - loss: 0.9251 - accuracy: 0.62 - ETA: 2s - loss: 0.9209 - accuracy: 0.63 - ETA: 1s - loss: 0.9226 - accuracy: 0.63 - ETA: 1s - loss: 0.9258 - accuracy: 0.62 - ETA: 1s - loss: 0.9271 - accuracy: 0.62 - ETA: 1s - loss: 0.9274 - accuracy: 0.62 - ETA: 1s - loss: 0.9268 - accuracy: 0.62 - ETA: 1s - loss: 0.9257 - accuracy: 0.62 - ETA: 1s - loss: 0.9253 - accuracy: 0.63 - ETA: 1s - loss: 0.9247 - accuracy: 0.63 - ETA: 1s - loss: 0.9245 - accuracy: 0.63 - ETA: 1s - loss: 0.9248 - accuracy: 0.63 - ETA: 1s - loss: 0.9248 - accuracy: 0.63 - ETA: 1s - loss: 0.9244 - accuracy: 0.63 - ETA: 1s - loss: 0.9243 - accuracy: 0.63 - ETA: 1s - loss: 0.9229 - accuracy: 0.63 - ETA: 1s - loss: 0.9227 - accuracy: 0.63 - ETA: 1s - loss: 0.9218 - accuracy: 0.63 - ETA: 1s - loss: 0.9210 - accuracy: 0.64 - ETA: 0s - loss: 0.9196 - accuracy: 0.64 - ETA: 0s - loss: 0.9191 - accuracy: 0.64 - ETA: 0s - loss: 0.9193 - accuracy: 0.64 - ETA: 0s - loss: 0.9191 - accuracy: 0.64 - ETA: 0s - loss: 0.9186 - accuracy: 0.64 - ETA: 0s - loss: 0.9181 - accuracy: 0.64 - ETA: 0s - loss: 0.9174 - accuracy: 0.64 - ETA: 0s - loss: 0.9169 - accuracy: 0.64 - ETA: 0s - loss: 0.9164 - accuracy: 0.64 - ETA: 0s - loss: 0.9157 - accuracy: 0.64 - ETA: 0s - loss: 0.9150 - accuracy: 0.64 - ETA: 0s - loss: 0.9143 - accuracy: 0.65 - ETA: 0s - loss: 0.9137 - accuracy: 0.65 - ETA: 0s - loss: 0.9131 - accuracy: 0.65 - ETA: 0s - loss: 0.9127 - accuracy: 0.65 - ETA: 0s - loss: 0.9123 - accuracy: 0.65 - ETA: 0s - loss: 0.9122 - accuracy: 0.65 - 2s 156us/step - loss: 0.9116 - accuracy: 0.6574 - val_loss: 0.9154 - val_accuracy: 0.6920\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.8790 - accuracy: 0.72 - ETA: 2s - loss: 0.8831 - accuracy: 0.75 - ETA: 2s - loss: 0.8807 - accuracy: 0.75 - ETA: 2s - loss: 0.8830 - accuracy: 0.75 - ETA: 2s - loss: 0.8816 - accuracy: 0.75 - ETA: 2s - loss: 0.8809 - accuracy: 0.74 - ETA: 2s - loss: 0.8771 - accuracy: 0.75 - ETA: 2s - loss: 0.8779 - accuracy: 0.74 - ETA: 1s - loss: 0.8778 - accuracy: 0.74 - ETA: 1s - loss: 0.8768 - accuracy: 0.74 - ETA: 1s - loss: 0.8765 - accuracy: 0.73 - ETA: 1s - loss: 0.8764 - accuracy: 0.73 - ETA: 1s - loss: 0.8770 - accuracy: 0.72 - ETA: 1s - loss: 0.8769 - accuracy: 0.72 - ETA: 1s - loss: 0.8771 - accuracy: 0.71 - ETA: 1s - loss: 0.8748 - accuracy: 0.72 - ETA: 1s - loss: 0.8744 - accuracy: 0.72 - ETA: 1s - loss: 0.8720 - accuracy: 0.72 - ETA: 1s - loss: 0.8723 - accuracy: 0.72 - ETA: 1s - loss: 0.8711 - accuracy: 0.72 - ETA: 1s - loss: 0.8706 - accuracy: 0.72 - ETA: 1s - loss: 0.8702 - accuracy: 0.73 - ETA: 1s - loss: 0.8706 - accuracy: 0.73 - ETA: 1s - loss: 0.8704 - accuracy: 0.73 - ETA: 1s - loss: 0.8706 - accuracy: 0.73 - ETA: 0s - loss: 0.8702 - accuracy: 0.73 - ETA: 0s - loss: 0.8692 - accuracy: 0.73 - ETA: 0s - loss: 0.8687 - accuracy: 0.73 - ETA: 0s - loss: 0.8686 - accuracy: 0.73 - ETA: 0s - loss: 0.8675 - accuracy: 0.73 - ETA: 0s - loss: 0.8673 - accuracy: 0.73 - ETA: 0s - loss: 0.8666 - accuracy: 0.73 - ETA: 0s - loss: 0.8656 - accuracy: 0.73 - ETA: 0s - loss: 0.8645 - accuracy: 0.73 - ETA: 0s - loss: 0.8637 - accuracy: 0.73 - ETA: 0s - loss: 0.8634 - accuracy: 0.73 - ETA: 0s - loss: 0.8627 - accuracy: 0.73 - ETA: 0s - loss: 0.8627 - accuracy: 0.73 - ETA: 0s - loss: 0.8618 - accuracy: 0.73 - 2s 159us/step - loss: 0.8609 - accuracy: 0.7368 - val_loss: 0.8727 - val_accuracy: 0.6961\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.8187 - accuracy: 0.77 - ETA: 2s - loss: 0.8097 - accuracy: 0.76 - ETA: 2s - loss: 0.8090 - accuracy: 0.76 - ETA: 2s - loss: 0.8150 - accuracy: 0.74 - ETA: 2s - loss: 0.8159 - accuracy: 0.74 - ETA: 2s - loss: 0.8175 - accuracy: 0.74 - ETA: 2s - loss: 0.8177 - accuracy: 0.74 - ETA: 2s - loss: 0.8186 - accuracy: 0.73 - ETA: 1s - loss: 0.8214 - accuracy: 0.73 - ETA: 1s - loss: 0.8225 - accuracy: 0.73 - ETA: 1s - loss: 0.8236 - accuracy: 0.73 - ETA: 1s - loss: 0.8223 - accuracy: 0.73 - ETA: 1s - loss: 0.8221 - accuracy: 0.73 - ETA: 1s - loss: 0.8236 - accuracy: 0.74 - ETA: 1s - loss: 0.8230 - accuracy: 0.74 - ETA: 1s - loss: 0.8231 - accuracy: 0.74 - ETA: 1s - loss: 0.8216 - accuracy: 0.75 - ETA: 1s - loss: 0.8208 - accuracy: 0.75 - ETA: 1s - loss: 0.8201 - accuracy: 0.76 - ETA: 1s - loss: 0.8201 - accuracy: 0.76 - ETA: 1s - loss: 0.8192 - accuracy: 0.76 - ETA: 1s - loss: 0.8196 - accuracy: 0.76 - ETA: 1s - loss: 0.8189 - accuracy: 0.76 - ETA: 0s - loss: 0.8192 - accuracy: 0.76 - ETA: 0s - loss: 0.8188 - accuracy: 0.76 - ETA: 0s - loss: 0.8174 - accuracy: 0.76 - ETA: 0s - loss: 0.8164 - accuracy: 0.76 - ETA: 0s - loss: 0.8161 - accuracy: 0.76 - ETA: 0s - loss: 0.8159 - accuracy: 0.76 - ETA: 0s - loss: 0.8149 - accuracy: 0.76 - ETA: 0s - loss: 0.8140 - accuracy: 0.76 - ETA: 0s - loss: 0.8130 - accuracy: 0.77 - ETA: 0s - loss: 0.8132 - accuracy: 0.76 - ETA: 0s - loss: 0.8129 - accuracy: 0.76 - ETA: 0s - loss: 0.8124 - accuracy: 0.77 - ETA: 0s - loss: 0.8121 - accuracy: 0.77 - ETA: 0s - loss: 0.8115 - accuracy: 0.77 - ETA: 0s - loss: 0.8113 - accuracy: 0.77 - ETA: 0s - loss: 0.8105 - accuracy: 0.77 - ETA: 0s - loss: 0.8101 - accuracy: 0.77 - 3s 160us/step - loss: 0.8103 - accuracy: 0.7698 - val_loss: 0.8324 - val_accuracy: 0.7084\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.7640 - accuracy: 0.78 - ETA: 2s - loss: 0.7840 - accuracy: 0.77 - ETA: 2s - loss: 0.7924 - accuracy: 0.77 - ETA: 2s - loss: 0.7841 - accuracy: 0.78 - ETA: 2s - loss: 0.7824 - accuracy: 0.78 - ETA: 2s - loss: 0.7830 - accuracy: 0.78 - ETA: 2s - loss: 0.7819 - accuracy: 0.77 - ETA: 1s - loss: 0.7800 - accuracy: 0.78 - ETA: 1s - loss: 0.7772 - accuracy: 0.78 - ETA: 1s - loss: 0.7783 - accuracy: 0.77 - ETA: 1s - loss: 0.7768 - accuracy: 0.78 - ETA: 1s - loss: 0.7757 - accuracy: 0.78 - ETA: 1s - loss: 0.7747 - accuracy: 0.78 - ETA: 1s - loss: 0.7748 - accuracy: 0.78 - ETA: 1s - loss: 0.7736 - accuracy: 0.78 - ETA: 1s - loss: 0.7743 - accuracy: 0.78 - ETA: 1s - loss: 0.7736 - accuracy: 0.78 - ETA: 1s - loss: 0.7740 - accuracy: 0.79 - ETA: 1s - loss: 0.7741 - accuracy: 0.79 - ETA: 1s - loss: 0.7732 - accuracy: 0.79 - ETA: 1s - loss: 0.7739 - accuracy: 0.79 - ETA: 1s - loss: 0.7722 - accuracy: 0.79 - ETA: 1s - loss: 0.7719 - accuracy: 0.79 - ETA: 1s - loss: 0.7712 - accuracy: 0.79 - ETA: 1s - loss: 0.7711 - accuracy: 0.79 - ETA: 1s - loss: 0.7698 - accuracy: 0.79 - ETA: 1s - loss: 0.7689 - accuracy: 0.79 - ETA: 1s - loss: 0.7688 - accuracy: 0.79 - ETA: 0s - loss: 0.7686 - accuracy: 0.79 - ETA: 0s - loss: 0.7680 - accuracy: 0.79 - ETA: 0s - loss: 0.7679 - accuracy: 0.79 - ETA: 0s - loss: 0.7672 - accuracy: 0.79 - ETA: 0s - loss: 0.7669 - accuracy: 0.79 - ETA: 0s - loss: 0.7656 - accuracy: 0.79 - ETA: 0s - loss: 0.7653 - accuracy: 0.79 - ETA: 0s - loss: 0.7658 - accuracy: 0.79 - ETA: 0s - loss: 0.7652 - accuracy: 0.79 - ETA: 0s - loss: 0.7650 - accuracy: 0.79 - ETA: 0s - loss: 0.7651 - accuracy: 0.79 - ETA: 0s - loss: 0.7648 - accuracy: 0.79 - ETA: 0s - loss: 0.7643 - accuracy: 0.79 - ETA: 0s - loss: 0.7643 - accuracy: 0.79 - ETA: 0s - loss: 0.7638 - accuracy: 0.79 - ETA: 0s - loss: 0.7630 - accuracy: 0.79 - ETA: 0s - loss: 0.7625 - accuracy: 0.80 - ETA: 0s - loss: 0.7617 - accuracy: 0.80 - ETA: 0s - loss: 0.7614 - accuracy: 0.80 - 3s 169us/step - loss: 0.7613 - accuracy: 0.8011 - val_loss: 0.7942 - val_accuracy: 0.7342\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.7345 - accuracy: 0.81 - ETA: 2s - loss: 0.7279 - accuracy: 0.81 - ETA: 2s - loss: 0.7432 - accuracy: 0.78 - ETA: 2s - loss: 0.7486 - accuracy: 0.78 - ETA: 2s - loss: 0.7467 - accuracy: 0.78 - ETA: 2s - loss: 0.7502 - accuracy: 0.78 - ETA: 2s - loss: 0.7447 - accuracy: 0.78 - ETA: 2s - loss: 0.7457 - accuracy: 0.79 - ETA: 2s - loss: 0.7405 - accuracy: 0.80 - ETA: 2s - loss: 0.7395 - accuracy: 0.80 - ETA: 2s - loss: 0.7396 - accuracy: 0.81 - ETA: 2s - loss: 0.7408 - accuracy: 0.81 - ETA: 2s - loss: 0.7402 - accuracy: 0.81 - ETA: 1s - loss: 0.7390 - accuracy: 0.81 - ETA: 1s - loss: 0.7397 - accuracy: 0.81 - ETA: 1s - loss: 0.7375 - accuracy: 0.81 - ETA: 1s - loss: 0.7355 - accuracy: 0.81 - ETA: 1s - loss: 0.7347 - accuracy: 0.81 - ETA: 1s - loss: 0.7349 - accuracy: 0.80 - ETA: 1s - loss: 0.7353 - accuracy: 0.80 - ETA: 1s - loss: 0.7347 - accuracy: 0.80 - ETA: 1s - loss: 0.7340 - accuracy: 0.80 - ETA: 1s - loss: 0.7338 - accuracy: 0.80 - ETA: 1s - loss: 0.7334 - accuracy: 0.80 - ETA: 1s - loss: 0.7316 - accuracy: 0.80 - ETA: 1s - loss: 0.7308 - accuracy: 0.81 - ETA: 1s - loss: 0.7305 - accuracy: 0.81 - ETA: 1s - loss: 0.7295 - accuracy: 0.81 - ETA: 1s - loss: 0.7278 - accuracy: 0.81 - ETA: 1s - loss: 0.7281 - accuracy: 0.81 - ETA: 1s - loss: 0.7274 - accuracy: 0.81 - ETA: 0s - loss: 0.7265 - accuracy: 0.81 - ETA: 0s - loss: 0.7266 - accuracy: 0.81 - ETA: 0s - loss: 0.7263 - accuracy: 0.81 - ETA: 0s - loss: 0.7257 - accuracy: 0.81 - ETA: 0s - loss: 0.7253 - accuracy: 0.81 - ETA: 0s - loss: 0.7249 - accuracy: 0.81 - ETA: 0s - loss: 0.7245 - accuracy: 0.81 - ETA: 0s - loss: 0.7242 - accuracy: 0.81 - ETA: 0s - loss: 0.7235 - accuracy: 0.81 - ETA: 0s - loss: 0.7219 - accuracy: 0.81 - ETA: 0s - loss: 0.7210 - accuracy: 0.81 - ETA: 0s - loss: 0.7202 - accuracy: 0.81 - ETA: 0s - loss: 0.7204 - accuracy: 0.81 - ETA: 0s - loss: 0.7190 - accuracy: 0.81 - ETA: 0s - loss: 0.7182 - accuracy: 0.81 - ETA: 0s - loss: 0.7177 - accuracy: 0.81 - ETA: 0s - loss: 0.7161 - accuracy: 0.81 - 3s 172us/step - loss: 0.7160 - accuracy: 0.8191 - val_loss: 0.7598 - val_accuracy: 0.7385\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.6769 - accuracy: 0.80 - ETA: 2s - loss: 0.6902 - accuracy: 0.77 - ETA: 2s - loss: 0.6896 - accuracy: 0.79 - ETA: 2s - loss: 0.6851 - accuracy: 0.80 - ETA: 2s - loss: 0.6806 - accuracy: 0.81 - ETA: 2s - loss: 0.6849 - accuracy: 0.80 - ETA: 2s - loss: 0.6839 - accuracy: 0.81 - ETA: 2s - loss: 0.6841 - accuracy: 0.81 - ETA: 2s - loss: 0.6870 - accuracy: 0.81 - ETA: 2s - loss: 0.6872 - accuracy: 0.81 - ETA: 2s - loss: 0.6850 - accuracy: 0.82 - ETA: 2s - loss: 0.6862 - accuracy: 0.82 - ETA: 2s - loss: 0.6851 - accuracy: 0.82 - ETA: 2s - loss: 0.6849 - accuracy: 0.82 - ETA: 2s - loss: 0.6865 - accuracy: 0.82 - ETA: 1s - loss: 0.6844 - accuracy: 0.82 - ETA: 1s - loss: 0.6818 - accuracy: 0.82 - ETA: 1s - loss: 0.6813 - accuracy: 0.82 - ETA: 1s - loss: 0.6826 - accuracy: 0.82 - ETA: 1s - loss: 0.6841 - accuracy: 0.81 - ETA: 1s - loss: 0.6836 - accuracy: 0.82 - ETA: 1s - loss: 0.6828 - accuracy: 0.82 - ETA: 1s - loss: 0.6815 - accuracy: 0.82 - ETA: 1s - loss: 0.6813 - accuracy: 0.82 - ETA: 1s - loss: 0.6803 - accuracy: 0.82 - ETA: 1s - loss: 0.6791 - accuracy: 0.82 - ETA: 1s - loss: 0.6794 - accuracy: 0.82 - ETA: 1s - loss: 0.6792 - accuracy: 0.82 - ETA: 1s - loss: 0.6786 - accuracy: 0.82 - ETA: 1s - loss: 0.6787 - accuracy: 0.82 - ETA: 1s - loss: 0.6792 - accuracy: 0.82 - ETA: 1s - loss: 0.6783 - accuracy: 0.82 - ETA: 0s - loss: 0.6780 - accuracy: 0.82 - ETA: 0s - loss: 0.6775 - accuracy: 0.82 - ETA: 0s - loss: 0.6777 - accuracy: 0.82 - ETA: 0s - loss: 0.6762 - accuracy: 0.82 - ETA: 0s - loss: 0.6769 - accuracy: 0.82 - ETA: 0s - loss: 0.6768 - accuracy: 0.83 - ETA: 0s - loss: 0.6758 - accuracy: 0.83 - ETA: 0s - loss: 0.6755 - accuracy: 0.83 - ETA: 0s - loss: 0.6741 - accuracy: 0.83 - ETA: 0s - loss: 0.6741 - accuracy: 0.83 - ETA: 0s - loss: 0.6744 - accuracy: 0.83 - ETA: 0s - loss: 0.6736 - accuracy: 0.83 - ETA: 0s - loss: 0.6738 - accuracy: 0.83 - ETA: 0s - loss: 0.6741 - accuracy: 0.83 - ETA: 0s - loss: 0.6731 - accuracy: 0.83 - ETA: 0s - loss: 0.6728 - accuracy: 0.83 - ETA: 0s - loss: 0.6726 - accuracy: 0.83 - 3s 185us/step - loss: 0.6728 - accuracy: 0.8332 - val_loss: 0.7275 - val_accuracy: 0.7730\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.6615 - accuracy: 0.83 - ETA: 2s - loss: 0.6537 - accuracy: 0.85 - ETA: 2s - loss: 0.6458 - accuracy: 0.85 - ETA: 2s - loss: 0.6517 - accuracy: 0.85 - ETA: 2s - loss: 0.6539 - accuracy: 0.85 - ETA: 2s - loss: 0.6532 - accuracy: 0.85 - ETA: 2s - loss: 0.6490 - accuracy: 0.85 - ETA: 2s - loss: 0.6487 - accuracy: 0.85 - ETA: 2s - loss: 0.6518 - accuracy: 0.85 - ETA: 2s - loss: 0.6513 - accuracy: 0.84 - ETA: 2s - loss: 0.6529 - accuracy: 0.84 - ETA: 2s - loss: 0.6504 - accuracy: 0.84 - ETA: 2s - loss: 0.6469 - accuracy: 0.84 - ETA: 2s - loss: 0.6477 - accuracy: 0.84 - ETA: 1s - loss: 0.6457 - accuracy: 0.84 - ETA: 1s - loss: 0.6434 - accuracy: 0.84 - ETA: 1s - loss: 0.6421 - accuracy: 0.84 - ETA: 1s - loss: 0.6407 - accuracy: 0.85 - ETA: 1s - loss: 0.6407 - accuracy: 0.85 - ETA: 1s - loss: 0.6404 - accuracy: 0.84 - ETA: 1s - loss: 0.6389 - accuracy: 0.84 - ETA: 1s - loss: 0.6396 - accuracy: 0.84 - ETA: 1s - loss: 0.6402 - accuracy: 0.84 - ETA: 1s - loss: 0.6406 - accuracy: 0.84 - ETA: 1s - loss: 0.6411 - accuracy: 0.84 - ETA: 1s - loss: 0.6415 - accuracy: 0.84 - ETA: 1s - loss: 0.6412 - accuracy: 0.84 - ETA: 1s - loss: 0.6402 - accuracy: 0.84 - ETA: 1s - loss: 0.6402 - accuracy: 0.84 - ETA: 1s - loss: 0.6401 - accuracy: 0.84 - ETA: 0s - loss: 0.6395 - accuracy: 0.84 - ETA: 0s - loss: 0.6386 - accuracy: 0.84 - ETA: 0s - loss: 0.6377 - accuracy: 0.84 - ETA: 0s - loss: 0.6372 - accuracy: 0.84 - ETA: 0s - loss: 0.6369 - accuracy: 0.84 - ETA: 0s - loss: 0.6360 - accuracy: 0.84 - ETA: 0s - loss: 0.6360 - accuracy: 0.84 - ETA: 0s - loss: 0.6355 - accuracy: 0.84 - ETA: 0s - loss: 0.6352 - accuracy: 0.84 - ETA: 0s - loss: 0.6352 - accuracy: 0.84 - ETA: 0s - loss: 0.6351 - accuracy: 0.84 - ETA: 0s - loss: 0.6340 - accuracy: 0.84 - ETA: 0s - loss: 0.6333 - accuracy: 0.84 - ETA: 0s - loss: 0.6332 - accuracy: 0.84 - ETA: 0s - loss: 0.6333 - accuracy: 0.84 - ETA: 0s - loss: 0.6332 - accuracy: 0.84 - ETA: 0s - loss: 0.6331 - accuracy: 0.84 - 3s 172us/step - loss: 0.6331 - accuracy: 0.8470 - val_loss: 0.6989 - val_accuracy: 0.7822\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.6125 - accuracy: 0.84 - ETA: 2s - loss: 0.6222 - accuracy: 0.85 - ETA: 2s - loss: 0.6195 - accuracy: 0.85 - ETA: 2s - loss: 0.6134 - accuracy: 0.86 - ETA: 2s - loss: 0.6207 - accuracy: 0.85 - ETA: 2s - loss: 0.6153 - accuracy: 0.85 - ETA: 2s - loss: 0.6128 - accuracy: 0.85 - ETA: 2s - loss: 0.6150 - accuracy: 0.84 - ETA: 2s - loss: 0.6159 - accuracy: 0.84 - ETA: 2s - loss: 0.6153 - accuracy: 0.84 - ETA: 2s - loss: 0.6137 - accuracy: 0.84 - ETA: 1s - loss: 0.6151 - accuracy: 0.84 - ETA: 1s - loss: 0.6145 - accuracy: 0.84 - ETA: 1s - loss: 0.6137 - accuracy: 0.84 - ETA: 1s - loss: 0.6121 - accuracy: 0.84 - ETA: 1s - loss: 0.6138 - accuracy: 0.84 - ETA: 1s - loss: 0.6132 - accuracy: 0.84 - ETA: 1s - loss: 0.6111 - accuracy: 0.84 - ETA: 1s - loss: 0.6112 - accuracy: 0.84 - ETA: 1s - loss: 0.6113 - accuracy: 0.84 - ETA: 1s - loss: 0.6099 - accuracy: 0.85 - ETA: 1s - loss: 0.6090 - accuracy: 0.85 - ETA: 1s - loss: 0.6069 - accuracy: 0.85 - ETA: 1s - loss: 0.6066 - accuracy: 0.85 - ETA: 1s - loss: 0.6056 - accuracy: 0.85 - ETA: 1s - loss: 0.6050 - accuracy: 0.85 - ETA: 1s - loss: 0.6056 - accuracy: 0.85 - ETA: 1s - loss: 0.6040 - accuracy: 0.85 - ETA: 1s - loss: 0.6044 - accuracy: 0.85 - ETA: 1s - loss: 0.6040 - accuracy: 0.85 - ETA: 0s - loss: 0.6036 - accuracy: 0.85 - ETA: 0s - loss: 0.6030 - accuracy: 0.85 - ETA: 0s - loss: 0.6025 - accuracy: 0.85 - ETA: 0s - loss: 0.6012 - accuracy: 0.85 - ETA: 0s - loss: 0.6016 - accuracy: 0.85 - ETA: 0s - loss: 0.6014 - accuracy: 0.85 - ETA: 0s - loss: 0.6008 - accuracy: 0.85 - ETA: 0s - loss: 0.6008 - accuracy: 0.85 - ETA: 0s - loss: 0.6006 - accuracy: 0.85 - ETA: 0s - loss: 0.6003 - accuracy: 0.85 - ETA: 0s - loss: 0.5998 - accuracy: 0.85 - ETA: 0s - loss: 0.5993 - accuracy: 0.85 - ETA: 0s - loss: 0.5980 - accuracy: 0.85 - ETA: 0s - loss: 0.5974 - accuracy: 0.85 - ETA: 0s - loss: 0.5979 - accuracy: 0.85 - ETA: 0s - loss: 0.5977 - accuracy: 0.85 - ETA: 0s - loss: 0.5983 - accuracy: 0.85 - ETA: 0s - loss: 0.5975 - accuracy: 0.85 - 3s 173us/step - loss: 0.5970 - accuracy: 0.8581 - val_loss: 0.6728 - val_accuracy: 0.7845\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.6313 - accuracy: 0.79 - ETA: 2s - loss: 0.5976 - accuracy: 0.82 - ETA: 2s - loss: 0.5868 - accuracy: 0.84 - ETA: 2s - loss: 0.5870 - accuracy: 0.84 - ETA: 2s - loss: 0.5846 - accuracy: 0.85 - ETA: 2s - loss: 0.5817 - accuracy: 0.85 - ETA: 2s - loss: 0.5805 - accuracy: 0.85 - ETA: 2s - loss: 0.5797 - accuracy: 0.86 - ETA: 2s - loss: 0.5806 - accuracy: 0.86 - ETA: 2s - loss: 0.5792 - accuracy: 0.86 - ETA: 2s - loss: 0.5772 - accuracy: 0.86 - ETA: 2s - loss: 0.5776 - accuracy: 0.86 - ETA: 2s - loss: 0.5748 - accuracy: 0.87 - ETA: 2s - loss: 0.5715 - accuracy: 0.87 - ETA: 2s - loss: 0.5726 - accuracy: 0.87 - ETA: 1s - loss: 0.5725 - accuracy: 0.87 - ETA: 1s - loss: 0.5740 - accuracy: 0.87 - ETA: 1s - loss: 0.5736 - accuracy: 0.87 - ETA: 1s - loss: 0.5722 - accuracy: 0.86 - ETA: 1s - loss: 0.5722 - accuracy: 0.86 - ETA: 1s - loss: 0.5721 - accuracy: 0.86 - ETA: 1s - loss: 0.5721 - accuracy: 0.86 - ETA: 1s - loss: 0.5721 - accuracy: 0.86 - ETA: 1s - loss: 0.5717 - accuracy: 0.86 - ETA: 1s - loss: 0.5714 - accuracy: 0.86 - ETA: 1s - loss: 0.5699 - accuracy: 0.86 - ETA: 1s - loss: 0.5692 - accuracy: 0.86 - ETA: 1s - loss: 0.5698 - accuracy: 0.86 - ETA: 1s - loss: 0.5690 - accuracy: 0.86 - ETA: 1s - loss: 0.5686 - accuracy: 0.86 - ETA: 1s - loss: 0.5676 - accuracy: 0.86 - ETA: 0s - loss: 0.5666 - accuracy: 0.86 - ETA: 0s - loss: 0.5667 - accuracy: 0.86 - ETA: 0s - loss: 0.5666 - accuracy: 0.86 - ETA: 0s - loss: 0.5662 - accuracy: 0.86 - ETA: 0s - loss: 0.5658 - accuracy: 0.86 - ETA: 0s - loss: 0.5662 - accuracy: 0.86 - ETA: 0s - loss: 0.5663 - accuracy: 0.86 - ETA: 0s - loss: 0.5661 - accuracy: 0.86 - ETA: 0s - loss: 0.5650 - accuracy: 0.86 - ETA: 0s - loss: 0.5658 - accuracy: 0.86 - ETA: 0s - loss: 0.5653 - accuracy: 0.86 - ETA: 0s - loss: 0.5650 - accuracy: 0.86 - ETA: 0s - loss: 0.5639 - accuracy: 0.86 - ETA: 0s - loss: 0.5632 - accuracy: 0.86 - ETA: 0s - loss: 0.5630 - accuracy: 0.86 - ETA: 0s - loss: 0.5630 - accuracy: 0.86 - ETA: 0s - loss: 0.5635 - accuracy: 0.86 - ETA: 0s - loss: 0.5638 - accuracy: 0.86 - 3s 178us/step - loss: 0.5639 - accuracy: 0.8676 - val_loss: 0.6485 - val_accuracy: 0.7937\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.5813 - accuracy: 0.83 - ETA: 2s - loss: 0.5743 - accuracy: 0.84 - ETA: 2s - loss: 0.5634 - accuracy: 0.86 - ETA: 2s - loss: 0.5626 - accuracy: 0.86 - ETA: 2s - loss: 0.5526 - accuracy: 0.87 - ETA: 2s - loss: 0.5532 - accuracy: 0.86 - ETA: 2s - loss: 0.5508 - accuracy: 0.87 - ETA: 2s - loss: 0.5538 - accuracy: 0.87 - ETA: 2s - loss: 0.5558 - accuracy: 0.87 - ETA: 2s - loss: 0.5520 - accuracy: 0.87 - ETA: 2s - loss: 0.5502 - accuracy: 0.87 - ETA: 2s - loss: 0.5528 - accuracy: 0.87 - ETA: 1s - loss: 0.5530 - accuracy: 0.87 - ETA: 1s - loss: 0.5530 - accuracy: 0.86 - ETA: 1s - loss: 0.5511 - accuracy: 0.86 - ETA: 1s - loss: 0.5496 - accuracy: 0.87 - ETA: 1s - loss: 0.5473 - accuracy: 0.87 - ETA: 1s - loss: 0.5460 - accuracy: 0.87 - ETA: 1s - loss: 0.5458 - accuracy: 0.87 - ETA: 1s - loss: 0.5459 - accuracy: 0.87 - ETA: 1s - loss: 0.5461 - accuracy: 0.87 - ETA: 1s - loss: 0.5455 - accuracy: 0.87 - ETA: 1s - loss: 0.5453 - accuracy: 0.87 - ETA: 1s - loss: 0.5442 - accuracy: 0.87 - ETA: 1s - loss: 0.5436 - accuracy: 0.87 - ETA: 1s - loss: 0.5440 - accuracy: 0.87 - ETA: 1s - loss: 0.5426 - accuracy: 0.87 - ETA: 1s - loss: 0.5424 - accuracy: 0.87 - ETA: 1s - loss: 0.5427 - accuracy: 0.87 - ETA: 1s - loss: 0.5416 - accuracy: 0.87 - ETA: 0s - loss: 0.5413 - accuracy: 0.87 - ETA: 0s - loss: 0.5418 - accuracy: 0.87 - ETA: 0s - loss: 0.5412 - accuracy: 0.87 - ETA: 0s - loss: 0.5412 - accuracy: 0.87 - ETA: 0s - loss: 0.5408 - accuracy: 0.87 - ETA: 0s - loss: 0.5397 - accuracy: 0.87 - ETA: 0s - loss: 0.5387 - accuracy: 0.87 - ETA: 0s - loss: 0.5384 - accuracy: 0.87 - ETA: 0s - loss: 0.5377 - accuracy: 0.87 - ETA: 0s - loss: 0.5368 - accuracy: 0.87 - ETA: 0s - loss: 0.5366 - accuracy: 0.87 - ETA: 0s - loss: 0.5365 - accuracy: 0.87 - ETA: 0s - loss: 0.5363 - accuracy: 0.87 - ETA: 0s - loss: 0.5359 - accuracy: 0.87 - ETA: 0s - loss: 0.5347 - accuracy: 0.87 - ETA: 0s - loss: 0.5339 - accuracy: 0.87 - ETA: 0s - loss: 0.5332 - accuracy: 0.87 - 3s 169us/step - loss: 0.5326 - accuracy: 0.8775 - val_loss: 0.6281 - val_accuracy: 0.8008\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.5094 - accuracy: 0.87 - ETA: 2s - loss: 0.5066 - accuracy: 0.88 - ETA: 2s - loss: 0.5210 - accuracy: 0.88 - ETA: 2s - loss: 0.5158 - accuracy: 0.88 - ETA: 2s - loss: 0.5173 - accuracy: 0.88 - ETA: 2s - loss: 0.5162 - accuracy: 0.88 - ETA: 2s - loss: 0.5152 - accuracy: 0.88 - ETA: 2s - loss: 0.5135 - accuracy: 0.88 - ETA: 2s - loss: 0.5134 - accuracy: 0.88 - ETA: 1s - loss: 0.5099 - accuracy: 0.88 - ETA: 1s - loss: 0.5092 - accuracy: 0.88 - ETA: 1s - loss: 0.5094 - accuracy: 0.88 - ETA: 1s - loss: 0.5063 - accuracy: 0.88 - ETA: 1s - loss: 0.5077 - accuracy: 0.88 - ETA: 1s - loss: 0.5083 - accuracy: 0.88 - ETA: 1s - loss: 0.5105 - accuracy: 0.88 - ETA: 1s - loss: 0.5123 - accuracy: 0.88 - ETA: 1s - loss: 0.5112 - accuracy: 0.88 - ETA: 1s - loss: 0.5111 - accuracy: 0.88 - ETA: 1s - loss: 0.5109 - accuracy: 0.88 - ETA: 1s - loss: 0.5107 - accuracy: 0.88 - ETA: 1s - loss: 0.5116 - accuracy: 0.88 - ETA: 1s - loss: 0.5107 - accuracy: 0.88 - ETA: 1s - loss: 0.5113 - accuracy: 0.88 - ETA: 1s - loss: 0.5089 - accuracy: 0.88 - ETA: 0s - loss: 0.5087 - accuracy: 0.88 - ETA: 0s - loss: 0.5085 - accuracy: 0.88 - ETA: 0s - loss: 0.5088 - accuracy: 0.88 - ETA: 0s - loss: 0.5071 - accuracy: 0.88 - ETA: 0s - loss: 0.5066 - accuracy: 0.88 - ETA: 0s - loss: 0.5058 - accuracy: 0.88 - ETA: 0s - loss: 0.5056 - accuracy: 0.88 - ETA: 0s - loss: 0.5070 - accuracy: 0.88 - ETA: 0s - loss: 0.5071 - accuracy: 0.88 - ETA: 0s - loss: 0.5072 - accuracy: 0.88 - ETA: 0s - loss: 0.5074 - accuracy: 0.88 - ETA: 0s - loss: 0.5072 - accuracy: 0.88 - ETA: 0s - loss: 0.5078 - accuracy: 0.88 - ETA: 0s - loss: 0.5065 - accuracy: 0.88 - ETA: 0s - loss: 0.5061 - accuracy: 0.88 - ETA: 0s - loss: 0.5060 - accuracy: 0.88 - ETA: 0s - loss: 0.5054 - accuracy: 0.88 - ETA: 0s - loss: 0.5052 - accuracy: 0.88 - 3s 164us/step - loss: 0.5047 - accuracy: 0.8865 - val_loss: 0.6079 - val_accuracy: 0.7949\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.5036 - accuracy: 0.86 - ETA: 2s - loss: 0.4777 - accuracy: 0.89 - ETA: 2s - loss: 0.4842 - accuracy: 0.88 - ETA: 2s - loss: 0.4888 - accuracy: 0.88 - ETA: 2s - loss: 0.4891 - accuracy: 0.88 - ETA: 2s - loss: 0.4891 - accuracy: 0.88 - ETA: 2s - loss: 0.4924 - accuracy: 0.88 - ETA: 2s - loss: 0.4962 - accuracy: 0.88 - ETA: 2s - loss: 0.4944 - accuracy: 0.88 - ETA: 2s - loss: 0.4974 - accuracy: 0.88 - ETA: 2s - loss: 0.4949 - accuracy: 0.88 - ETA: 2s - loss: 0.4933 - accuracy: 0.88 - ETA: 2s - loss: 0.4935 - accuracy: 0.88 - ETA: 1s - loss: 0.4921 - accuracy: 0.88 - ETA: 1s - loss: 0.4938 - accuracy: 0.88 - ETA: 1s - loss: 0.4942 - accuracy: 0.88 - ETA: 1s - loss: 0.4924 - accuracy: 0.88 - ETA: 1s - loss: 0.4905 - accuracy: 0.88 - ETA: 1s - loss: 0.4903 - accuracy: 0.88 - ETA: 1s - loss: 0.4891 - accuracy: 0.88 - ETA: 1s - loss: 0.4883 - accuracy: 0.88 - ETA: 1s - loss: 0.4886 - accuracy: 0.88 - ETA: 1s - loss: 0.4877 - accuracy: 0.88 - ETA: 1s - loss: 0.4863 - accuracy: 0.88 - ETA: 1s - loss: 0.4857 - accuracy: 0.88 - ETA: 1s - loss: 0.4850 - accuracy: 0.88 - ETA: 1s - loss: 0.4838 - accuracy: 0.88 - ETA: 1s - loss: 0.4837 - accuracy: 0.88 - ETA: 1s - loss: 0.4833 - accuracy: 0.88 - ETA: 1s - loss: 0.4822 - accuracy: 0.88 - ETA: 1s - loss: 0.4823 - accuracy: 0.88 - ETA: 0s - loss: 0.4822 - accuracy: 0.88 - ETA: 0s - loss: 0.4826 - accuracy: 0.88 - ETA: 0s - loss: 0.4823 - accuracy: 0.88 - ETA: 0s - loss: 0.4827 - accuracy: 0.88 - ETA: 0s - loss: 0.4819 - accuracy: 0.89 - ETA: 0s - loss: 0.4811 - accuracy: 0.89 - ETA: 0s - loss: 0.4812 - accuracy: 0.89 - ETA: 0s - loss: 0.4804 - accuracy: 0.89 - ETA: 0s - loss: 0.4799 - accuracy: 0.89 - ETA: 0s - loss: 0.4801 - accuracy: 0.89 - ETA: 0s - loss: 0.4796 - accuracy: 0.89 - ETA: 0s - loss: 0.4788 - accuracy: 0.89 - ETA: 0s - loss: 0.4784 - accuracy: 0.89 - ETA: 0s - loss: 0.4786 - accuracy: 0.89 - ETA: 0s - loss: 0.4780 - accuracy: 0.89 - ETA: 0s - loss: 0.4775 - accuracy: 0.89 - ETA: 0s - loss: 0.4779 - accuracy: 0.89 - ETA: 0s - loss: 0.4777 - accuracy: 0.89 - 3s 175us/step - loss: 0.4777 - accuracy: 0.8925 - val_loss: 0.5892 - val_accuracy: 0.8082\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.4700 - accuracy: 0.90 - ETA: 2s - loss: 0.4597 - accuracy: 0.90 - ETA: 2s - loss: 0.4610 - accuracy: 0.89 - ETA: 2s - loss: 0.4639 - accuracy: 0.89 - ETA: 2s - loss: 0.4555 - accuracy: 0.90 - ETA: 2s - loss: 0.4572 - accuracy: 0.90 - ETA: 2s - loss: 0.4644 - accuracy: 0.90 - ETA: 2s - loss: 0.4639 - accuracy: 0.90 - ETA: 2s - loss: 0.4639 - accuracy: 0.90 - ETA: 2s - loss: 0.4608 - accuracy: 0.90 - ETA: 2s - loss: 0.4611 - accuracy: 0.90 - ETA: 2s - loss: 0.4618 - accuracy: 0.90 - ETA: 1s - loss: 0.4597 - accuracy: 0.90 - ETA: 1s - loss: 0.4592 - accuracy: 0.90 - ETA: 1s - loss: 0.4577 - accuracy: 0.90 - ETA: 1s - loss: 0.4593 - accuracy: 0.90 - ETA: 1s - loss: 0.4608 - accuracy: 0.90 - ETA: 1s - loss: 0.4595 - accuracy: 0.90 - ETA: 1s - loss: 0.4602 - accuracy: 0.90 - ETA: 1s - loss: 0.4588 - accuracy: 0.90 - ETA: 1s - loss: 0.4579 - accuracy: 0.90 - ETA: 1s - loss: 0.4600 - accuracy: 0.90 - ETA: 1s - loss: 0.4605 - accuracy: 0.90 - ETA: 1s - loss: 0.4604 - accuracy: 0.90 - ETA: 1s - loss: 0.4591 - accuracy: 0.90 - ETA: 1s - loss: 0.4573 - accuracy: 0.90 - ETA: 1s - loss: 0.4568 - accuracy: 0.90 - ETA: 1s - loss: 0.4571 - accuracy: 0.90 - ETA: 1s - loss: 0.4565 - accuracy: 0.90 - ETA: 1s - loss: 0.4569 - accuracy: 0.90 - ETA: 0s - loss: 0.4578 - accuracy: 0.90 - ETA: 0s - loss: 0.4567 - accuracy: 0.90 - ETA: 0s - loss: 0.4565 - accuracy: 0.90 - ETA: 0s - loss: 0.4570 - accuracy: 0.90 - ETA: 0s - loss: 0.4562 - accuracy: 0.90 - ETA: 0s - loss: 0.4565 - accuracy: 0.90 - ETA: 0s - loss: 0.4561 - accuracy: 0.90 - ETA: 0s - loss: 0.4557 - accuracy: 0.90 - ETA: 0s - loss: 0.4550 - accuracy: 0.90 - ETA: 0s - loss: 0.4544 - accuracy: 0.90 - ETA: 0s - loss: 0.4542 - accuracy: 0.90 - ETA: 0s - loss: 0.4534 - accuracy: 0.90 - ETA: 0s - loss: 0.4537 - accuracy: 0.90 - ETA: 0s - loss: 0.4528 - accuracy: 0.90 - ETA: 0s - loss: 0.4532 - accuracy: 0.90 - ETA: 0s - loss: 0.4531 - accuracy: 0.90 - 3s 166us/step - loss: 0.4533 - accuracy: 0.9006 - val_loss: 0.5726 - val_accuracy: 0.8128\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.4380 - accuracy: 0.91 - ETA: 2s - loss: 0.4597 - accuracy: 0.90 - ETA: 2s - loss: 0.4624 - accuracy: 0.89 - ETA: 2s - loss: 0.4591 - accuracy: 0.89 - ETA: 2s - loss: 0.4484 - accuracy: 0.89 - ETA: 2s - loss: 0.4447 - accuracy: 0.90 - ETA: 1s - loss: 0.4464 - accuracy: 0.90 - ETA: 2s - loss: 0.4432 - accuracy: 0.90 - ETA: 1s - loss: 0.4431 - accuracy: 0.90 - ETA: 1s - loss: 0.4403 - accuracy: 0.90 - ETA: 1s - loss: 0.4414 - accuracy: 0.90 - ETA: 1s - loss: 0.4407 - accuracy: 0.90 - ETA: 1s - loss: 0.4392 - accuracy: 0.90 - ETA: 1s - loss: 0.4388 - accuracy: 0.90 - ETA: 1s - loss: 0.4383 - accuracy: 0.90 - ETA: 1s - loss: 0.4373 - accuracy: 0.90 - ETA: 1s - loss: 0.4364 - accuracy: 0.90 - ETA: 1s - loss: 0.4354 - accuracy: 0.90 - ETA: 1s - loss: 0.4354 - accuracy: 0.90 - ETA: 1s - loss: 0.4366 - accuracy: 0.90 - ETA: 1s - loss: 0.4354 - accuracy: 0.90 - ETA: 1s - loss: 0.4352 - accuracy: 0.90 - ETA: 1s - loss: 0.4351 - accuracy: 0.90 - ETA: 1s - loss: 0.4354 - accuracy: 0.90 - ETA: 1s - loss: 0.4350 - accuracy: 0.90 - ETA: 1s - loss: 0.4353 - accuracy: 0.90 - ETA: 1s - loss: 0.4339 - accuracy: 0.90 - ETA: 0s - loss: 0.4331 - accuracy: 0.90 - ETA: 0s - loss: 0.4327 - accuracy: 0.90 - ETA: 0s - loss: 0.4337 - accuracy: 0.90 - ETA: 0s - loss: 0.4338 - accuracy: 0.90 - ETA: 0s - loss: 0.4335 - accuracy: 0.90 - ETA: 0s - loss: 0.4333 - accuracy: 0.90 - ETA: 0s - loss: 0.4333 - accuracy: 0.90 - ETA: 0s - loss: 0.4333 - accuracy: 0.90 - ETA: 0s - loss: 0.4324 - accuracy: 0.90 - ETA: 0s - loss: 0.4313 - accuracy: 0.90 - ETA: 0s - loss: 0.4312 - accuracy: 0.90 - ETA: 0s - loss: 0.4314 - accuracy: 0.90 - ETA: 0s - loss: 0.4316 - accuracy: 0.90 - ETA: 0s - loss: 0.4313 - accuracy: 0.90 - ETA: 0s - loss: 0.4312 - accuracy: 0.90 - ETA: 0s - loss: 0.4311 - accuracy: 0.90 - ETA: 0s - loss: 0.4306 - accuracy: 0.90 - ETA: 0s - loss: 0.4309 - accuracy: 0.90 - ETA: 0s - loss: 0.4307 - accuracy: 0.90 - 3s 169us/step - loss: 0.4302 - accuracy: 0.9077 - val_loss: 0.5577 - val_accuracy: 0.8182\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.3990 - accuracy: 0.93 - ETA: 2s - loss: 0.4017 - accuracy: 0.92 - ETA: 2s - loss: 0.4012 - accuracy: 0.92 - ETA: 2s - loss: 0.4141 - accuracy: 0.91 - ETA: 2s - loss: 0.4112 - accuracy: 0.91 - ETA: 2s - loss: 0.4154 - accuracy: 0.91 - ETA: 2s - loss: 0.4154 - accuracy: 0.91 - ETA: 2s - loss: 0.4111 - accuracy: 0.91 - ETA: 2s - loss: 0.4094 - accuracy: 0.91 - ETA: 2s - loss: 0.4111 - accuracy: 0.91 - ETA: 2s - loss: 0.4124 - accuracy: 0.91 - ETA: 2s - loss: 0.4107 - accuracy: 0.91 - ETA: 1s - loss: 0.4108 - accuracy: 0.91 - ETA: 1s - loss: 0.4107 - accuracy: 0.91 - ETA: 1s - loss: 0.4101 - accuracy: 0.91 - ETA: 1s - loss: 0.4108 - accuracy: 0.91 - ETA: 1s - loss: 0.4108 - accuracy: 0.91 - ETA: 1s - loss: 0.4100 - accuracy: 0.91 - ETA: 1s - loss: 0.4109 - accuracy: 0.91 - ETA: 1s - loss: 0.4124 - accuracy: 0.91 - ETA: 1s - loss: 0.4129 - accuracy: 0.91 - ETA: 1s - loss: 0.4131 - accuracy: 0.91 - ETA: 1s - loss: 0.4136 - accuracy: 0.91 - ETA: 1s - loss: 0.4141 - accuracy: 0.91 - ETA: 1s - loss: 0.4131 - accuracy: 0.90 - ETA: 1s - loss: 0.4115 - accuracy: 0.90 - ETA: 1s - loss: 0.4105 - accuracy: 0.91 - ETA: 1s - loss: 0.4093 - accuracy: 0.91 - ETA: 1s - loss: 0.4096 - accuracy: 0.91 - ETA: 1s - loss: 0.4086 - accuracy: 0.91 - ETA: 1s - loss: 0.4091 - accuracy: 0.91 - ETA: 0s - loss: 0.4095 - accuracy: 0.91 - ETA: 0s - loss: 0.4103 - accuracy: 0.91 - ETA: 0s - loss: 0.4104 - accuracy: 0.91 - ETA: 0s - loss: 0.4099 - accuracy: 0.91 - ETA: 0s - loss: 0.4099 - accuracy: 0.91 - ETA: 0s - loss: 0.4099 - accuracy: 0.91 - ETA: 0s - loss: 0.4107 - accuracy: 0.90 - ETA: 0s - loss: 0.4102 - accuracy: 0.91 - ETA: 0s - loss: 0.4097 - accuracy: 0.91 - ETA: 0s - loss: 0.4095 - accuracy: 0.91 - ETA: 0s - loss: 0.4095 - accuracy: 0.91 - ETA: 0s - loss: 0.4095 - accuracy: 0.91 - ETA: 0s - loss: 0.4098 - accuracy: 0.91 - ETA: 0s - loss: 0.4090 - accuracy: 0.91 - ETA: 0s - loss: 0.4092 - accuracy: 0.91 - ETA: 0s - loss: 0.4091 - accuracy: 0.91 - ETA: 0s - loss: 0.4094 - accuracy: 0.91 - ETA: 0s - loss: 0.4092 - accuracy: 0.91 - 3s 174us/step - loss: 0.4087 - accuracy: 0.9130 - val_loss: 0.5433 - val_accuracy: 0.8230\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.3432 - accuracy: 0.96 - ETA: 2s - loss: 0.3762 - accuracy: 0.94 - ETA: 2s - loss: 0.3795 - accuracy: 0.92 - ETA: 2s - loss: 0.3851 - accuracy: 0.92 - ETA: 2s - loss: 0.3892 - accuracy: 0.91 - ETA: 2s - loss: 0.3915 - accuracy: 0.91 - ETA: 2s - loss: 0.3954 - accuracy: 0.91 - ETA: 2s - loss: 0.3974 - accuracy: 0.91 - ETA: 2s - loss: 0.3983 - accuracy: 0.91 - ETA: 2s - loss: 0.3979 - accuracy: 0.91 - ETA: 2s - loss: 0.3967 - accuracy: 0.91 - ETA: 2s - loss: 0.3950 - accuracy: 0.91 - ETA: 1s - loss: 0.3959 - accuracy: 0.91 - ETA: 1s - loss: 0.3926 - accuracy: 0.91 - ETA: 1s - loss: 0.3924 - accuracy: 0.91 - ETA: 1s - loss: 0.3919 - accuracy: 0.91 - ETA: 1s - loss: 0.3922 - accuracy: 0.91 - ETA: 1s - loss: 0.3913 - accuracy: 0.91 - ETA: 1s - loss: 0.3927 - accuracy: 0.91 - ETA: 1s - loss: 0.3928 - accuracy: 0.91 - ETA: 1s - loss: 0.3924 - accuracy: 0.91 - ETA: 1s - loss: 0.3922 - accuracy: 0.91 - ETA: 1s - loss: 0.3921 - accuracy: 0.91 - ETA: 1s - loss: 0.3923 - accuracy: 0.91 - ETA: 1s - loss: 0.3919 - accuracy: 0.91 - ETA: 1s - loss: 0.3924 - accuracy: 0.91 - ETA: 1s - loss: 0.3917 - accuracy: 0.91 - ETA: 1s - loss: 0.3914 - accuracy: 0.91 - ETA: 1s - loss: 0.3913 - accuracy: 0.91 - ETA: 1s - loss: 0.3911 - accuracy: 0.91 - ETA: 1s - loss: 0.3909 - accuracy: 0.91 - ETA: 0s - loss: 0.3903 - accuracy: 0.91 - ETA: 0s - loss: 0.3898 - accuracy: 0.91 - ETA: 0s - loss: 0.3900 - accuracy: 0.91 - ETA: 0s - loss: 0.3901 - accuracy: 0.91 - ETA: 0s - loss: 0.3900 - accuracy: 0.91 - ETA: 0s - loss: 0.3905 - accuracy: 0.91 - ETA: 0s - loss: 0.3913 - accuracy: 0.91 - ETA: 0s - loss: 0.3909 - accuracy: 0.91 - ETA: 0s - loss: 0.3907 - accuracy: 0.91 - ETA: 0s - loss: 0.3903 - accuracy: 0.91 - ETA: 0s - loss: 0.3898 - accuracy: 0.91 - ETA: 0s - loss: 0.3903 - accuracy: 0.91 - ETA: 0s - loss: 0.3897 - accuracy: 0.91 - ETA: 0s - loss: 0.3899 - accuracy: 0.91 - ETA: 0s - loss: 0.3895 - accuracy: 0.91 - ETA: 0s - loss: 0.3891 - accuracy: 0.91 - ETA: 0s - loss: 0.3889 - accuracy: 0.91 - ETA: 0s - loss: 0.3888 - accuracy: 0.91 - 3s 173us/step - loss: 0.3888 - accuracy: 0.9180 - val_loss: 0.5301 - val_accuracy: 0.8274\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.3685 - accuracy: 0.93 - ETA: 2s - loss: 0.3767 - accuracy: 0.93 - ETA: 2s - loss: 0.3819 - accuracy: 0.92 - ETA: 2s - loss: 0.3804 - accuracy: 0.92 - ETA: 2s - loss: 0.3797 - accuracy: 0.92 - ETA: 2s - loss: 0.3830 - accuracy: 0.92 - ETA: 2s - loss: 0.3858 - accuracy: 0.92 - ETA: 2s - loss: 0.3811 - accuracy: 0.92 - ETA: 2s - loss: 0.3795 - accuracy: 0.92 - ETA: 2s - loss: 0.3793 - accuracy: 0.92 - ETA: 2s - loss: 0.3782 - accuracy: 0.92 - ETA: 2s - loss: 0.3768 - accuracy: 0.92 - ETA: 2s - loss: 0.3780 - accuracy: 0.92 - ETA: 1s - loss: 0.3770 - accuracy: 0.92 - ETA: 1s - loss: 0.3780 - accuracy: 0.92 - ETA: 1s - loss: 0.3778 - accuracy: 0.92 - ETA: 1s - loss: 0.3770 - accuracy: 0.92 - ETA: 1s - loss: 0.3762 - accuracy: 0.92 - ETA: 1s - loss: 0.3759 - accuracy: 0.92 - ETA: 1s - loss: 0.3766 - accuracy: 0.92 - ETA: 1s - loss: 0.3776 - accuracy: 0.92 - ETA: 1s - loss: 0.3776 - accuracy: 0.92 - ETA: 1s - loss: 0.3771 - accuracy: 0.92 - ETA: 1s - loss: 0.3769 - accuracy: 0.92 - ETA: 1s - loss: 0.3769 - accuracy: 0.92 - ETA: 1s - loss: 0.3764 - accuracy: 0.92 - ETA: 1s - loss: 0.3759 - accuracy: 0.92 - ETA: 1s - loss: 0.3757 - accuracy: 0.92 - ETA: 1s - loss: 0.3752 - accuracy: 0.92 - ETA: 1s - loss: 0.3754 - accuracy: 0.92 - ETA: 1s - loss: 0.3750 - accuracy: 0.92 - ETA: 0s - loss: 0.3740 - accuracy: 0.92 - ETA: 0s - loss: 0.3732 - accuracy: 0.92 - ETA: 0s - loss: 0.3734 - accuracy: 0.92 - ETA: 0s - loss: 0.3736 - accuracy: 0.92 - ETA: 0s - loss: 0.3725 - accuracy: 0.92 - ETA: 0s - loss: 0.3731 - accuracy: 0.92 - ETA: 0s - loss: 0.3732 - accuracy: 0.92 - ETA: 0s - loss: 0.3731 - accuracy: 0.92 - ETA: 0s - loss: 0.3731 - accuracy: 0.92 - ETA: 0s - loss: 0.3721 - accuracy: 0.92 - ETA: 0s - loss: 0.3715 - accuracy: 0.92 - ETA: 0s - loss: 0.3712 - accuracy: 0.92 - ETA: 0s - loss: 0.3704 - accuracy: 0.92 - ETA: 0s - loss: 0.3705 - accuracy: 0.92 - ETA: 0s - loss: 0.3703 - accuracy: 0.92 - ETA: 0s - loss: 0.3701 - accuracy: 0.92 - ETA: 0s - loss: 0.3698 - accuracy: 0.92 - 3s 176us/step - loss: 0.3699 - accuracy: 0.9242 - val_loss: 0.5182 - val_accuracy: 0.8261\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - ETA: 2s - loss: 0.3701 - accuracy: 0.91 - ETA: 2s - loss: 0.3496 - accuracy: 0.92 - ETA: 2s - loss: 0.3392 - accuracy: 0.93 - ETA: 2s - loss: 0.3478 - accuracy: 0.92 - ETA: 2s - loss: 0.3476 - accuracy: 0.92 - ETA: 2s - loss: 0.3515 - accuracy: 0.92 - ETA: 2s - loss: 0.3572 - accuracy: 0.92 - ETA: 2s - loss: 0.3553 - accuracy: 0.92 - ETA: 2s - loss: 0.3566 - accuracy: 0.92 - ETA: 2s - loss: 0.3596 - accuracy: 0.92 - ETA: 2s - loss: 0.3584 - accuracy: 0.92 - ETA: 2s - loss: 0.3606 - accuracy: 0.92 - ETA: 2s - loss: 0.3601 - accuracy: 0.92 - ETA: 2s - loss: 0.3603 - accuracy: 0.92 - ETA: 1s - loss: 0.3596 - accuracy: 0.92 - ETA: 1s - loss: 0.3617 - accuracy: 0.92 - ETA: 1s - loss: 0.3600 - accuracy: 0.92 - ETA: 1s - loss: 0.3588 - accuracy: 0.92 - ETA: 1s - loss: 0.3585 - accuracy: 0.92 - ETA: 1s - loss: 0.3579 - accuracy: 0.92 - ETA: 1s - loss: 0.3583 - accuracy: 0.92 - ETA: 1s - loss: 0.3589 - accuracy: 0.92 - ETA: 1s - loss: 0.3583 - accuracy: 0.92 - ETA: 1s - loss: 0.3574 - accuracy: 0.92 - ETA: 1s - loss: 0.3581 - accuracy: 0.92 - ETA: 1s - loss: 0.3583 - accuracy: 0.92 - ETA: 1s - loss: 0.3565 - accuracy: 0.92 - ETA: 1s - loss: 0.3566 - accuracy: 0.92 - ETA: 1s - loss: 0.3567 - accuracy: 0.92 - ETA: 1s - loss: 0.3558 - accuracy: 0.92 - ETA: 1s - loss: 0.3560 - accuracy: 0.92 - ETA: 1s - loss: 0.3564 - accuracy: 0.92 - ETA: 0s - loss: 0.3553 - accuracy: 0.92 - ETA: 0s - loss: 0.3552 - accuracy: 0.92 - ETA: 0s - loss: 0.3552 - accuracy: 0.92 - ETA: 0s - loss: 0.3555 - accuracy: 0.92 - ETA: 0s - loss: 0.3560 - accuracy: 0.92 - ETA: 0s - loss: 0.3555 - accuracy: 0.92 - ETA: 0s - loss: 0.3549 - accuracy: 0.92 - ETA: 0s - loss: 0.3545 - accuracy: 0.92 - ETA: 0s - loss: 0.3539 - accuracy: 0.92 - ETA: 0s - loss: 0.3538 - accuracy: 0.92 - ETA: 0s - loss: 0.3539 - accuracy: 0.92 - ETA: 0s - loss: 0.3540 - accuracy: 0.92 - ETA: 0s - loss: 0.3541 - accuracy: 0.92 - ETA: 0s - loss: 0.3538 - accuracy: 0.92 - ETA: 0s - loss: 0.3536 - accuracy: 0.92 - ETA: 0s - loss: 0.3529 - accuracy: 0.92 - ETA: 0s - loss: 0.3526 - accuracy: 0.92 - 3s 184us/step - loss: 0.3524 - accuracy: 0.9293 - val_loss: 0.5085 - val_accuracy: 0.8233\n",
      "Epoch 25/25\n",
      "15663/15663 [==============================] - ETA: 2s - loss: 0.3368 - accuracy: 0.93 - ETA: 2s - loss: 0.3351 - accuracy: 0.92 - ETA: 2s - loss: 0.3485 - accuracy: 0.92 - ETA: 2s - loss: 0.3442 - accuracy: 0.92 - ETA: 2s - loss: 0.3457 - accuracy: 0.92 - ETA: 2s - loss: 0.3461 - accuracy: 0.92 - ETA: 2s - loss: 0.3438 - accuracy: 0.92 - ETA: 2s - loss: 0.3408 - accuracy: 0.93 - ETA: 2s - loss: 0.3410 - accuracy: 0.93 - ETA: 2s - loss: 0.3394 - accuracy: 0.93 - ETA: 2s - loss: 0.3361 - accuracy: 0.93 - ETA: 2s - loss: 0.3364 - accuracy: 0.93 - ETA: 1s - loss: 0.3372 - accuracy: 0.93 - ETA: 1s - loss: 0.3393 - accuracy: 0.93 - ETA: 1s - loss: 0.3386 - accuracy: 0.93 - ETA: 1s - loss: 0.3393 - accuracy: 0.93 - ETA: 1s - loss: 0.3395 - accuracy: 0.93 - ETA: 1s - loss: 0.3388 - accuracy: 0.93 - ETA: 1s - loss: 0.3401 - accuracy: 0.93 - ETA: 1s - loss: 0.3417 - accuracy: 0.93 - ETA: 1s - loss: 0.3413 - accuracy: 0.93 - ETA: 1s - loss: 0.3393 - accuracy: 0.93 - ETA: 1s - loss: 0.3396 - accuracy: 0.93 - ETA: 1s - loss: 0.3403 - accuracy: 0.93 - ETA: 1s - loss: 0.3405 - accuracy: 0.93 - ETA: 1s - loss: 0.3398 - accuracy: 0.93 - ETA: 1s - loss: 0.3397 - accuracy: 0.93 - ETA: 1s - loss: 0.3399 - accuracy: 0.93 - ETA: 1s - loss: 0.3407 - accuracy: 0.93 - ETA: 1s - loss: 0.3407 - accuracy: 0.93 - ETA: 0s - loss: 0.3403 - accuracy: 0.93 - ETA: 0s - loss: 0.3402 - accuracy: 0.93 - ETA: 0s - loss: 0.3398 - accuracy: 0.93 - ETA: 0s - loss: 0.3396 - accuracy: 0.93 - ETA: 0s - loss: 0.3392 - accuracy: 0.93 - ETA: 0s - loss: 0.3392 - accuracy: 0.93 - ETA: 0s - loss: 0.3385 - accuracy: 0.93 - ETA: 0s - loss: 0.3387 - accuracy: 0.93 - ETA: 0s - loss: 0.3386 - accuracy: 0.93 - ETA: 0s - loss: 0.3385 - accuracy: 0.93 - ETA: 0s - loss: 0.3383 - accuracy: 0.93 - ETA: 0s - loss: 0.3376 - accuracy: 0.93 - ETA: 0s - loss: 0.3374 - accuracy: 0.93 - ETA: 0s - loss: 0.3370 - accuracy: 0.93 - ETA: 0s - loss: 0.3365 - accuracy: 0.93 - ETA: 0s - loss: 0.3363 - accuracy: 0.93 - ETA: 0s - loss: 0.3362 - accuracy: 0.93 - ETA: 0s - loss: 0.3359 - accuracy: 0.93 - 3s 172us/step - loss: 0.3358 - accuracy: 0.9348 - val_loss: 0.4972 - val_accuracy: 0.8287\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=160,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Define Keras Model</3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=14, activation='relu')) #Model expects rows of data with 66 variables #The first hidden layer has 12 nodes and uses the relu activation function\n",
    "model.add(Dense(8, activation='relu')) #The seond hidden layer has 8 nodes and use relu activation function\n",
    "model.add(Dense(1, activation='sigmoid')) #The output layer has one node and use the sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Compile Keras Model</3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Fit Keras Model<h/3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/500\n",
      "3200/3200 [==============================] - ETA: 19s - loss: 0.7205 - accuracy: 0.500 - ETA: 0s - loss: 0.6236 - accuracy: 0.284 - ETA: 0s - loss: 0.5729 - accuracy: 0.28 - ETA: 0s - loss: 0.5387 - accuracy: 0.29 - 0s 96us/step - loss: 0.5299 - accuracy: 0.2859 - val_loss: 0.4118 - val_accuracy: 0.2738\n",
      "Epoch 2/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.40 - ETA: 0s - loss: 0.4532 - accuracy: 0.28 - ETA: 0s - loss: 0.4084 - accuracy: 0.28 - ETA: 0s - loss: 0.3963 - accuracy: 0.28 - ETA: 0s - loss: 0.3795 - accuracy: 0.28 - 0s 76us/step - loss: 0.3793 - accuracy: 0.2800 - val_loss: 0.3262 - val_accuracy: 0.2738\n",
      "Epoch 3/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.30 - ETA: 0s - loss: 0.3511 - accuracy: 0.26 - ETA: 0s - loss: 0.3213 - accuracy: 0.27 - ETA: 0s - loss: 0.3367 - accuracy: 0.28 - 0s 74us/step - loss: 0.3332 - accuracy: 0.2800 - val_loss: 0.2959 - val_accuracy: 0.2738\n",
      "Epoch 4/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1905 - accuracy: 0.200 - ETA: 0s - loss: 0.3201 - accuracy: 0.266 - ETA: 0s - loss: 0.2709 - accuracy: 0.27 - ETA: 0s - loss: 0.3053 - accuracy: 0.27 - 0s 71us/step - loss: 0.3175 - accuracy: 0.2800 - val_loss: 0.2844 - val_accuracy: 0.2738\n",
      "Epoch 5/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1191 - accuracy: 0.30 - ETA: 0s - loss: 0.3043 - accuracy: 0.27 - ETA: 0s - loss: 0.3594 - accuracy: 0.26 - ETA: 0s - loss: 0.2933 - accuracy: 0.27 - ETA: 0s - loss: 0.3024 - accuracy: 0.27 - 0s 82us/step - loss: 0.3122 - accuracy: 0.2800 - val_loss: 0.2792 - val_accuracy: 0.2738\n",
      "Epoch 6/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7479 - accuracy: 0.50 - ETA: 0s - loss: 0.3257 - accuracy: 0.27 - ETA: 0s - loss: 0.3070 - accuracy: 0.28 - ETA: 0s - loss: 0.3331 - accuracy: 0.28 - 0s 74us/step - loss: 0.3104 - accuracy: 0.2800 - val_loss: 0.2768 - val_accuracy: 0.2738\n",
      "Epoch 7/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5721 - accuracy: 0.300 - ETA: 0s - loss: 0.3923 - accuracy: 0.276 - ETA: 0s - loss: 0.3363 - accuracy: 0.27 - ETA: 0s - loss: 0.3149 - accuracy: 0.27 - 0s 72us/step - loss: 0.3093 - accuracy: 0.2800 - val_loss: 0.2754 - val_accuracy: 0.2738\n",
      "Epoch 8/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.40 - ETA: 0s - loss: 0.2481 - accuracy: 0.26 - ETA: 0s - loss: 0.2576 - accuracy: 0.27 - ETA: 0s - loss: 0.2888 - accuracy: 0.27 - 0s 72us/step - loss: 0.3091 - accuracy: 0.2800 - val_loss: 0.2753 - val_accuracy: 0.2738\n",
      "Epoch 9/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.40 - ETA: 0s - loss: 0.3261 - accuracy: 0.30 - ETA: 0s - loss: 0.3356 - accuracy: 0.28 - ETA: 0s - loss: 0.3196 - accuracy: 0.27 - 0s 73us/step - loss: 0.3086 - accuracy: 0.2800 - val_loss: 0.2744 - val_accuracy: 0.2738\n",
      "Epoch 10/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.20 - ETA: 0s - loss: 0.2744 - accuracy: 0.26 - ETA: 0s - loss: 0.2915 - accuracy: 0.27 - ETA: 0s - loss: 0.2993 - accuracy: 0.27 - 0s 72us/step - loss: 0.3080 - accuracy: 0.2800 - val_loss: 0.2741 - val_accuracy: 0.2738\n",
      "Epoch 11/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.10 - ETA: 0s - loss: 0.2976 - accuracy: 0.29 - ETA: 0s - loss: 0.3472 - accuracy: 0.27 - ETA: 0s - loss: 0.3203 - accuracy: 0.27 - ETA: 0s - loss: 0.3067 - accuracy: 0.28 - 0s 75us/step - loss: 0.3076 - accuracy: 0.2800 - val_loss: 0.2733 - val_accuracy: 0.2738\n",
      "Epoch 12/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9723 - accuracy: 0.20 - ETA: 0s - loss: 0.3757 - accuracy: 0.27 - ETA: 0s - loss: 0.3570 - accuracy: 0.27 - ETA: 0s - loss: 0.3199 - accuracy: 0.28 - 0s 71us/step - loss: 0.3070 - accuracy: 0.2800 - val_loss: 0.2725 - val_accuracy: 0.2738\n",
      "Epoch 13/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8379 - accuracy: 0.30 - ETA: 0s - loss: 0.3522 - accuracy: 0.25 - ETA: 0s - loss: 0.3161 - accuracy: 0.27 - ETA: 0s - loss: 0.3009 - accuracy: 0.27 - ETA: 0s - loss: 0.3055 - accuracy: 0.27 - 0s 78us/step - loss: 0.3065 - accuracy: 0.2800 - val_loss: 0.2719 - val_accuracy: 0.2738\n",
      "Epoch 14/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1473 - accuracy: 0.300 - ETA: 0s - loss: 0.3193 - accuracy: 0.279 - ETA: 0s - loss: 0.2945 - accuracy: 0.26 - ETA: 0s - loss: 0.3027 - accuracy: 0.27 - ETA: 0s - loss: 0.3062 - accuracy: 0.28 - 0s 79us/step - loss: 0.3059 - accuracy: 0.2800 - val_loss: 0.2714 - val_accuracy: 0.2738\n",
      "Epoch 15/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1417 - accuracy: 0.300 - ETA: 0s - loss: 0.3888 - accuracy: 0.269 - ETA: 0s - loss: 0.3810 - accuracy: 0.27 - ETA: 0s - loss: 0.3317 - accuracy: 0.27 - 0s 75us/step - loss: 0.3046 - accuracy: 0.2800 - val_loss: 0.2701 - val_accuracy: 0.2738\n",
      "Epoch 16/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4449 - accuracy: 0.400 - ETA: 0s - loss: 0.3199 - accuracy: 0.278 - ETA: 0s - loss: 0.3282 - accuracy: 0.27 - ETA: 0s - loss: 0.2850 - accuracy: 0.28 - 0s 74us/step - loss: 0.3044 - accuracy: 0.2800 - val_loss: 0.2697 - val_accuracy: 0.2738\n",
      "Epoch 17/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.20 - ETA: 0s - loss: 0.3049 - accuracy: 0.26 - ETA: 0s - loss: 0.2973 - accuracy: 0.29 - ETA: 0s - loss: 0.3074 - accuracy: 0.28 - 0s 71us/step - loss: 0.3035 - accuracy: 0.2800 - val_loss: 0.2691 - val_accuracy: 0.2738\n",
      "Epoch 18/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.20 - ETA: 0s - loss: 0.2700 - accuracy: 0.28 - ETA: 0s - loss: 0.2729 - accuracy: 0.28 - ETA: 0s - loss: 0.2726 - accuracy: 0.28 - 0s 74us/step - loss: 0.3020 - accuracy: 0.2800 - val_loss: 0.2690 - val_accuracy: 0.2738\n",
      "Epoch 19/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7582 - accuracy: 0.10 - ETA: 0s - loss: 0.2634 - accuracy: 0.26 - ETA: 0s - loss: 0.2914 - accuracy: 0.27 - ETA: 0s - loss: 0.2888 - accuracy: 0.27 - 0s 71us/step - loss: 0.3014 - accuracy: 0.2800 - val_loss: 0.2679 - val_accuracy: 0.2738\n",
      "Epoch 20/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8022 - accuracy: 0.400 - ETA: 0s - loss: 0.2614 - accuracy: 0.269 - ETA: 0s - loss: 0.2689 - accuracy: 0.28 - ETA: 0s - loss: 0.2943 - accuracy: 0.28 - 0s 74us/step - loss: 0.2998 - accuracy: 0.2800 - val_loss: 0.2668 - val_accuracy: 0.2738\n",
      "Epoch 21/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8254 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.2532 - accuracy: 0.2950    - ETA: 0s - loss: 0.2731 - accuracy: 0.29 - ETA: 0s - loss: 0.3034 - accuracy: 0.28 - ETA: 0s - loss: 0.2993 - accuracy: 0.27 - 0s 76us/step - loss: 0.2986 - accuracy: 0.2800 - val_loss: 0.2649 - val_accuracy: 0.2738\n",
      "Epoch 22/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7090 - accuracy: 0.50 - ETA: 0s - loss: 0.3658 - accuracy: 0.25 - ETA: 0s - loss: 0.3332 - accuracy: 0.27 - ETA: 0s - loss: 0.3010 - accuracy: 0.27 - ETA: 0s - loss: 0.3057 - accuracy: 0.28 - 0s 79us/step - loss: 0.2967 - accuracy: 0.2800 - val_loss: 0.2621 - val_accuracy: 0.2738\n",
      "Epoch 23/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.1036 - accuracy: 0.300 - ETA: 0s - loss: 0.2140 - accuracy: 0.280 - ETA: 0s - loss: 0.2602 - accuracy: 0.28 - ETA: 0s - loss: 0.2684 - accuracy: 0.28 - 0s 72us/step - loss: 0.2943 - accuracy: 0.2800 - val_loss: 0.2633 - val_accuracy: 0.2738\n",
      "Epoch 24/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1743 - accuracy: 0.30 - ETA: 0s - loss: 0.1941 - accuracy: 0.25 - ETA: 0s - loss: 0.2678 - accuracy: 0.26 - ETA: 0s - loss: 0.2787 - accuracy: 0.27 - ETA: 0s - loss: 0.2961 - accuracy: 0.27 - 0s 77us/step - loss: 0.2925 - accuracy: 0.2800 - val_loss: 0.2605 - val_accuracy: 0.2738\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6451 - accuracy: 0.300 - ETA: 0s - loss: 0.2250 - accuracy: 0.301 - ETA: 0s - loss: 0.2482 - accuracy: 0.28 - ETA: 0s - loss: 0.2900 - accuracy: 0.28 - ETA: 0s - loss: 0.2895 - accuracy: 0.27 - 0s 77us/step - loss: 0.2897 - accuracy: 0.2800 - val_loss: 0.2580 - val_accuracy: 0.2738\n",
      "Epoch 26/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9954 - accuracy: 0.40 - ETA: 0s - loss: 0.3588 - accuracy: 0.28 - ETA: 0s - loss: 0.3517 - accuracy: 0.28 - ETA: 0s - loss: 0.3096 - accuracy: 0.29 - 0s 74us/step - loss: 0.2856 - accuracy: 0.2800 - val_loss: 0.2518 - val_accuracy: 0.2738\n",
      "Epoch 27/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0946 - accuracy: 0.200 - ETA: 0s - loss: 0.2121 - accuracy: 0.297 - ETA: 0s - loss: 0.2645 - accuracy: 0.28 - ETA: 0s - loss: 0.2689 - accuracy: 0.28 - ETA: 0s - loss: 0.2806 - accuracy: 0.28 - 0s 77us/step - loss: 0.2812 - accuracy: 0.2800 - val_loss: 0.2494 - val_accuracy: 0.2738\n",
      "Epoch 28/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6489 - accuracy: 0.100 - ETA: 0s - loss: 0.2165 - accuracy: 0.289 - ETA: 0s - loss: 0.2657 - accuracy: 0.28 - ETA: 0s - loss: 0.2928 - accuracy: 0.28 - ETA: 0s - loss: 0.2737 - accuracy: 0.27 - 0s 76us/step - loss: 0.2765 - accuracy: 0.2800 - val_loss: 0.2415 - val_accuracy: 0.2738\n",
      "Epoch 29/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5064 - accuracy: 0.400 - ETA: 0s - loss: 0.2201 - accuracy: 0.281 - ETA: 0s - loss: 0.2477 - accuracy: 0.29 - ETA: 0s - loss: 0.2466 - accuracy: 0.28 - 0s 74us/step - loss: 0.2705 - accuracy: 0.2800 - val_loss: 0.2434 - val_accuracy: 0.2738\n",
      "Epoch 30/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.40 - ETA: 0s - loss: 0.3376 - accuracy: 0.28 - ETA: 0s - loss: 0.3125 - accuracy: 0.27 - ETA: 0s - loss: 0.2532 - accuracy: 0.27 - ETA: 0s - loss: 0.2504 - accuracy: 0.28 - 0s 78us/step - loss: 0.2657 - accuracy: 0.2800 - val_loss: 0.2355 - val_accuracy: 0.2738\n",
      "Epoch 31/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.10 - ETA: 0s - loss: 0.3258 - accuracy: 0.26 - ETA: 0s - loss: 0.3236 - accuracy: 0.27 - ETA: 0s - loss: 0.2462 - accuracy: 0.27 - 0s 74us/step - loss: 0.2568 - accuracy: 0.2800 - val_loss: 0.2272 - val_accuracy: 0.2738\n",
      "Epoch 32/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6096 - accuracy: 0.30 - ETA: 0s - loss: 0.1451 - accuracy: 0.25 - ETA: 0s - loss: 0.2432 - accuracy: 0.27 - ETA: 0s - loss: 0.2522 - accuracy: 0.27 - 0s 74us/step - loss: 0.2533 - accuracy: 0.2800 - val_loss: 0.2220 - val_accuracy: 0.2738\n",
      "Epoch 33/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.20 - ETA: 0s - loss: 0.2494 - accuracy: 0.28 - ETA: 0s - loss: 0.2528 - accuracy: 0.27 - ETA: 0s - loss: 0.2557 - accuracy: 0.28 - ETA: 0s - loss: 0.2387 - accuracy: 0.28 - 0s 81us/step - loss: 0.2490 - accuracy: 0.2800 - val_loss: 0.2175 - val_accuracy: 0.2738\n",
      "Epoch 34/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.30 - ETA: 0s - loss: 0.2854 - accuracy: 0.29 - ETA: 0s - loss: 0.2546 - accuracy: 0.28 - ETA: 0s - loss: 0.2218 - accuracy: 0.27 - ETA: 0s - loss: 0.2421 - accuracy: 0.27 - 0s 81us/step - loss: 0.2428 - accuracy: 0.2800 - val_loss: 0.2107 - val_accuracy: 0.2738\n",
      "Epoch 35/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3988 - accuracy: 0.200 - ETA: 0s - loss: 0.3346 - accuracy: 0.289 - ETA: 0s - loss: 0.1977 - accuracy: 0.28 - ETA: 0s - loss: 0.2128 - accuracy: 0.27 - ETA: 0s - loss: 0.2082 - accuracy: 0.28 - 0s 82us/step - loss: 0.2324 - accuracy: 0.2800 - val_loss: 0.2208 - val_accuracy: 0.2738\n",
      "Epoch 36/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.40 - ETA: 0s - loss: 0.2772 - accuracy: 0.26 - ETA: 0s - loss: 0.2294 - accuracy: 0.26 - ETA: 0s - loss: 0.2572 - accuracy: 0.27 - ETA: 0s - loss: 0.2410 - accuracy: 0.28 - 0s 76us/step - loss: 0.2314 - accuracy: 0.2800 - val_loss: 0.1917 - val_accuracy: 0.2738\n",
      "Epoch 37/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0510 - accuracy: 0.400 - ETA: 0s - loss: 0.3031 - accuracy: 0.297 - ETA: 0s - loss: 0.2849 - accuracy: 0.27 - ETA: 0s - loss: 0.2410 - accuracy: 0.28 - ETA: 0s - loss: 0.2201 - accuracy: 0.27 - 0s 77us/step - loss: 0.2250 - accuracy: 0.2800 - val_loss: 0.1974 - val_accuracy: 0.2738\n",
      "Epoch 38/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7041 - accuracy: 0.30 - ETA: 0s - loss: 0.1144 - accuracy: 0.27 - ETA: 0s - loss: 0.1435 - accuracy: 0.28 - ETA: 0s - loss: 0.2108 - accuracy: 0.28 - ETA: 0s - loss: 0.2077 - accuracy: 0.28 - 0s 82us/step - loss: 0.2126 - accuracy: 0.2800 - val_loss: 0.1772 - val_accuracy: 0.2738\n",
      "Epoch 39/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4475 - accuracy: 0.100 - ETA: 0s - loss: 0.2414 - accuracy: 0.273 - ETA: 0s - loss: 0.2355 - accuracy: 0.26 - ETA: 0s - loss: 0.2416 - accuracy: 0.27 - ETA: 0s - loss: 0.1878 - accuracy: 0.28 - 0s 78us/step - loss: 0.2010 - accuracy: 0.2800 - val_loss: 0.1740 - val_accuracy: 0.2738\n",
      "Epoch 40/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.40 - ETA: 0s - loss: 0.1898 - accuracy: 0.28 - ETA: 0s - loss: 0.2108 - accuracy: 0.27 - ETA: 0s - loss: 0.2280 - accuracy: 0.27 - ETA: 0s - loss: 0.2032 - accuracy: 0.27 - 0s 80us/step - loss: 0.1986 - accuracy: 0.2800 - val_loss: 0.1626 - val_accuracy: 0.2738\n",
      "Epoch 41/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5132 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.1937 - accuracy: 0.2675    - ETA: 0s - loss: 0.2683 - accuracy: 0.27 - ETA: 0s - loss: 0.2279 - accuracy: 0.28 - 0s 71us/step - loss: 0.1903 - accuracy: 0.2800 - val_loss: 0.1541 - val_accuracy: 0.2738\n",
      "Epoch 42/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.3624 - accuracy: 0.400 - ETA: 0s - loss: 0.2726 - accuracy: 0.270 - ETA: 0s - loss: 0.2390 - accuracy: 0.27 - ETA: 0s - loss: 0.1751 - accuracy: 0.27 - ETA: 0s - loss: 0.1738 - accuracy: 0.27 - 0s 76us/step - loss: 0.1761 - accuracy: 0.2800 - val_loss: 0.1534 - val_accuracy: 0.2738\n",
      "Epoch 43/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1795 - accuracy: 0.300 - ETA: 0s - loss: 0.2113 - accuracy: 0.276 - ETA: 0s - loss: 0.2864 - accuracy: 0.28 - ETA: 0s - loss: 0.2440 - accuracy: 0.27 - ETA: 0s - loss: 0.1673 - accuracy: 0.27 - 0s 77us/step - loss: 0.1696 - accuracy: 0.2800 - val_loss: 0.1399 - val_accuracy: 0.2738\n",
      "Epoch 44/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.40 - ETA: 0s - loss: 0.1341 - accuracy: 0.27 - ETA: 0s - loss: 0.1555 - accuracy: 0.28 - ETA: 0s - loss: 0.2097 - accuracy: 0.28 - 0s 74us/step - loss: 0.1739 - accuracy: 0.2800 - val_loss: 0.1288 - val_accuracy: 0.2738\n",
      "Epoch 45/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4721 - accuracy: 0.30 - ETA: 0s - loss: 0.0709 - accuracy: 0.30 - ETA: 0s - loss: 0.1434 - accuracy: 0.28 - ETA: 0s - loss: 0.1698 - accuracy: 0.28 - 0s 75us/step - loss: 0.1570 - accuracy: 0.2800 - val_loss: 0.1177 - val_accuracy: 0.2738\n",
      "Epoch 46/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4160 - accuracy: 0.20 - ETA: 0s - loss: 0.1881 - accuracy: 0.27 - ETA: 0s - loss: 0.1425 - accuracy: 0.28 - ETA: 0s - loss: 0.1226 - accuracy: 0.27 - ETA: 0s - loss: 0.1509 - accuracy: 0.28 - 0s 76us/step - loss: 0.1541 - accuracy: 0.2800 - val_loss: 0.1179 - val_accuracy: 0.2738\n",
      "Epoch 47/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6830 - accuracy: 0.100 - ETA: 0s - loss: 0.2002 - accuracy: 0.287 - ETA: 0s - loss: 0.2928 - accuracy: 0.26 - ETA: 0s - loss: 0.1631 - accuracy: 0.26 - 0s 75us/step - loss: 0.1291 - accuracy: 0.2800 - val_loss: 0.1247 - val_accuracy: 0.2738\n",
      "Epoch 48/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.1349 - accuracy: 0.200 - ETA: 0s - loss: 0.2313 - accuracy: 0.293 - ETA: 0s - loss: 0.1941 - accuracy: 0.28 - ETA: 0s - loss: 0.1645 - accuracy: 0.27 - 0s 74us/step - loss: 0.1153 - accuracy: 0.2800 - val_loss: 0.0880 - val_accuracy: 0.2738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.20 - ETA: 0s - loss: 0.1457 - accuracy: 0.29 - ETA: 0s - loss: 0.1097 - accuracy: 0.27 - ETA: 0s - loss: 0.0765 - accuracy: 0.28 - 0s 73us/step - loss: 0.1072 - accuracy: 0.2800 - val_loss: 0.1334 - val_accuracy: 0.2738\n",
      "Epoch 50/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.4555 - accuracy: 0.300 - ETA: 0s - loss: -0.0406 - accuracy: 0.286 - ETA: 0s - loss: 0.0498 - accuracy: 0.282 - ETA: 0s - loss: 0.0729 - accuracy: 0.28 - ETA: 0s - loss: 0.0989 - accuracy: 0.27 - 0s 76us/step - loss: 0.0896 - accuracy: 0.2800 - val_loss: 0.0538 - val_accuracy: 0.2738\n",
      "Epoch 51/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 3.4113 - accuracy: 0.20 - ETA: 0s - loss: 0.1541 - accuracy: 0.28 - ETA: 0s - loss: -0.0157 - accuracy: 0.288 - ETA: 0s - loss: 0.0180 - accuracy: 0.281 - 0s 72us/step - loss: 0.0727 - accuracy: 0.2800 - val_loss: 0.0538 - val_accuracy: 0.2738\n",
      "Epoch 52/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6109 - accuracy: 0.30 - ETA: 0s - loss: 0.3663 - accuracy: 0.28 - ETA: 0s - loss: 0.1001 - accuracy: 0.28 - ETA: 0s - loss: 0.0377 - accuracy: 0.28 - 0s 70us/step - loss: 0.0229 - accuracy: 0.2800 - val_loss: 0.0153 - val_accuracy: 0.2738\n",
      "Epoch 53/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.9236 - accuracy: 0.40 - ETA: 0s - loss: -0.0221 - accuracy: 0.274 - ETA: 0s - loss: -0.0284 - accuracy: 0.275 - ETA: 0s - loss: -0.0686 - accuracy: 0.277 - 0s 74us/step - loss: -0.0257 - accuracy: 0.2800 - val_loss: 0.1304 - val_accuracy: 0.2738\n",
      "Epoch 54/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.9372 - accuracy: 0.300 - ETA: 0s - loss: 0.1395 - accuracy: 0.272 - ETA: 0s - loss: -0.0625 - accuracy: 0.283 - ETA: 0s - loss: -0.0592 - accuracy: 0.281 - 0s 74us/step - loss: -0.0736 - accuracy: 0.2800 - val_loss: -0.0549 - val_accuracy: 0.2738\n",
      "Epoch 55/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1687 - accuracy: 0.400 - ETA: 0s - loss: 0.2317 - accuracy: 0.283 - ETA: 0s - loss: -0.1747 - accuracy: 0.272 - ETA: 0s - loss: -0.1638 - accuracy: 0.274 - 0s 75us/step - loss: -0.0957 - accuracy: 0.2800 - val_loss: -0.0540 - val_accuracy: 0.2738\n",
      "Epoch 56/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.1345 - accuracy: 0.20 - ETA: 0s - loss: 0.0411 - accuracy: 0.25 - ETA: 0s - loss: 0.1217 - accuracy: 0.27 - ETA: 0s - loss: -0.2505 - accuracy: 0.273 - ETA: 0s - loss: -0.2066 - accuracy: 0.279 - 0s 81us/step - loss: -0.1790 - accuracy: 0.2800 - val_loss: -0.1611 - val_accuracy: 0.2738\n",
      "Epoch 57/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9357 - accuracy: 0.60 - ETA: 0s - loss: -0.6148 - accuracy: 0.298 - ETA: 0s - loss: -0.1054 - accuracy: 0.283 - ETA: 0s - loss: -0.1592 - accuracy: 0.278 - ETA: 0s - loss: -0.1372 - accuracy: 0.278 - 0s 78us/step - loss: -0.2074 - accuracy: 0.2800 - val_loss: -0.3168 - val_accuracy: 0.2738\n",
      "Epoch 58/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -7.9280 - accuracy: 0.100 - ETA: 0s - loss: -0.0107 - accuracy: 0.302 - ETA: 0s - loss: 0.0248 - accuracy: 0.286 - ETA: 0s - loss: -0.4358 - accuracy: 0.269 - ETA: 0s - loss: -0.3792 - accuracy: 0.280 - 0s 81us/step - loss: -0.3002 - accuracy: 0.2800 - val_loss: -0.3999 - val_accuracy: 0.2738\n",
      "Epoch 59/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -3.9450 - accuracy: 0.300 - ETA: 0s - loss: -0.4500 - accuracy: 0.275 - ETA: 0s - loss: -0.7842 - accuracy: 0.283 - ETA: 0s - loss: -0.8146 - accuracy: 0.285 - ETA: 0s - loss: -0.7041 - accuracy: 0.279 - 0s 84us/step - loss: -0.4965 - accuracy: 0.2800 - val_loss: -0.2088 - val_accuracy: 0.2738\n",
      "Epoch 60/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -4.0486 - accuracy: 0.400 - ETA: 0s - loss: -1.5751 - accuracy: 0.292 - ETA: 0s - loss: -1.3239 - accuracy: 0.292 - ETA: 0s - loss: -1.1158 - accuracy: 0.284 - ETA: 0s - loss: -0.8011 - accuracy: 0.279 - 0s 89us/step - loss: -0.5662 - accuracy: 0.2800 - val_loss: -0.2302 - val_accuracy: 0.2738\n",
      "Epoch 61/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.5305 - accuracy: 0.20 - ETA: 0s - loss: -1.3525 - accuracy: 0.281 - ETA: 0s - loss: -0.3738 - accuracy: 0.280 - ETA: 0s - loss: -0.4800 - accuracy: 0.282 - ETA: 0s - loss: -0.7223 - accuracy: 0.282 - 0s 78us/step - loss: -0.7712 - accuracy: 0.2800 - val_loss: -0.9332 - val_accuracy: 0.2738\n",
      "Epoch 62/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1728 - accuracy: 0.400 - ETA: 0s - loss: -1.2063 - accuracy: 0.266 - ETA: 0s - loss: -1.2668 - accuracy: 0.283 - ETA: 0s - loss: -1.1582 - accuracy: 0.282 - 0s 73us/step - loss: -1.1939 - accuracy: 0.2800 - val_loss: -0.7357 - val_accuracy: 0.2750\n",
      "Epoch 63/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.70 - ETA: 0s - loss: -2.3830 - accuracy: 0.303 - ETA: 0s - loss: -1.9784 - accuracy: 0.291 - ETA: 0s - loss: -1.8592 - accuracy: 0.286 - 0s 74us/step - loss: -1.5209 - accuracy: 0.2800 - val_loss: -2.3123 - val_accuracy: 0.2738\n",
      "Epoch 64/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 7.9277 - accuracy: 0.40 - ETA: 0s - loss: -2.2089 - accuracy: 0.292 - ETA: 0s - loss: -1.8644 - accuracy: 0.274 - ETA: 0s - loss: -1.1008 - accuracy: 0.276 - ETA: 0s - loss: -2.2302 - accuracy: 0.280 - 0s 77us/step - loss: -2.0945 - accuracy: 0.2800 - val_loss: -3.1551 - val_accuracy: 0.2750\n",
      "Epoch 65/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -23.9677 - accuracy: 0.20 - ETA: 0s - loss: -3.4485 - accuracy: 0.2679 - ETA: 0s - loss: -1.1393 - accuracy: 0.276 - ETA: 0s - loss: -2.2297 - accuracy: 0.284 - ETA: 0s - loss: -2.7696 - accuracy: 0.279 - 0s 76us/step - loss: -2.6983 - accuracy: 0.2800 - val_loss: -3.5796 - val_accuracy: 0.2750\n",
      "Epoch 66/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -19.9322 - accuracy: 0.30 - ETA: 0s - loss: -6.6803 - accuracy: 0.2775 - ETA: 0s - loss: -4.0942 - accuracy: 0.275 - ETA: 0s - loss: -2.6754 - accuracy: 0.274 - 0s 70us/step - loss: -4.2286 - accuracy: 0.2800 - val_loss: -7.1302 - val_accuracy: 0.2750\n",
      "Epoch 67/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 9.5951 - accuracy: 0.30 - ETA: 0s - loss: -8.8362 - accuracy: 0.270 - ETA: 0s - loss: -7.7136 - accuracy: 0.275 - ETA: 0s - loss: -7.3523 - accuracy: 0.282 - 0s 73us/step - loss: -5.3811 - accuracy: 0.2803 - val_loss: -9.7722 - val_accuracy: 0.2750\n",
      "Epoch 68/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 20.1670 - accuracy: 0.500 - ETA: 0s - loss: -15.9944 - accuracy: 0.27 - ETA: 0s - loss: -9.3961 - accuracy: 0.2919 - ETA: 0s - loss: -9.3915 - accuracy: 0.284 - 0s 71us/step - loss: -8.6732 - accuracy: 0.2803 - val_loss: -8.1167 - val_accuracy: 0.2750\n",
      "Epoch 69/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1442 - accuracy: 0.200 - ETA: 0s - loss: -5.2639 - accuracy: 0.271 - ETA: 0s - loss: -8.3328 - accuracy: 0.269 - ETA: 0s - loss: -11.4445 - accuracy: 0.27 - 0s 74us/step - loss: -13.9888 - accuracy: 0.2803 - val_loss: -26.1295 - val_accuracy: 0.2750\n",
      "Epoch 70/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 20.7755 - accuracy: 0.200 - ETA: 0s - loss: -32.5750 - accuracy: 0.29 - ETA: 0s - loss: -39.1605 - accuracy: 0.28 - ETA: 0s - loss: -26.5744 - accuracy: 0.27 - 0s 74us/step - loss: -23.7607 - accuracy: 0.2803 - val_loss: -22.3810 - val_accuracy: 0.2738\n",
      "Epoch 71/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1750 - accuracy: 0.10 - ETA: 0s - loss: -52.5208 - accuracy: 0.26 - ETA: 0s - loss: -45.5582 - accuracy: 0.27 - ETA: 0s - loss: -58.6476 - accuracy: 0.28 - 0s 73us/step - loss: -47.1325 - accuracy: 0.2800 - val_loss: -111.1676 - val_accuracy: 0.2738\n",
      "Epoch 72/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 492.3813 - accuracy: 0.20 - ETA: 0s - loss: -5.5090 - accuracy: 0.2768 - ETA: 0s - loss: -113.0582 - accuracy: 0.279 - ETA: 0s - loss: -64.1482 - accuracy: 0.277 - 0s 74us/step - loss: -67.7996 - accuracy: 0.2800 - val_loss: -48.6544 - val_accuracy: 0.2750\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: -2998.2495 - accuracy: 0.20 - ETA: 0s - loss: -147.9407 - accuracy: 0.2659 - ETA: 0s - loss: -119.6578 - accuracy: 0.273 - ETA: 0s - loss: -81.4981 - accuracy: 0.274 - 0s 75us/step - loss: -111.8646 - accuracy: 0.2806 - val_loss: -222.9888 - val_accuracy: 0.2837\n",
      "Epoch 74/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -748.1100 - accuracy: 0.200 - ETA: 0s - loss: -51.8945 - accuracy: 0.286 - ETA: 0s - loss: 175.5722 - accuracy: 0.27 - ETA: 0s - loss: -127.3005 - accuracy: 0.288 - 0s 74us/step - loss: -97.0030 - accuracy: 0.2831 - val_loss: -44.4672 - val_accuracy: 0.2738\n",
      "Epoch 75/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 204.1272 - accuracy: 0.20 - ETA: 0s - loss: -372.3741 - accuracy: 0.293 - ETA: 0s - loss: -47.2949 - accuracy: 0.292 - ETA: 0s - loss: -579.7281 - accuracy: 0.280 - 0s 74us/step - loss: 863.9354 - accuracy: 0.2800 - val_loss: -391.0372 - val_accuracy: 0.2738\n",
      "Epoch 76/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4262 - accuracy: 0.400 - ETA: 0s - loss: -445.1256 - accuracy: 0.303 - ETA: 0s - loss: -1815.0757 - accuracy: 0.28 - ETA: 0s - loss: -2966.9879 - accuracy: 0.28 - 0s 74us/step - loss: -1882.9845 - accuracy: 0.2812 - val_loss: -140.4212 - val_accuracy: 0.2750\n",
      "Epoch 77/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.30 - ETA: 0s - loss: -1439.0125 - accuracy: 0.26 - ETA: 0s - loss: 6552.7094 - accuracy: 0.2761 - ETA: 0s - loss: 3602.8213 - accuracy: 0.276 - 0s 75us/step - loss: 1069.8310 - accuracy: 0.2800 - val_loss: -8696.7427 - val_accuracy: 0.2738\n",
      "Epoch 78/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 640.5997 - accuracy: 0.20 - ETA: 0s - loss: 3771387.8907 - accuracy: 0.28 - ETA: 0s - loss: 1885694.0946 - accuracy: 0.28 - ETA: 0s - loss: 1257129.4939 - accuracy: 0.27 - 0s 74us/step - loss: 942847.1863 - accuracy: 0.2797 - val_loss: 0.2757 - val_accuracy: 0.2738\n",
      "Epoch 79/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3426 - accuracy: 0.10 - ETA: 0s - loss: 0.2709 - accuracy: 0.23 - ETA: 0s - loss: 0.3313 - accuracy: 0.27 - ETA: 0s - loss: 0.3111 - accuracy: 0.27 - 0s 74us/step - loss: 0.3138 - accuracy: 0.2800 - val_loss: 0.2755 - val_accuracy: 0.2738\n",
      "Epoch 80/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.50 - ETA: 0s - loss: 0.2577 - accuracy: 0.26 - ETA: 0s - loss: 0.2812 - accuracy: 0.26 - ETA: 0s - loss: 0.2982 - accuracy: 0.27 - 0s 74us/step - loss: 0.3130 - accuracy: 0.2800 - val_loss: 0.2755 - val_accuracy: 0.2738\n",
      "Epoch 81/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2982 - accuracy: 0.50 - ETA: 0s - loss: 0.4370 - accuracy: 0.28 - ETA: 0s - loss: 0.3647 - accuracy: 0.28 - ETA: 0s - loss: 0.3363 - accuracy: 0.28 - 0s 74us/step - loss: 0.3124 - accuracy: 0.2800 - val_loss: 0.2756 - val_accuracy: 0.2738\n",
      "Epoch 82/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.30 - ETA: 0s - loss: 0.3240 - accuracy: 0.25 - ETA: 0s - loss: 0.3405 - accuracy: 0.27 - ETA: 0s - loss: 0.3260 - accuracy: 0.27 - ETA: 0s - loss: 0.3135 - accuracy: 0.28 - 0s 75us/step - loss: 0.3121 - accuracy: 0.2800 - val_loss: 0.2757 - val_accuracy: 0.2738\n",
      "Epoch 83/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1476 - accuracy: 0.300 - ETA: 0s - loss: 0.4216 - accuracy: 0.287 - ETA: 0s - loss: 0.3874 - accuracy: 0.27 - ETA: 0s - loss: 0.3421 - accuracy: 0.27 - 0s 73us/step - loss: 0.3119 - accuracy: 0.2800 - val_loss: 0.2758 - val_accuracy: 0.2738\n",
      "Epoch 84/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.40 - ETA: 0s - loss: 0.3813 - accuracy: 0.26 - ETA: 0s - loss: 0.3278 - accuracy: 0.28 - ETA: 0s - loss: 0.2925 - accuracy: 0.27 - 0s 71us/step - loss: 0.3118 - accuracy: 0.2800 - val_loss: 0.2760 - val_accuracy: 0.2738\n",
      "Epoch 85/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.30 - ETA: 0s - loss: 0.4417 - accuracy: 0.26 - ETA: 0s - loss: 0.3362 - accuracy: 0.26 - ETA: 0s - loss: 0.3348 - accuracy: 0.27 - 0s 74us/step - loss: 0.3117 - accuracy: 0.2800 - val_loss: 0.2761 - val_accuracy: 0.2738\n",
      "Epoch 86/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1394 - accuracy: 0.300 - ETA: 0s - loss: 0.2512 - accuracy: 0.272 - ETA: 0s - loss: 0.2796 - accuracy: 0.27 - ETA: 0s - loss: 0.2886 - accuracy: 0.27 - 0s 74us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2763 - val_accuracy: 0.2738\n",
      "Epoch 87/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5557 - accuracy: 0.40 - ETA: 0s - loss: 0.3032 - accuracy: 0.29 - ETA: 0s - loss: 0.2957 - accuracy: 0.27 - ETA: 0s - loss: 0.2698 - accuracy: 0.27 - ETA: 0s - loss: 0.3158 - accuracy: 0.27 - 0s 77us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2764 - val_accuracy: 0.2738\n",
      "Epoch 88/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5930 - accuracy: 0.300 - ETA: 0s - loss: 0.2700 - accuracy: 0.288 - ETA: 0s - loss: 0.2759 - accuracy: 0.27 - ETA: 0s - loss: 0.2738 - accuracy: 0.28 - ETA: 0s - loss: 0.3068 - accuracy: 0.28 - 0s 77us/step - loss: 0.3115 - accuracy: 0.2800 - val_loss: 0.2767 - val_accuracy: 0.2738\n",
      "Epoch 89/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.30 - ETA: 0s - loss: 0.3005 - accuracy: 0.27 - ETA: 0s - loss: 0.3285 - accuracy: 0.28 - ETA: 0s - loss: 0.3378 - accuracy: 0.28 - 0s 73us/step - loss: 0.3117 - accuracy: 0.2800 - val_loss: 0.2766 - val_accuracy: 0.2738\n",
      "Epoch 90/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3580 - accuracy: 0.200 - ETA: 0s - loss: 0.2476 - accuracy: 0.292 - ETA: 0s - loss: 0.3090 - accuracy: 0.27 - ETA: 0s - loss: 0.3288 - accuracy: 0.26 - ETA: 0s - loss: 0.3116 - accuracy: 0.27 - 0s 76us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2765 - val_accuracy: 0.2738\n",
      "Epoch 91/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6963 - accuracy: 0.30 - ETA: 0s - loss: 0.3690 - accuracy: 0.29 - ETA: 0s - loss: 0.3158 - accuracy: 0.29 - ETA: 0s - loss: 0.3056 - accuracy: 0.29 - ETA: 0s - loss: 0.3056 - accuracy: 0.28 - 0s 77us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2765 - val_accuracy: 0.2738\n",
      "Epoch 92/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1311 - accuracy: 0.300 - ETA: 0s - loss: 0.3173 - accuracy: 0.276 - ETA: 0s - loss: 0.2926 - accuracy: 0.27 - ETA: 0s - loss: 0.3151 - accuracy: 0.28 - ETA: 0s - loss: 0.3142 - accuracy: 0.27 - 0s 77us/step - loss: 0.3115 - accuracy: 0.2800 - val_loss: 0.2767 - val_accuracy: 0.2738\n",
      "Epoch 93/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7791 - accuracy: 0.30 - ETA: 0s - loss: 0.3369 - accuracy: 0.28 - ETA: 0s - loss: 0.3498 - accuracy: 0.27 - ETA: 0s - loss: 0.3135 - accuracy: 0.28 - ETA: 0s - loss: 0.3087 - accuracy: 0.27 - 0s 76us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2765 - val_accuracy: 0.2738\n",
      "Epoch 94/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7806 - accuracy: 0.30 - ETA: 0s - loss: 0.2907 - accuracy: 0.31 - ETA: 0s - loss: 0.3258 - accuracy: 0.28 - ETA: 0s - loss: 0.3209 - accuracy: 0.28 - ETA: 0s - loss: 0.3157 - accuracy: 0.28 - 0s 78us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2765 - val_accuracy: 0.2738\n",
      "Epoch 95/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.2446 - accuracy: 0.2899   - ETA: 0s - loss: 0.2488 - accuracy: 0.26 - ETA: 0s - loss: 0.2951 - accuracy: 0.26 - 0s 75us/step - loss: 0.3114 - accuracy: 0.2800 - val_loss: 0.2768 - val_accuracy: 0.2738\n",
      "Epoch 96/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5766 - accuracy: 0.300 - ETA: 0s - loss: 0.3828 - accuracy: 0.254 - ETA: 0s - loss: 0.2832 - accuracy: 0.25 - ETA: 0s - loss: 0.3210 - accuracy: 0.27 - ETA: 0s - loss: 0.3043 - accuracy: 0.27 - 0s 76us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2767 - val_accuracy: 0.2738\n",
      "Epoch 97/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.40 - ETA: 0s - loss: 0.3726 - accuracy: 0.29 - ETA: 0s - loss: 0.2859 - accuracy: 0.28 - ETA: 0s - loss: 0.3087 - accuracy: 0.27 - 0s 74us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2767 - val_accuracy: 0.2738\n",
      "Epoch 98/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2591 - accuracy: 0.200 - ETA: 0s - loss: 0.2560 - accuracy: 0.259 - ETA: 0s - loss: 0.3137 - accuracy: 0.27 - ETA: 0s - loss: 0.2902 - accuracy: 0.28 - ETA: 0s - loss: 0.3120 - accuracy: 0.28 - 0s 78us/step - loss: 0.3117 - accuracy: 0.2800 - val_loss: 0.2766 - val_accuracy: 0.2738\n",
      "Epoch 99/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1274 - accuracy: 0.300 - ETA: 0s - loss: 0.3029 - accuracy: 0.295 - ETA: 0s - loss: 0.3012 - accuracy: 0.27 - ETA: 0s - loss: 0.3275 - accuracy: 0.27 - ETA: 0s - loss: 0.3101 - accuracy: 0.28 - 0s 76us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2766 - val_accuracy: 0.2738\n",
      "Epoch 100/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5812 - accuracy: 0.100 - ETA: 0s - loss: 0.3487 - accuracy: 0.262 - ETA: 0s - loss: 0.3257 - accuracy: 0.28 - ETA: 0s - loss: 0.3295 - accuracy: 0.27 - ETA: 0s - loss: 0.3108 - accuracy: 0.27 - 0s 76us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2765 - val_accuracy: 0.2738\n",
      "Epoch 101/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5520 - accuracy: 0.20 - ETA: 0s - loss: 0.3033 - accuracy: 0.26 - ETA: 0s - loss: 0.3103 - accuracy: 0.27 - ETA: 0s - loss: 0.3051 - accuracy: 0.28 - 0s 74us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2765 - val_accuracy: 0.2738\n",
      "Epoch 102/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.50 - ETA: 0s - loss: 0.2861 - accuracy: 0.29 - ETA: 0s - loss: 0.2816 - accuracy: 0.29 - ETA: 0s - loss: 0.3144 - accuracy: 0.28 - 0s 75us/step - loss: 0.3116 - accuracy: 0.2800 - val_loss: 0.2764 - val_accuracy: 0.2738\n",
      "Epoch 103/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3575 - accuracy: 0.200 - ETA: 0s - loss: 0.3593 - accuracy: 0.275 - ETA: 0s - loss: 0.3570 - accuracy: 0.28 - ETA: 0s - loss: 0.3399 - accuracy: 0.27 - ETA: 0s - loss: 0.3107 - accuracy: 0.28 - 0s 76us/step - loss: 0.3115 - accuracy: 0.2800 - val_loss: 0.2763 - val_accuracy: 0.2738\n",
      "Epoch 104/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5867 - accuracy: 0.100 - ETA: 0s - loss: 0.2805 - accuracy: 0.292 - ETA: 0s - loss: 0.3411 - accuracy: 0.27 - ETA: 0s - loss: 0.3445 - accuracy: 0.28 - ETA: 0s - loss: 0.3114 - accuracy: 0.27 - 0s 76us/step - loss: 0.3114 - accuracy: 0.2800 - val_loss: 0.2763 - val_accuracy: 0.2738\n",
      "Epoch 105/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7815 - accuracy: 0.10 - ETA: 0s - loss: 0.3537 - accuracy: 0.27 - ETA: 0s - loss: 0.2703 - accuracy: 0.29 - ETA: 0s - loss: 0.3022 - accuracy: 0.28 - 0s 74us/step - loss: 0.3115 - accuracy: 0.2800 - val_loss: 0.2763 - val_accuracy: 0.2738\n",
      "Epoch 106/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.40 - ETA: 0s - loss: 0.3256 - accuracy: 0.27 - ETA: 0s - loss: 0.3383 - accuracy: 0.27 - ETA: 0s - loss: 0.3180 - accuracy: 0.27 - 0s 74us/step - loss: 0.3115 - accuracy: 0.2800 - val_loss: 0.2762 - val_accuracy: 0.2738\n",
      "Epoch 107/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7801 - accuracy: 0.10 - ETA: 0s - loss: 0.3285 - accuracy: 0.28 - ETA: 0s - loss: 0.3383 - accuracy: 0.29 - ETA: 0s - loss: 0.3293 - accuracy: 0.28 - 0s 75us/step - loss: 0.3114 - accuracy: 0.2800 - val_loss: 0.2761 - val_accuracy: 0.2738\n",
      "Epoch 108/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7792 - accuracy: 0.10 - ETA: 0s - loss: 0.2037 - accuracy: 0.29 - ETA: 0s - loss: 0.2640 - accuracy: 0.27 - ETA: 0s - loss: 0.3133 - accuracy: 0.28 - 0s 75us/step - loss: 0.3113 - accuracy: 0.2800 - val_loss: 0.2760 - val_accuracy: 0.2738\n",
      "Epoch 109/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0080 - accuracy: 0.40 - ETA: 0s - loss: 0.2755 - accuracy: 0.29 - ETA: 0s - loss: 0.3056 - accuracy: 0.28 - ETA: 0s - loss: 0.3348 - accuracy: 0.27 - ETA: 0s - loss: 0.3140 - accuracy: 0.27 - 0s 76us/step - loss: 0.3112 - accuracy: 0.2800 - val_loss: 0.2759 - val_accuracy: 0.2738\n",
      "Epoch 110/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.40 - ETA: 0s - loss: 0.2871 - accuracy: 0.29 - ETA: 0s - loss: 0.2836 - accuracy: 0.29 - ETA: 0s - loss: 0.2758 - accuracy: 0.27 - 0s 74us/step - loss: 0.3109 - accuracy: 0.2800 - val_loss: 0.2760 - val_accuracy: 0.2738\n",
      "Epoch 111/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7788 - accuracy: 0.30 - ETA: 0s - loss: 0.1767 - accuracy: 0.27 - ETA: 0s - loss: 0.2349 - accuracy: 0.27 - ETA: 0s - loss: 0.2908 - accuracy: 0.27 - ETA: 0s - loss: 0.3085 - accuracy: 0.28 - 0s 76us/step - loss: 0.3107 - accuracy: 0.2800 - val_loss: 0.2759 - val_accuracy: 0.2738\n",
      "Epoch 112/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.10 - ETA: 0s - loss: 0.3047 - accuracy: 0.28 - ETA: 0s - loss: 0.3175 - accuracy: 0.28 - ETA: 0s - loss: 0.2992 - accuracy: 0.28 - ETA: 0s - loss: 0.3135 - accuracy: 0.27 - 0s 77us/step - loss: 0.3108 - accuracy: 0.2800 - val_loss: 0.2757 - val_accuracy: 0.2738\n",
      "Epoch 113/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2339 - accuracy: 0.10 - ETA: 0s - loss: 0.3279 - accuracy: 0.27 - ETA: 0s - loss: 0.3647 - accuracy: 0.27 - ETA: 0s - loss: 0.3390 - accuracy: 0.28 - ETA: 0s - loss: 0.3105 - accuracy: 0.27 - 0s 76us/step - loss: 0.3106 - accuracy: 0.2800 - val_loss: 0.2753 - val_accuracy: 0.2738\n",
      "Epoch 114/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.40 - ETA: 0s - loss: 0.3135 - accuracy: 0.28 - ETA: 0s - loss: 0.2942 - accuracy: 0.28 - ETA: 0s - loss: 0.3053 - accuracy: 0.28 - 0s 74us/step - loss: 0.3105 - accuracy: 0.2800 - val_loss: 0.2752 - val_accuracy: 0.2738\n",
      "Epoch 115/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1263 - accuracy: 0.300 - ETA: 0s - loss: 0.2977 - accuracy: 0.256 - ETA: 0s - loss: 0.3066 - accuracy: 0.26 - ETA: 0s - loss: 0.2983 - accuracy: 0.27 - 0s 75us/step - loss: 0.3104 - accuracy: 0.2800 - val_loss: 0.2750 - val_accuracy: 0.2738\n",
      "Epoch 116/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.20 - ETA: 0s - loss: 0.2190 - accuracy: 0.27 - ETA: 0s - loss: 0.2898 - accuracy: 0.29 - ETA: 0s - loss: 0.3062 - accuracy: 0.28 - 0s 75us/step - loss: 0.3103 - accuracy: 0.2800 - val_loss: 0.2749 - val_accuracy: 0.2738\n",
      "Epoch 117/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5720 - accuracy: 0.300 - ETA: 0s - loss: 0.2959 - accuracy: 0.277 - ETA: 0s - loss: 0.3326 - accuracy: 0.28 - ETA: 0s - loss: 0.3000 - accuracy: 0.28 - ETA: 0s - loss: 0.3083 - accuracy: 0.28 - 0s 78us/step - loss: 0.3101 - accuracy: 0.2800 - val_loss: 0.2747 - val_accuracy: 0.2738\n",
      "Epoch 118/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4567 - accuracy: 0.40 - ETA: 0s - loss: 0.2759 - accuracy: 0.27 - ETA: 0s - loss: 0.2913 - accuracy: 0.28 - ETA: 0s - loss: 0.3052 - accuracy: 0.28 - ETA: 0s - loss: 0.3091 - accuracy: 0.28 - 0s 77us/step - loss: 0.3101 - accuracy: 0.2800 - val_loss: 0.2745 - val_accuracy: 0.2738\n",
      "Epoch 119/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5537 - accuracy: 0.40 - ETA: 0s - loss: 0.3066 - accuracy: 0.26 - ETA: 0s - loss: 0.2782 - accuracy: 0.26 - ETA: 0s - loss: 0.3170 - accuracy: 0.27 - ETA: 0s - loss: 0.3069 - accuracy: 0.28 - 0s 77us/step - loss: 0.3099 - accuracy: 0.2800 - val_loss: 0.2744 - val_accuracy: 0.2738\n",
      "Epoch 120/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8077 - accuracy: 0.400 - ETA: 0s - loss: 0.1377 - accuracy: 0.298 - ETA: 0s - loss: 0.2790 - accuracy: 0.30 - ETA: 0s - loss: 0.3053 - accuracy: 0.28 - ETA: 0s - loss: 0.3101 - accuracy: 0.28 - 0s 78us/step - loss: 0.3097 - accuracy: 0.2800 - val_loss: 0.2744 - val_accuracy: 0.2738\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7652 - accuracy: 0.30 - ETA: 0s - loss: 0.4201 - accuracy: 0.28 - ETA: 0s - loss: 0.3490 - accuracy: 0.29 - ETA: 0s - loss: 0.3137 - accuracy: 0.28 - ETA: 0s - loss: 0.3117 - accuracy: 0.27 - 0s 80us/step - loss: 0.3096 - accuracy: 0.2800 - val_loss: 0.2739 - val_accuracy: 0.2738\n",
      "Epoch 122/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7702 - accuracy: 0.30 - ETA: 0s - loss: 0.1724 - accuracy: 0.29 - ETA: 0s - loss: 0.2531 - accuracy: 0.26 - ETA: 0s - loss: 0.2763 - accuracy: 0.27 - ETA: 0s - loss: 0.3044 - accuracy: 0.27 - 0s 79us/step - loss: 0.3093 - accuracy: 0.2800 - val_loss: 0.2741 - val_accuracy: 0.2738\n",
      "Epoch 123/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5730 - accuracy: 0.500 - ETA: 0s - loss: 0.2527 - accuracy: 0.276 - ETA: 0s - loss: 0.3472 - accuracy: 0.27 - ETA: 0s - loss: 0.3084 - accuracy: 0.28 - ETA: 0s - loss: 0.3211 - accuracy: 0.27 - 0s 79us/step - loss: 0.3093 - accuracy: 0.2800 - val_loss: 0.2737 - val_accuracy: 0.2738\n",
      "Epoch 124/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.40 - ETA: 0s - loss: 0.2083 - accuracy: 0.28 - ETA: 0s - loss: 0.2574 - accuracy: 0.28 - ETA: 0s - loss: 0.2817 - accuracy: 0.28 - ETA: 0s - loss: 0.3119 - accuracy: 0.27 - 0s 79us/step - loss: 0.3090 - accuracy: 0.2800 - val_loss: 0.2735 - val_accuracy: 0.2738\n",
      "Epoch 125/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9931 - accuracy: 0.20 - ETA: 0s - loss: 0.2694 - accuracy: 0.26 - ETA: 0s - loss: 0.2840 - accuracy: 0.26 - ETA: 0s - loss: 0.3107 - accuracy: 0.27 - ETA: 0s - loss: 0.3206 - accuracy: 0.27 - 0s 82us/step - loss: 0.3089 - accuracy: 0.2800 - val_loss: 0.2731 - val_accuracy: 0.2738\n",
      "Epoch 126/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0071 - accuracy: 0.20 - ETA: 0s - loss: 0.2795 - accuracy: 0.29 - ETA: 0s - loss: 0.3255 - accuracy: 0.28 - ETA: 0s - loss: 0.2940 - accuracy: 0.28 - ETA: 0s - loss: 0.3161 - accuracy: 0.27 - 0s 79us/step - loss: 0.3087 - accuracy: 0.2800 - val_loss: 0.2726 - val_accuracy: 0.2738\n",
      "Epoch 127/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5773 - accuracy: 0.500 - ETA: 0s - loss: 0.3584 - accuracy: 0.270 - ETA: 0s - loss: 0.3275 - accuracy: 0.27 - ETA: 0s - loss: 0.2970 - accuracy: 0.27 - ETA: 0s - loss: 0.3080 - accuracy: 0.27 - 0s 78us/step - loss: 0.3084 - accuracy: 0.2800 - val_loss: 0.2721 - val_accuracy: 0.2738\n",
      "Epoch 128/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6702 - accuracy: 0.30 - ETA: 0s - loss: 0.3387 - accuracy: 0.26 - ETA: 0s - loss: 0.3543 - accuracy: 0.28 - ETA: 0s - loss: 0.2921 - accuracy: 0.28 - ETA: 0s - loss: 0.3019 - accuracy: 0.27 - 0s 78us/step - loss: 0.3081 - accuracy: 0.2800 - val_loss: 0.2719 - val_accuracy: 0.2738\n",
      "Epoch 129/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.40 - ETA: 0s - loss: 0.3235 - accuracy: 0.25 - ETA: 0s - loss: 0.3353 - accuracy: 0.26 - ETA: 0s - loss: 0.3415 - accuracy: 0.27 - ETA: 0s - loss: 0.3181 - accuracy: 0.27 - 0s 77us/step - loss: 0.3077 - accuracy: 0.2800 - val_loss: 0.2712 - val_accuracy: 0.2738\n",
      "Epoch 130/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7788 - accuracy: 0.10 - ETA: 0s - loss: 0.3703 - accuracy: 0.26 - ETA: 0s - loss: 0.2855 - accuracy: 0.28 - ETA: 0s - loss: 0.2859 - accuracy: 0.28 - ETA: 0s - loss: 0.3108 - accuracy: 0.27 - 0s 78us/step - loss: 0.3075 - accuracy: 0.2800 - val_loss: 0.2711 - val_accuracy: 0.2738\n",
      "Epoch 131/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.40 - ETA: 0s - loss: 0.2896 - accuracy: 0.29 - ETA: 0s - loss: 0.3267 - accuracy: 0.29 - ETA: 0s - loss: 0.3087 - accuracy: 0.28 - ETA: 0s - loss: 0.3084 - accuracy: 0.28 - 0s 77us/step - loss: 0.3071 - accuracy: 0.2800 - val_loss: 0.2705 - val_accuracy: 0.2738\n",
      "Epoch 132/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5712 - accuracy: 0.100 - ETA: 0s - loss: 0.2262 - accuracy: 0.290 - ETA: 0s - loss: 0.2736 - accuracy: 0.28 - ETA: 0s - loss: 0.3038 - accuracy: 0.28 - 0s 75us/step - loss: 0.3065 - accuracy: 0.2800 - val_loss: 0.2702 - val_accuracy: 0.2738\n",
      "Epoch 133/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.30 - ETA: 0s - loss: 0.2915 - accuracy: 0.25 - ETA: 0s - loss: 0.2882 - accuracy: 0.27 - ETA: 0s - loss: 0.2892 - accuracy: 0.28 - ETA: 0s - loss: 0.2987 - accuracy: 0.28 - 0s 77us/step - loss: 0.3062 - accuracy: 0.2800 - val_loss: 0.2696 - val_accuracy: 0.2738\n",
      "Epoch 134/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.10 - ETA: 0s - loss: 0.2067 - accuracy: 0.25 - ETA: 0s - loss: 0.2592 - accuracy: 0.27 - ETA: 0s - loss: 0.2901 - accuracy: 0.27 - 0s 74us/step - loss: 0.3057 - accuracy: 0.2800 - val_loss: 0.2692 - val_accuracy: 0.2738\n",
      "Epoch 135/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.2609 - accuracy: 0.2886   - ETA: 0s - loss: 0.2620 - accuracy: 0.27 - ETA: 0s - loss: 0.2739 - accuracy: 0.28 - 0s 75us/step - loss: 0.3052 - accuracy: 0.2800 - val_loss: 0.2688 - val_accuracy: 0.2738\n",
      "Epoch 136/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3275 - accuracy: 0.400 - ETA: 0s - loss: 0.3221 - accuracy: 0.262 - ETA: 0s - loss: 0.2649 - accuracy: 0.26 - ETA: 0s - loss: 0.3146 - accuracy: 0.27 - 0s 75us/step - loss: 0.3045 - accuracy: 0.2800 - val_loss: 0.2678 - val_accuracy: 0.2738\n",
      "Epoch 137/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.30 - ETA: 0s - loss: 0.3127 - accuracy: 0.26 - ETA: 0s - loss: 0.3437 - accuracy: 0.27 - ETA: 0s - loss: 0.3187 - accuracy: 0.27 - 0s 75us/step - loss: 0.3040 - accuracy: 0.2800 - val_loss: 0.2666 - val_accuracy: 0.2738\n",
      "Epoch 138/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5858 - accuracy: 0.500 - ETA: 0s - loss: 0.3746 - accuracy: 0.268 - ETA: 0s - loss: 0.3410 - accuracy: 0.27 - ETA: 0s - loss: 0.3103 - accuracy: 0.27 - 0s 74us/step - loss: 0.3031 - accuracy: 0.2800 - val_loss: 0.2653 - val_accuracy: 0.2738\n",
      "Epoch 139/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.20 - ETA: 0s - loss: 0.2539 - accuracy: 0.27 - ETA: 0s - loss: 0.2946 - accuracy: 0.27 - ETA: 0s - loss: 0.2986 - accuracy: 0.27 - 0s 74us/step - loss: 0.3026 - accuracy: 0.2800 - val_loss: 0.2645 - val_accuracy: 0.2738\n",
      "Epoch 140/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.50 - ETA: 0s - loss: 0.2586 - accuracy: 0.27 - ETA: 0s - loss: 0.2792 - accuracy: 0.27 - ETA: 0s - loss: 0.3087 - accuracy: 0.27 - 0s 74us/step - loss: 0.3015 - accuracy: 0.2800 - val_loss: 0.2637 - val_accuracy: 0.2738\n",
      "Epoch 141/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5278 - accuracy: 0.20 - ETA: 0s - loss: 0.2930 - accuracy: 0.26 - ETA: 0s - loss: 0.3427 - accuracy: 0.27 - ETA: 0s - loss: 0.3435 - accuracy: 0.27 - ETA: 0s - loss: 0.3015 - accuracy: 0.28 - 0s 76us/step - loss: 0.3001 - accuracy: 0.2800 - val_loss: 0.2619 - val_accuracy: 0.2738\n",
      "Epoch 142/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5747 - accuracy: 0.20 - ETA: 0s - loss: 0.2279 - accuracy: 0.27 - ETA: 0s - loss: 0.2498 - accuracy: 0.27 - ETA: 0s - loss: 0.3040 - accuracy: 0.27 - ETA: 0s - loss: 0.3020 - accuracy: 0.27 - 0s 77us/step - loss: 0.2994 - accuracy: 0.2800 - val_loss: 0.2610 - val_accuracy: 0.2738\n",
      "Epoch 143/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.20 - ETA: 0s - loss: 0.3253 - accuracy: 0.29 - ETA: 0s - loss: 0.3640 - accuracy: 0.27 - ETA: 0s - loss: 0.3067 - accuracy: 0.27 - 0s 71us/step - loss: 0.2983 - accuracy: 0.2800 - val_loss: 0.2590 - val_accuracy: 0.2738\n",
      "Epoch 144/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9868 - accuracy: 0.20 - ETA: 0s - loss: 0.2724 - accuracy: 0.27 - ETA: 0s - loss: 0.2841 - accuracy: 0.27 - ETA: 0s - loss: 0.3026 - accuracy: 0.27 - 0s 74us/step - loss: 0.2969 - accuracy: 0.2800 - val_loss: 0.2575 - val_accuracy: 0.2738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.40 - ETA: 0s - loss: 0.3225 - accuracy: 0.27 - ETA: 0s - loss: 0.2909 - accuracy: 0.28 - ETA: 0s - loss: 0.2989 - accuracy: 0.28 - 0s 74us/step - loss: 0.2957 - accuracy: 0.2800 - val_loss: 0.2556 - val_accuracy: 0.2738\n",
      "Epoch 146/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7796 - accuracy: 0.50 - ETA: 0s - loss: 0.3507 - accuracy: 0.28 - ETA: 0s - loss: 0.3213 - accuracy: 0.28 - ETA: 0s - loss: 0.3047 - accuracy: 0.27 - ETA: 0s - loss: 0.2997 - accuracy: 0.27 - 0s 76us/step - loss: 0.2937 - accuracy: 0.2800 - val_loss: 0.2538 - val_accuracy: 0.2750\n",
      "Epoch 147/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7192 - accuracy: 0.50 - ETA: 0s - loss: 0.2132 - accuracy: 0.30 - ETA: 0s - loss: 0.2646 - accuracy: 0.28 - ETA: 0s - loss: 0.2705 - accuracy: 0.28 - 0s 75us/step - loss: 0.2916 - accuracy: 0.2800 - val_loss: 0.2520 - val_accuracy: 0.2750\n",
      "Epoch 148/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.7980 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.3140 - accuracy: 0.2614   - ETA: 0s - loss: 0.3169 - accuracy: 0.27 - ETA: 0s - loss: 0.2947 - accuracy: 0.28 - 0s 71us/step - loss: 0.2900 - accuracy: 0.2800 - val_loss: 0.2492 - val_accuracy: 0.2750\n",
      "Epoch 149/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.20 - ETA: 0s - loss: 0.2205 - accuracy: 0.26 - ETA: 0s - loss: 0.2843 - accuracy: 0.27 - ETA: 0s - loss: 0.3050 - accuracy: 0.27 - 0s 74us/step - loss: 0.2871 - accuracy: 0.2800 - val_loss: 0.2465 - val_accuracy: 0.2750\n",
      "Epoch 150/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1647 - accuracy: 0.300 - ETA: 0s - loss: 0.2121 - accuracy: 0.279 - ETA: 0s - loss: 0.2690 - accuracy: 0.28 - ETA: 0s - loss: 0.2820 - accuracy: 0.28 - 0s 73us/step - loss: 0.2836 - accuracy: 0.2800 - val_loss: 0.2477 - val_accuracy: 0.2750\n",
      "Epoch 151/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.20 - ETA: 0s - loss: 0.3044 - accuracy: 0.28 - ETA: 0s - loss: 0.3155 - accuracy: 0.28 - ETA: 0s - loss: 0.3053 - accuracy: 0.28 - ETA: 0s - loss: 0.2892 - accuracy: 0.28 - 0s 78us/step - loss: 0.2820 - accuracy: 0.2800 - val_loss: 0.2416 - val_accuracy: 0.2750\n",
      "Epoch 152/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7501 - accuracy: 0.30 - ETA: 0s - loss: 0.3343 - accuracy: 0.25 - ETA: 0s - loss: 0.3192 - accuracy: 0.27 - ETA: 0s - loss: 0.2874 - accuracy: 0.27 - ETA: 0s - loss: 0.2740 - accuracy: 0.28 - 0s 78us/step - loss: 0.2799 - accuracy: 0.2800 - val_loss: 0.2383 - val_accuracy: 0.2750\n",
      "Epoch 153/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2202 - accuracy: 0.200 - ETA: 0s - loss: 0.3128 - accuracy: 0.265 - ETA: 0s - loss: 0.2858 - accuracy: 0.28 - ETA: 0s - loss: 0.2813 - accuracy: 0.28 - 0s 74us/step - loss: 0.2768 - accuracy: 0.2800 - val_loss: 0.2356 - val_accuracy: 0.2750\n",
      "Epoch 154/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2603 - accuracy: 0.200 - ETA: 0s - loss: 0.3386 - accuracy: 0.272 - ETA: 0s - loss: 0.3149 - accuracy: 0.27 - ETA: 0s - loss: 0.2992 - accuracy: 0.28 - 0s 74us/step - loss: 0.2719 - accuracy: 0.2800 - val_loss: 0.2371 - val_accuracy: 0.2750\n",
      "Epoch 155/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3295 - accuracy: 0.10 - ETA: 0s - loss: 0.3287 - accuracy: 0.27 - ETA: 0s - loss: 0.2492 - accuracy: 0.27 - ETA: 0s - loss: 0.2467 - accuracy: 0.27 - 0s 70us/step - loss: 0.2717 - accuracy: 0.2806 - val_loss: 0.2310 - val_accuracy: 0.2763\n",
      "Epoch 156/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1776 - accuracy: 0.100 - ETA: 0s - loss: 0.2553 - accuracy: 0.297 - ETA: 0s - loss: 0.2343 - accuracy: 0.27 - ETA: 0s - loss: 0.2663 - accuracy: 0.28 - 0s 73us/step - loss: 0.2682 - accuracy: 0.2806 - val_loss: 0.2294 - val_accuracy: 0.2763\n",
      "Epoch 157/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0951 - accuracy: 0.10 - ETA: 0s - loss: 0.2861 - accuracy: 0.29 - ETA: 0s - loss: 0.2719 - accuracy: 0.28 - ETA: 0s - loss: 0.2758 - accuracy: 0.28 - 0s 71us/step - loss: 0.2654 - accuracy: 0.2806 - val_loss: 0.2277 - val_accuracy: 0.2763\n",
      "Epoch 158/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.20 - ETA: 0s - loss: 0.2519 - accuracy: 0.27 - ETA: 0s - loss: 0.1896 - accuracy: 0.28 - ETA: 0s - loss: 0.2351 - accuracy: 0.28 - 0s 73us/step - loss: 0.2620 - accuracy: 0.2806 - val_loss: 0.2325 - val_accuracy: 0.2788\n",
      "Epoch 159/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4401 - accuracy: 0.30 - ETA: 0s - loss: 0.2588 - accuracy: 0.28 - ETA: 0s - loss: 0.2621 - accuracy: 0.28 - ETA: 0s - loss: 0.2823 - accuracy: 0.27 - ETA: 0s - loss: 0.2656 - accuracy: 0.28 - 0s 76us/step - loss: 0.2618 - accuracy: 0.2812 - val_loss: 0.2204 - val_accuracy: 0.2763\n",
      "Epoch 160/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1385 - accuracy: 0.400 - ETA: 0s - loss: 0.3054 - accuracy: 0.281 - ETA: 0s - loss: 0.2207 - accuracy: 0.28 - ETA: 0s - loss: 0.2349 - accuracy: 0.28 - ETA: 0s - loss: 0.2541 - accuracy: 0.28 - 0s 76us/step - loss: 0.2552 - accuracy: 0.2819 - val_loss: 0.2266 - val_accuracy: 0.2825\n",
      "Epoch 161/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.30 - ETA: 0s - loss: 0.2187 - accuracy: 0.26 - ETA: 0s - loss: 0.2788 - accuracy: 0.27 - ETA: 0s - loss: 0.2611 - accuracy: 0.28 - 0s 74us/step - loss: 0.2553 - accuracy: 0.2816 - val_loss: 0.2174 - val_accuracy: 0.2800\n",
      "Epoch 162/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.20 - ETA: 0s - loss: 0.2699 - accuracy: 0.27 - ETA: 0s - loss: 0.2498 - accuracy: 0.26 - ETA: 0s - loss: 0.2449 - accuracy: 0.27 - ETA: 0s - loss: 0.2528 - accuracy: 0.28 - 0s 77us/step - loss: 0.2513 - accuracy: 0.2819 - val_loss: 0.2140 - val_accuracy: 0.2800\n",
      "Epoch 163/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0138 - accuracy: 0.300 - ETA: 0s - loss: 0.2645 - accuracy: 0.292 - ETA: 0s - loss: 0.2846 - accuracy: 0.28 - ETA: 0s - loss: 0.2602 - accuracy: 0.27 - 0s 74us/step - loss: 0.2512 - accuracy: 0.2825 - val_loss: 0.2171 - val_accuracy: 0.2850\n",
      "Epoch 164/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6957 - accuracy: 0.300 - ETA: 0s - loss: 0.3045 - accuracy: 0.288 - ETA: 0s - loss: 0.2969 - accuracy: 0.28 - ETA: 0s - loss: 0.2856 - accuracy: 0.28 - ETA: 0s - loss: 0.2426 - accuracy: 0.28 - 0s 79us/step - loss: 0.2500 - accuracy: 0.2834 - val_loss: 0.2084 - val_accuracy: 0.2837\n",
      "Epoch 165/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4052 - accuracy: 0.30 - ETA: 0s - loss: 0.1533 - accuracy: 0.27 - ETA: 0s - loss: 0.2607 - accuracy: 0.27 - ETA: 0s - loss: 0.2750 - accuracy: 0.27 - 0s 74us/step - loss: 0.2456 - accuracy: 0.2819 - val_loss: 0.2079 - val_accuracy: 0.2850\n",
      "Epoch 166/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2558 - accuracy: 0.500 - ETA: 0s - loss: 0.2801 - accuracy: 0.296 - ETA: 0s - loss: 0.1789 - accuracy: 0.28 - ETA: 0s - loss: 0.2356 - accuracy: 0.27 - ETA: 0s - loss: 0.2417 - accuracy: 0.28 - 0s 78us/step - loss: 0.2441 - accuracy: 0.2828 - val_loss: 0.2052 - val_accuracy: 0.2850\n",
      "Epoch 167/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2332 - accuracy: 0.200 - ETA: 0s - loss: 0.1296 - accuracy: 0.286 - ETA: 0s - loss: 0.1906 - accuracy: 0.29 - ETA: 0s - loss: 0.2119 - accuracy: 0.28 - ETA: 0s - loss: 0.2402 - accuracy: 0.28 - 0s 78us/step - loss: 0.2440 - accuracy: 0.2847 - val_loss: 0.2142 - val_accuracy: 0.2887\n",
      "Epoch 168/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.40 - ETA: 0s - loss: 0.3002 - accuracy: 0.28 - ETA: 0s - loss: 0.2155 - accuracy: 0.26 - ETA: 0s - loss: 0.1737 - accuracy: 0.27 - ETA: 0s - loss: 0.2399 - accuracy: 0.28 - 0s 79us/step - loss: 0.2385 - accuracy: 0.2872 - val_loss: 0.2079 - val_accuracy: 0.2875\n",
      "Epoch 169/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0577 - accuracy: 0.50 - ETA: 0s - loss: 0.3415 - accuracy: 0.28 - ETA: 0s - loss: 0.2779 - accuracy: 0.28 - ETA: 0s - loss: 0.2694 - accuracy: 0.28 - ETA: 0s - loss: 0.2503 - accuracy: 0.29 - 0s 80us/step - loss: 0.2396 - accuracy: 0.2872 - val_loss: 0.1968 - val_accuracy: 0.2825\n",
      "Epoch 170/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2176 - accuracy: 0.10 - ETA: 0s - loss: 0.3280 - accuracy: 0.28 - ETA: 0s - loss: 0.2415 - accuracy: 0.28 - ETA: 0s - loss: 0.2321 - accuracy: 0.28 - ETA: 0s - loss: 0.2392 - accuracy: 0.28 - 0s 77us/step - loss: 0.2401 - accuracy: 0.2841 - val_loss: 0.1965 - val_accuracy: 0.2862\n",
      "Epoch 171/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6681 - accuracy: 0.100 - ETA: 0s - loss: 0.2386 - accuracy: 0.298 - ETA: 0s - loss: 0.3197 - accuracy: 0.30 - ETA: 0s - loss: 0.2716 - accuracy: 0.28 - ETA: 0s - loss: 0.2395 - accuracy: 0.28 - 0s 83us/step - loss: 0.2394 - accuracy: 0.2862 - val_loss: 0.1930 - val_accuracy: 0.2850\n",
      "Epoch 172/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4335 - accuracy: 0.200 - ETA: 0s - loss: 0.2758 - accuracy: 0.278 - ETA: 0s - loss: 0.2521 - accuracy: 0.27 - ETA: 0s - loss: 0.2412 - accuracy: 0.29 - ETA: 0s - loss: 0.2405 - accuracy: 0.28 - 0s 78us/step - loss: 0.2379 - accuracy: 0.2853 - val_loss: 0.2040 - val_accuracy: 0.2912\n",
      "Epoch 173/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7252 - accuracy: 0.30 - ETA: 0s - loss: 0.2450 - accuracy: 0.28 - ETA: 0s - loss: 0.2714 - accuracy: 0.28 - ETA: 0s - loss: 0.2674 - accuracy: 0.29 - ETA: 0s - loss: 0.2437 - accuracy: 0.28 - 0s 83us/step - loss: 0.2341 - accuracy: 0.2894 - val_loss: 0.1908 - val_accuracy: 0.2812\n",
      "Epoch 174/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.0392 - accuracy: 0.200 - ETA: 0s - loss: 0.1706 - accuracy: 0.265 - ETA: 0s - loss: 0.2265 - accuracy: 0.27 - ETA: 0s - loss: 0.2409 - accuracy: 0.28 - ETA: 0s - loss: 0.2377 - accuracy: 0.28 - 0s 77us/step - loss: 0.2370 - accuracy: 0.2875 - val_loss: 0.1889 - val_accuracy: 0.2837\n",
      "Epoch 175/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.40 - ETA: 0s - loss: 0.3326 - accuracy: 0.32 - ETA: 0s - loss: 0.3016 - accuracy: 0.30 - ETA: 0s - loss: 0.2576 - accuracy: 0.29 - ETA: 0s - loss: 0.2272 - accuracy: 0.29 - 0s 78us/step - loss: 0.2275 - accuracy: 0.2916 - val_loss: 0.1901 - val_accuracy: 0.2912\n",
      "Epoch 176/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.10 - ETA: 0s - loss: 0.3341 - accuracy: 0.27 - ETA: 0s - loss: 0.2906 - accuracy: 0.28 - ETA: 0s - loss: 0.2655 - accuracy: 0.29 - 0s 74us/step - loss: 0.2320 - accuracy: 0.2912 - val_loss: 0.1856 - val_accuracy: 0.2887\n",
      "Epoch 177/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2381 - accuracy: 0.20 - ETA: 0s - loss: 0.2212 - accuracy: 0.29 - ETA: 0s - loss: 0.2171 - accuracy: 0.29 - ETA: 0s - loss: 0.2571 - accuracy: 0.29 - 0s 75us/step - loss: 0.2282 - accuracy: 0.2906 - val_loss: 0.1840 - val_accuracy: 0.2912\n",
      "Epoch 178/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.10 - ETA: 0s - loss: 0.0946 - accuracy: 0.27 - ETA: 0s - loss: 0.2588 - accuracy: 0.28 - ETA: 0s - loss: 0.2346 - accuracy: 0.28 - ETA: 0s - loss: 0.2285 - accuracy: 0.29 - 0s 79us/step - loss: 0.2270 - accuracy: 0.2903 - val_loss: 0.1845 - val_accuracy: 0.2925\n",
      "Epoch 179/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.30 - ETA: 0s - loss: 0.3492 - accuracy: 0.26 - ETA: 0s - loss: 0.2860 - accuracy: 0.28 - ETA: 0s - loss: 0.2805 - accuracy: 0.28 - ETA: 0s - loss: 0.2572 - accuracy: 0.29 - 0s 84us/step - loss: 0.2319 - accuracy: 0.2887 - val_loss: 0.1856 - val_accuracy: 0.2937\n",
      "Epoch 180/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9113 - accuracy: 0.30 - ETA: 0s - loss: 0.2853 - accuracy: 0.28 - ETA: 0s - loss: 0.2125 - accuracy: 0.29 - ETA: 0s - loss: 0.2103 - accuracy: 0.29 - ETA: 0s - loss: 0.2278 - accuracy: 0.29 - 0s 77us/step - loss: 0.2266 - accuracy: 0.2928 - val_loss: 0.1798 - val_accuracy: 0.2912\n",
      "Epoch 181/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7716 - accuracy: 0.200 - ETA: 0s - loss: 0.2851 - accuracy: 0.298 - ETA: 0s - loss: 0.2044 - accuracy: 0.29 - ETA: 0s - loss: 0.2158 - accuracy: 0.28 - 0s 74us/step - loss: 0.2279 - accuracy: 0.2909 - val_loss: 0.1802 - val_accuracy: 0.2937\n",
      "Epoch 182/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8327 - accuracy: 0.30 - ETA: 0s - loss: 0.2291 - accuracy: 0.28 - ETA: 0s - loss: 0.2377 - accuracy: 0.29 - ETA: 0s - loss: 0.2462 - accuracy: 0.29 - 0s 74us/step - loss: 0.2275 - accuracy: 0.2947 - val_loss: 0.1756 - val_accuracy: 0.2925\n",
      "Epoch 183/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.1919 - accuracy: 0.30 - ETA: 0s - loss: 0.2317 - accuracy: 0.29 - ETA: 0s - loss: 0.2351 - accuracy: 0.29 - ETA: 0s - loss: 0.2272 - accuracy: 0.29 - 0s 73us/step - loss: 0.2268 - accuracy: 0.2903 - val_loss: 0.1758 - val_accuracy: 0.2937\n",
      "Epoch 184/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3122 - accuracy: 0.300 - ETA: 0s - loss: 0.2240 - accuracy: 0.286 - ETA: 0s - loss: 0.2094 - accuracy: 0.29 - ETA: 0s - loss: 0.2636 - accuracy: 0.29 - 0s 74us/step - loss: 0.2252 - accuracy: 0.2931 - val_loss: 0.1718 - val_accuracy: 0.2875\n",
      "Epoch 185/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8241 - accuracy: 0.200 - ETA: 0s - loss: 0.2828 - accuracy: 0.293 - ETA: 0s - loss: 0.2416 - accuracy: 0.29 - ETA: 0s - loss: 0.2677 - accuracy: 0.29 - ETA: 0s - loss: 0.2312 - accuracy: 0.29 - 0s 79us/step - loss: 0.2206 - accuracy: 0.2947 - val_loss: 0.1887 - val_accuracy: 0.2825\n",
      "Epoch 186/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1147 - accuracy: 0.20 - ETA: 0s - loss: 0.1999 - accuracy: 0.30 - ETA: 0s - loss: 0.1994 - accuracy: 0.30 - ETA: 0s - loss: 0.2082 - accuracy: 0.28 - ETA: 0s - loss: 0.2348 - accuracy: 0.29 - 0s 79us/step - loss: 0.2272 - accuracy: 0.2900 - val_loss: 0.1701 - val_accuracy: 0.2937\n",
      "Epoch 187/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5282 - accuracy: 0.70 - ETA: 0s - loss: 0.2471 - accuracy: 0.27 - ETA: 0s - loss: 0.1982 - accuracy: 0.26 - ETA: 0s - loss: 0.1729 - accuracy: 0.28 - ETA: 0s - loss: 0.2241 - accuracy: 0.28 - 0s 80us/step - loss: 0.2199 - accuracy: 0.2919 - val_loss: 0.1679 - val_accuracy: 0.2950\n",
      "Epoch 188/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0627 - accuracy: 0.30 - ETA: 0s - loss: 0.2538 - accuracy: 0.29 - ETA: 0s - loss: 0.1717 - accuracy: 0.29 - ETA: 0s - loss: 0.2129 - accuracy: 0.29 - ETA: 0s - loss: 0.2092 - accuracy: 0.29 - 0s 78us/step - loss: 0.2190 - accuracy: 0.2919 - val_loss: 0.2066 - val_accuracy: 0.3100\n",
      "Epoch 189/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7009 - accuracy: 0.40 - ETA: 0s - loss: 0.0150 - accuracy: 0.30 - ETA: 0s - loss: 0.1507 - accuracy: 0.30 - ETA: 0s - loss: 0.2122 - accuracy: 0.29 - ETA: 0s - loss: 0.2177 - accuracy: 0.29 - 0s 80us/step - loss: 0.2226 - accuracy: 0.2928 - val_loss: 0.1636 - val_accuracy: 0.2937\n",
      "Epoch 190/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9439 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.1616 - accuracy: 0.2895   - ETA: 0s - loss: 0.1946 - accuracy: 0.29 - ETA: 0s - loss: 0.1965 - accuracy: 0.29 - ETA: 0s - loss: 0.2176 - accuracy: 0.29 - 0s 79us/step - loss: 0.2122 - accuracy: 0.2928 - val_loss: 0.1732 - val_accuracy: 0.3063\n",
      "Epoch 191/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.20 - ETA: 0s - loss: 0.2192 - accuracy: 0.30 - ETA: 0s - loss: 0.2097 - accuracy: 0.30 - ETA: 0s - loss: 0.2297 - accuracy: 0.29 - ETA: 0s - loss: 0.2130 - accuracy: 0.29 - 0s 77us/step - loss: 0.2212 - accuracy: 0.2931 - val_loss: 0.1622 - val_accuracy: 0.2925\n",
      "Epoch 192/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2301 - accuracy: 0.400 - ETA: 0s - loss: 0.3016 - accuracy: 0.301 - ETA: 0s - loss: 0.2181 - accuracy: 0.29 - ETA: 0s - loss: 0.1985 - accuracy: 0.28 - 0s 73us/step - loss: 0.2156 - accuracy: 0.2950 - val_loss: 0.1618 - val_accuracy: 0.2937\n",
      "Epoch 193/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.60 - ETA: 0s - loss: 0.2315 - accuracy: 0.27 - ETA: 0s - loss: 0.2252 - accuracy: 0.29 - ETA: 0s - loss: 0.2064 - accuracy: 0.29 - ETA: 0s - loss: 0.2189 - accuracy: 0.29 - 0s 76us/step - loss: 0.2192 - accuracy: 0.2944 - val_loss: 0.1596 - val_accuracy: 0.2937\n",
      "Epoch 194/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1125 - accuracy: 0.300 - ETA: 0s - loss: 0.2142 - accuracy: 0.318 - ETA: 0s - loss: 0.2519 - accuracy: 0.28 - ETA: 0s - loss: 0.2203 - accuracy: 0.28 - 0s 71us/step - loss: 0.2157 - accuracy: 0.2922 - val_loss: 0.1607 - val_accuracy: 0.2950\n",
      "Epoch 195/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.40 - ETA: 0s - loss: 0.2679 - accuracy: 0.28 - ETA: 0s - loss: 0.2976 - accuracy: 0.29 - ETA: 0s - loss: 0.2698 - accuracy: 0.30 - ETA: 0s - loss: 0.2169 - accuracy: 0.29 - 0s 76us/step - loss: 0.2165 - accuracy: 0.2956 - val_loss: 0.1580 - val_accuracy: 0.2937\n",
      "Epoch 196/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6791 - accuracy: 0.100 - ETA: 0s - loss: 0.2367 - accuracy: 0.281 - ETA: 0s - loss: 0.2064 - accuracy: 0.29 - ETA: 0s - loss: 0.2177 - accuracy: 0.28 - 0s 71us/step - loss: 0.2175 - accuracy: 0.2934 - val_loss: 0.1610 - val_accuracy: 0.3050\n",
      "Epoch 197/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0824 - accuracy: 0.30 - ETA: 0s - loss: 0.2000 - accuracy: 0.30 - ETA: 0s - loss: 0.1887 - accuracy: 0.30 - ETA: 0s - loss: 0.1924 - accuracy: 0.29 - 0s 70us/step - loss: 0.2116 - accuracy: 0.2950 - val_loss: 0.2269 - val_accuracy: 0.3200\n",
      "Epoch 198/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9724 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.2459 - accuracy: 0.2861    - ETA: 0s - loss: 0.1417 - accuracy: 0.29 - ETA: 0s - loss: 0.1685 - accuracy: 0.29 - 0s 74us/step - loss: 0.2137 - accuracy: 0.2931 - val_loss: 0.1706 - val_accuracy: 0.3088\n",
      "Epoch 199/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.30 - ETA: 0s - loss: 0.2370 - accuracy: 0.30 - ETA: 0s - loss: 0.2392 - accuracy: 0.29 - ETA: 0s - loss: 0.2238 - accuracy: 0.29 - 0s 72us/step - loss: 0.2108 - accuracy: 0.2944 - val_loss: 0.1695 - val_accuracy: 0.3088\n",
      "Epoch 200/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5610 - accuracy: 0.500 - ETA: 0s - loss: 0.2055 - accuracy: 0.298 - ETA: 0s - loss: 0.2146 - accuracy: 0.29 - ETA: 0s - loss: 0.1651 - accuracy: 0.29 - 0s 73us/step - loss: 0.2159 - accuracy: 0.2928 - val_loss: 0.1525 - val_accuracy: 0.2950\n",
      "Epoch 201/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8964 - accuracy: 0.30 - ETA: 0s - loss: 0.0853 - accuracy: 0.29 - ETA: 0s - loss: 0.1292 - accuracy: 0.29 - ETA: 0s - loss: 0.2263 - accuracy: 0.29 - 0s 71us/step - loss: 0.2044 - accuracy: 0.2953 - val_loss: 0.1699 - val_accuracy: 0.3100\n",
      "Epoch 202/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6226 - accuracy: 0.200 - ETA: 0s - loss: 0.0311 - accuracy: 0.300 - ETA: 0s - loss: 0.1214 - accuracy: 0.29 - ETA: 0s - loss: 0.1992 - accuracy: 0.29 - 0s 74us/step - loss: 0.2129 - accuracy: 0.2975 - val_loss: 0.1818 - val_accuracy: 0.3113\n",
      "Epoch 203/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9022 - accuracy: 0.500 - ETA: 0s - loss: 0.2997 - accuracy: 0.311 - ETA: 0s - loss: 0.2103 - accuracy: 0.31 - ETA: 0s - loss: 0.2545 - accuracy: 0.31 - ETA: 0s - loss: 0.2120 - accuracy: 0.30 - 0s 77us/step - loss: 0.2094 - accuracy: 0.3009 - val_loss: 0.1527 - val_accuracy: 0.2937\n",
      "Epoch 204/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3575 - accuracy: 0.500 - ETA: 0s - loss: 0.2042 - accuracy: 0.297 - ETA: 0s - loss: 0.2029 - accuracy: 0.28 - ETA: 0s - loss: 0.1894 - accuracy: 0.29 - ETA: 0s - loss: 0.2211 - accuracy: 0.29 - 0s 77us/step - loss: 0.2152 - accuracy: 0.2959 - val_loss: 0.1496 - val_accuracy: 0.2937\n",
      "Epoch 205/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0431 - accuracy: 0.300 - ETA: 0s - loss: 0.1522 - accuracy: 0.307 - ETA: 0s - loss: 0.2510 - accuracy: 0.28 - ETA: 0s - loss: 0.2152 - accuracy: 0.29 - 0s 74us/step - loss: 0.2149 - accuracy: 0.2947 - val_loss: 0.1488 - val_accuracy: 0.2962\n",
      "Epoch 206/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6893 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.2213 - accuracy: 0.3325    - ETA: 0s - loss: 0.2287 - accuracy: 0.30 - ETA: 0s - loss: 0.1584 - accuracy: 0.30 - 0s 74us/step - loss: 0.2132 - accuracy: 0.2975 - val_loss: 0.1544 - val_accuracy: 0.3075\n",
      "Epoch 207/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1113 - accuracy: 0.100 - ETA: 0s - loss: 0.1685 - accuracy: 0.304 - ETA: 0s - loss: 0.1358 - accuracy: 0.29 - ETA: 0s - loss: 0.2106 - accuracy: 0.29 - ETA: 0s - loss: 0.2098 - accuracy: 0.29 - 0s 77us/step - loss: 0.2110 - accuracy: 0.2950 - val_loss: 0.1917 - val_accuracy: 0.3162\n",
      "Epoch 208/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3500 - accuracy: 0.400 - ETA: 0s - loss: 0.1789 - accuracy: 0.284 - ETA: 0s - loss: 0.1836 - accuracy: 0.28 - ETA: 0s - loss: 0.2137 - accuracy: 0.29 - 0s 74us/step - loss: 0.2103 - accuracy: 0.2978 - val_loss: 0.1466 - val_accuracy: 0.2975\n",
      "Epoch 209/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8201 - accuracy: 0.20 - ETA: 0s - loss: 0.3670 - accuracy: 0.30 - ETA: 0s - loss: 0.3114 - accuracy: 0.30 - ETA: 0s - loss: 0.2487 - accuracy: 0.29 - 0s 74us/step - loss: 0.2103 - accuracy: 0.2972 - val_loss: 0.1468 - val_accuracy: 0.2988\n",
      "Epoch 210/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.60 - ETA: 0s - loss: 0.1359 - accuracy: 0.30 - ETA: 0s - loss: 0.2304 - accuracy: 0.29 - ETA: 0s - loss: 0.2109 - accuracy: 0.29 - 0s 75us/step - loss: 0.2106 - accuracy: 0.2981 - val_loss: 0.1457 - val_accuracy: 0.2988\n",
      "Epoch 211/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7616 - accuracy: 0.200 - ETA: 0s - loss: 0.2376 - accuracy: 0.281 - ETA: 0s - loss: 0.2595 - accuracy: 0.29 - ETA: 0s - loss: 0.2098 - accuracy: 0.29 - 0s 71us/step - loss: 0.2104 - accuracy: 0.2984 - val_loss: 0.1683 - val_accuracy: 0.3100\n",
      "Epoch 212/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9186 - accuracy: 0.20 - ETA: 0s - loss: 0.3722 - accuracy: 0.31 - ETA: 0s - loss: 0.2475 - accuracy: 0.30 - ETA: 0s - loss: 0.1991 - accuracy: 0.30 - 0s 75us/step - loss: 0.2069 - accuracy: 0.3006 - val_loss: 0.1480 - val_accuracy: 0.3013\n",
      "Epoch 213/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4341 - accuracy: 0.50 - ETA: 0s - loss: 0.0984 - accuracy: 0.30 - ETA: 0s - loss: 0.1736 - accuracy: 0.29 - ETA: 0s - loss: 0.2496 - accuracy: 0.30 - 0s 74us/step - loss: 0.2063 - accuracy: 0.3003 - val_loss: 0.1432 - val_accuracy: 0.3000\n",
      "Epoch 214/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8500 - accuracy: 0.300 - ETA: 0s - loss: 0.1233 - accuracy: 0.288 - ETA: 0s - loss: 0.1339 - accuracy: 0.29 - ETA: 0s - loss: 0.1772 - accuracy: 0.30 - 0s 71us/step - loss: 0.2101 - accuracy: 0.2981 - val_loss: 0.1450 - val_accuracy: 0.2912\n",
      "Epoch 215/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.0423 - accuracy: 0.40 - ETA: 0s - loss: 0.2644 - accuracy: 0.29 - ETA: 0s - loss: 0.2047 - accuracy: 0.28 - ETA: 0s - loss: 0.2516 - accuracy: 0.29 - ETA: 0s - loss: 0.2238 - accuracy: 0.29 - 0s 76us/step - loss: 0.2137 - accuracy: 0.2966 - val_loss: 0.1489 - val_accuracy: 0.3075\n",
      "Epoch 216/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.30 - ETA: 0s - loss: 0.2033 - accuracy: 0.27 - ETA: 0s - loss: 0.1923 - accuracy: 0.29 - ETA: 0s - loss: 0.1690 - accuracy: 0.29 - 0s 74us/step - loss: 0.2111 - accuracy: 0.2991 - val_loss: 0.1905 - val_accuracy: 0.3175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6907 - accuracy: 0.20 - ETA: 0s - loss: 0.3387 - accuracy: 0.30 - ETA: 0s - loss: 0.2186 - accuracy: 0.29 - ETA: 0s - loss: 0.2119 - accuracy: 0.29 - 0s 74us/step - loss: 0.2054 - accuracy: 0.2991 - val_loss: 0.1595 - val_accuracy: 0.3088\n",
      "Epoch 218/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1485 - accuracy: 0.200 - ETA: 0s - loss: 0.1113 - accuracy: 0.286 - ETA: 0s - loss: 0.1146 - accuracy: 0.27 - ETA: 0s - loss: 0.1666 - accuracy: 0.29 - 0s 74us/step - loss: 0.2071 - accuracy: 0.2975 - val_loss: 0.1387 - val_accuracy: 0.2962\n",
      "Epoch 219/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7475 - accuracy: 0.400 - ETA: 0s - loss: 0.2671 - accuracy: 0.306 - ETA: 0s - loss: 0.2533 - accuracy: 0.29 - ETA: 0s - loss: 0.2232 - accuracy: 0.29 - 0s 72us/step - loss: 0.2118 - accuracy: 0.2978 - val_loss: 0.1393 - val_accuracy: 0.2937\n",
      "Epoch 220/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7574 - accuracy: 0.500 - ETA: 0s - loss: 0.2720 - accuracy: 0.297 - ETA: 0s - loss: 0.2625 - accuracy: 0.30 - ETA: 0s - loss: 0.2449 - accuracy: 0.30 - ETA: 0s - loss: 0.2123 - accuracy: 0.29 - 0s 78us/step - loss: 0.2101 - accuracy: 0.2975 - val_loss: 0.1401 - val_accuracy: 0.2975\n",
      "Epoch 221/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.40 - ETA: 0s - loss: 0.2136 - accuracy: 0.30 - ETA: 0s - loss: 0.1810 - accuracy: 0.29 - ETA: 0s - loss: 0.2106 - accuracy: 0.29 - ETA: 0s - loss: 0.2180 - accuracy: 0.29 - 0s 77us/step - loss: 0.2114 - accuracy: 0.2984 - val_loss: 0.1399 - val_accuracy: 0.2937\n",
      "Epoch 222/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6826 - accuracy: 0.40 - ETA: 0s - loss: 0.2486 - accuracy: 0.29 - ETA: 0s - loss: 0.3263 - accuracy: 0.30 - ETA: 0s - loss: 0.2547 - accuracy: 0.29 - 0s 70us/step - loss: 0.2047 - accuracy: 0.2969 - val_loss: 0.1416 - val_accuracy: 0.3038\n",
      "Epoch 223/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0065 - accuracy: 0.200 - ETA: 0s - loss: 0.1134 - accuracy: 0.291 - ETA: 0s - loss: 0.0880 - accuracy: 0.31 - ETA: 0s - loss: 0.1653 - accuracy: 0.30 - ETA: 0s - loss: 0.2083 - accuracy: 0.30 - 0s 78us/step - loss: 0.2039 - accuracy: 0.3028 - val_loss: 0.1406 - val_accuracy: 0.3013\n",
      "Epoch 224/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8186 - accuracy: 0.100 - ETA: 0s - loss: 0.2711 - accuracy: 0.289 - ETA: 0s - loss: 0.1941 - accuracy: 0.29 - ETA: 0s - loss: 0.2109 - accuracy: 0.29 - ETA: 0s - loss: 0.2190 - accuracy: 0.29 - 0s 77us/step - loss: 0.2072 - accuracy: 0.2984 - val_loss: 0.1387 - val_accuracy: 0.3000\n",
      "Epoch 225/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.20 - ETA: 0s - loss: 0.2575 - accuracy: 0.28 - ETA: 0s - loss: 0.2322 - accuracy: 0.29 - ETA: 0s - loss: 0.1934 - accuracy: 0.29 - 0s 71us/step - loss: 0.2076 - accuracy: 0.3019 - val_loss: 0.1365 - val_accuracy: 0.2962\n",
      "Epoch 226/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6298 - accuracy: 0.40 - ETA: 0s - loss: 0.2543 - accuracy: 0.31 - ETA: 0s - loss: 0.2585 - accuracy: 0.31 - ETA: 0s - loss: 0.2501 - accuracy: 0.30 - 0s 71us/step - loss: 0.2087 - accuracy: 0.2969 - val_loss: 0.1360 - val_accuracy: 0.2962\n",
      "Epoch 227/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0740 - accuracy: 0.400 - ETA: 0s - loss: 0.1596 - accuracy: 0.311 - ETA: 0s - loss: 0.0755 - accuracy: 0.30 - ETA: 0s - loss: 0.1885 - accuracy: 0.30 - 0s 71us/step - loss: 0.2085 - accuracy: 0.2984 - val_loss: 0.1601 - val_accuracy: 0.3113\n",
      "Epoch 228/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3733 - accuracy: 0.20 - ETA: 0s - loss: 0.1690 - accuracy: 0.30 - ETA: 0s - loss: 0.1696 - accuracy: 0.31 - ETA: 0s - loss: 0.1462 - accuracy: 0.30 - ETA: 0s - loss: 0.2054 - accuracy: 0.29 - 0s 76us/step - loss: 0.2058 - accuracy: 0.2981 - val_loss: 0.1600 - val_accuracy: 0.3125\n",
      "Epoch 229/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8989 - accuracy: 0.50 - ETA: 0s - loss: 0.1821 - accuracy: 0.30 - ETA: 0s - loss: 0.2135 - accuracy: 0.29 - ETA: 0s - loss: 0.1962 - accuracy: 0.30 - 0s 70us/step - loss: 0.2103 - accuracy: 0.3034 - val_loss: 0.1405 - val_accuracy: 0.3038\n",
      "Epoch 230/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6749 - accuracy: 0.400 - ETA: 0s - loss: 0.2440 - accuracy: 0.295 - ETA: 0s - loss: 0.1230 - accuracy: 0.29 - ETA: 0s - loss: 0.1691 - accuracy: 0.29 - 0s 71us/step - loss: 0.2055 - accuracy: 0.2956 - val_loss: 0.1435 - val_accuracy: 0.3075\n",
      "Epoch 231/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3317 - accuracy: 0.200 - ETA: 0s - loss: 0.1053 - accuracy: 0.296 - ETA: 0s - loss: 0.1527 - accuracy: 0.30 - ETA: 0s - loss: 0.2103 - accuracy: 0.30 - 0s 71us/step - loss: 0.2107 - accuracy: 0.3025 - val_loss: 0.1342 - val_accuracy: 0.2962\n",
      "Epoch 232/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0629 - accuracy: 0.100 - ETA: 0s - loss: 0.0311 - accuracy: 0.273 - ETA: 0s - loss: 0.1297 - accuracy: 0.29 - ETA: 0s - loss: 0.1753 - accuracy: 0.29 - 0s 71us/step - loss: 0.2051 - accuracy: 0.2969 - val_loss: 0.1947 - val_accuracy: 0.3212\n",
      "Epoch 233/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7505 - accuracy: 0.30 - ETA: 0s - loss: 0.2593 - accuracy: 0.31 - ETA: 0s - loss: 0.1917 - accuracy: 0.30 - ETA: 0s - loss: 0.2357 - accuracy: 0.29 - 0s 72us/step - loss: 0.2117 - accuracy: 0.2972 - val_loss: 0.1372 - val_accuracy: 0.3038\n",
      "Epoch 234/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.40 - ETA: 0s - loss: 0.0873 - accuracy: 0.29 - ETA: 0s - loss: 0.2034 - accuracy: 0.29 - ETA: 0s - loss: 0.2095 - accuracy: 0.29 - 0s 71us/step - loss: 0.2001 - accuracy: 0.2975 - val_loss: 0.1406 - val_accuracy: 0.3075\n",
      "Epoch 235/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5882 - accuracy: 0.100 - ETA: 0s - loss: 0.1507 - accuracy: 0.283 - ETA: 0s - loss: 0.1552 - accuracy: 0.29 - ETA: 0s - loss: 0.1569 - accuracy: 0.29 - 0s 72us/step - loss: 0.2011 - accuracy: 0.3009 - val_loss: 0.1337 - val_accuracy: 0.3025\n",
      "Epoch 236/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1878 - accuracy: 0.20 - ETA: 0s - loss: 0.3471 - accuracy: 0.30 - ETA: 0s - loss: 0.2019 - accuracy: 0.30 - ETA: 0s - loss: 0.2175 - accuracy: 0.30 - 0s 70us/step - loss: 0.2077 - accuracy: 0.2991 - val_loss: 0.1482 - val_accuracy: 0.3088\n",
      "Epoch 237/500\n",
      "3200/3200 [==============================] - ETA: 2s - loss: 0.2584 - accuracy: 0.20 - ETA: 0s - loss: 0.1742 - accuracy: 0.31 - ETA: 0s - loss: 0.2396 - accuracy: 0.30 - ETA: 0s - loss: 0.2432 - accuracy: 0.30 - 0s 73us/step - loss: 0.2094 - accuracy: 0.3006 - val_loss: 0.1328 - val_accuracy: 0.2988\n",
      "Epoch 238/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4547 - accuracy: 0.20 - ETA: 0s - loss: 0.2450 - accuracy: 0.28 - ETA: 0s - loss: 0.1693 - accuracy: 0.29 - ETA: 0s - loss: 0.2141 - accuracy: 0.29 - 0s 67us/step - loss: 0.2022 - accuracy: 0.3006 - val_loss: 0.1395 - val_accuracy: 0.2925\n",
      "Epoch 239/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.10 - ETA: 0s - loss: 0.2019 - accuracy: 0.31 - ETA: 0s - loss: 0.1881 - accuracy: 0.31 - ETA: 0s - loss: 0.1797 - accuracy: 0.29 - 0s 68us/step - loss: 0.1998 - accuracy: 0.2966 - val_loss: 0.1660 - val_accuracy: 0.3225\n",
      "Epoch 240/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0603 - accuracy: 0.30 - ETA: 0s - loss: 0.2072 - accuracy: 0.30 - ETA: 0s - loss: 0.2202 - accuracy: 0.29 - ETA: 0s - loss: 0.2615 - accuracy: 0.30 - 0s 73us/step - loss: 0.2080 - accuracy: 0.3019 - val_loss: 0.1323 - val_accuracy: 0.2988\n",
      "Epoch 241/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.3935 - accuracy: 0.100 - ETA: 0s - loss: 0.3582 - accuracy: 0.295 - ETA: 0s - loss: 0.2843 - accuracy: 0.29 - ETA: 0s - loss: 0.2273 - accuracy: 0.29 - 0s 71us/step - loss: 0.2100 - accuracy: 0.2994 - val_loss: 0.1320 - val_accuracy: 0.2988\n",
      "Epoch 242/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 2.0113 - accuracy: 0.40 - ETA: 0s - loss: 0.4486 - accuracy: 0.27 - ETA: 0s - loss: 0.2749 - accuracy: 0.29 - ETA: 0s - loss: 0.2363 - accuracy: 0.30 - 0s 71us/step - loss: 0.2012 - accuracy: 0.3025 - val_loss: 0.1322 - val_accuracy: 0.3025\n",
      "Epoch 243/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.7820 - accuracy: 0.30 - ETA: 0s - loss: 0.2943 - accuracy: 0.29 - ETA: 0s - loss: 0.2587 - accuracy: 0.30 - ETA: 0s - loss: 0.2219 - accuracy: 0.30 - 0s 70us/step - loss: 0.2000 - accuracy: 0.3034 - val_loss: 0.1305 - val_accuracy: 0.2988\n",
      "Epoch 244/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9255 - accuracy: 0.400 - ETA: 0s - loss: 0.2014 - accuracy: 0.295 - ETA: 0s - loss: 0.1779 - accuracy: 0.29 - ETA: 0s - loss: 0.2440 - accuracy: 0.30 - 0s 71us/step - loss: 0.2020 - accuracy: 0.2978 - val_loss: 0.1304 - val_accuracy: 0.3000\n",
      "Epoch 245/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5035 - accuracy: 0.500 - ETA: 0s - loss: 0.2491 - accuracy: 0.315 - ETA: 0s - loss: 0.2382 - accuracy: 0.31 - ETA: 0s - loss: 0.2048 - accuracy: 0.30 - 0s 71us/step - loss: 0.2063 - accuracy: 0.2994 - val_loss: 0.1296 - val_accuracy: 0.2988\n",
      "Epoch 246/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7454 - accuracy: 0.300 - ETA: 0s - loss: 0.1892 - accuracy: 0.270 - ETA: 0s - loss: 0.1738 - accuracy: 0.29 - ETA: 0s - loss: 0.1656 - accuracy: 0.30 - 0s 71us/step - loss: 0.2071 - accuracy: 0.2997 - val_loss: 0.1347 - val_accuracy: 0.3075\n",
      "Epoch 247/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.30 - ETA: 0s - loss: 0.1626 - accuracy: 0.30 - ETA: 0s - loss: 0.2670 - accuracy: 0.29 - ETA: 0s - loss: 0.2756 - accuracy: 0.30 - 0s 70us/step - loss: 0.2054 - accuracy: 0.3028 - val_loss: 0.1317 - val_accuracy: 0.2925\n",
      "Epoch 248/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.1724 - accuracy: 0.600 - ETA: 0s - loss: 0.2444 - accuracy: 0.278 - ETA: 0s - loss: 0.2146 - accuracy: 0.29 - ETA: 0s - loss: 0.1710 - accuracy: 0.29 - 0s 69us/step - loss: 0.2007 - accuracy: 0.3019 - val_loss: 0.1672 - val_accuracy: 0.3237\n",
      "Epoch 249/500\n",
      "3200/3200 [==============================] - ETA: 4s - loss: 1.5317 - accuracy: 0.20 - ETA: 0s - loss: 0.3222 - accuracy: 0.33 - ETA: 0s - loss: 0.2266 - accuracy: 0.31 - ETA: 0s - loss: 0.1742 - accuracy: 0.30 - 0s 76us/step - loss: 0.2021 - accuracy: 0.3031 - val_loss: 0.1292 - val_accuracy: 0.3038\n",
      "Epoch 250/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5208 - accuracy: 0.50 - ETA: 0s - loss: 0.2510 - accuracy: 0.30 - ETA: 0s - loss: 0.2033 - accuracy: 0.30 - ETA: 0s - loss: 0.2468 - accuracy: 0.30 - 0s 71us/step - loss: 0.2036 - accuracy: 0.3003 - val_loss: 0.1300 - val_accuracy: 0.2925\n",
      "Epoch 251/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6916 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.1975 - accuracy: 0.3000   - ETA: 0s - loss: 0.1949 - accuracy: 0.27 - ETA: 0s - loss: 0.1901 - accuracy: 0.29 - ETA: 0s - loss: 0.1989 - accuracy: 0.30 - 0s 76us/step - loss: 0.2029 - accuracy: 0.3009 - val_loss: 0.1285 - val_accuracy: 0.2950\n",
      "Epoch 252/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2744 - accuracy: 0.100 - ETA: 0s - loss: 0.2111 - accuracy: 0.291 - ETA: 0s - loss: 0.1832 - accuracy: 0.30 - ETA: 0s - loss: 0.1704 - accuracy: 0.29 - ETA: 0s - loss: 0.2087 - accuracy: 0.30 - 0s 83us/step - loss: 0.2074 - accuracy: 0.2984 - val_loss: 0.1317 - val_accuracy: 0.3088\n",
      "Epoch 253/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4056 - accuracy: 0.400 - ETA: 0s - loss: 0.2174 - accuracy: 0.302 - ETA: 0s - loss: 0.2209 - accuracy: 0.29 - ETA: 0s - loss: 0.2208 - accuracy: 0.30 - 0s 75us/step - loss: 0.2025 - accuracy: 0.2994 - val_loss: 0.1372 - val_accuracy: 0.3075\n",
      "Epoch 254/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4039 - accuracy: 0.30 - ETA: 0s - loss: -0.0341 - accuracy: 0.277 - ETA: 0s - loss: 0.0950 - accuracy: 0.281 - ETA: 0s - loss: 0.1324 - accuracy: 0.29 - ETA: 0s - loss: 0.2027 - accuracy: 0.29 - 0s 79us/step - loss: 0.2015 - accuracy: 0.3003 - val_loss: 0.1309 - val_accuracy: 0.3088\n",
      "Epoch 255/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.50 - ETA: 0s - loss: 0.2989 - accuracy: 0.30 - ETA: 0s - loss: 0.2414 - accuracy: 0.30 - ETA: 0s - loss: 0.1984 - accuracy: 0.29 - ETA: 0s - loss: 0.2180 - accuracy: 0.29 - 0s 81us/step - loss: 0.2034 - accuracy: 0.2997 - val_loss: 0.1268 - val_accuracy: 0.2950\n",
      "Epoch 256/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.4910 - accuracy: 0.300 - ETA: 0s - loss: 0.2738 - accuracy: 0.291 - ETA: 0s - loss: 0.2305 - accuracy: 0.30 - ETA: 0s - loss: 0.2001 - accuracy: 0.30 - 0s 75us/step - loss: 0.2066 - accuracy: 0.2972 - val_loss: 0.1534 - val_accuracy: 0.3187\n",
      "Epoch 257/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8164 - accuracy: 0.30 - ETA: 0s - loss: 0.2423 - accuracy: 0.34 - ETA: 0s - loss: 0.2252 - accuracy: 0.31 - ETA: 0s - loss: 0.2484 - accuracy: 0.31 - ETA: 0s - loss: 0.2015 - accuracy: 0.30 - 0s 85us/step - loss: 0.2069 - accuracy: 0.3016 - val_loss: 0.1293 - val_accuracy: 0.3075\n",
      "Epoch 258/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.9387 - accuracy: 0.30 - ETA: 0s - loss: 0.1208 - accuracy: 0.29 - ETA: 0s - loss: 0.2151 - accuracy: 0.29 - ETA: 0s - loss: 0.2344 - accuracy: 0.29 - 0s 70us/step - loss: 0.2042 - accuracy: 0.2991 - val_loss: 0.1290 - val_accuracy: 0.2937\n",
      "Epoch 259/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.40 - ETA: 0s - loss: 0.1665 - accuracy: 0.27 - ETA: 0s - loss: 0.1735 - accuracy: 0.28 - ETA: 0s - loss: 0.2051 - accuracy: 0.29 - 0s 74us/step - loss: 0.2043 - accuracy: 0.2988 - val_loss: 0.1311 - val_accuracy: 0.3075\n",
      "Epoch 260/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8423 - accuracy: 0.200 - ETA: 0s - loss: 0.3018 - accuracy: 0.300 - ETA: 0s - loss: 0.2235 - accuracy: 0.30 - ETA: 0s - loss: 0.2136 - accuracy: 0.30 - 0s 73us/step - loss: 0.2035 - accuracy: 0.3022 - val_loss: 0.1249 - val_accuracy: 0.3038\n",
      "Epoch 261/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6663 - accuracy: 0.300 - ETA: 0s - loss: 0.1787 - accuracy: 0.309 - ETA: 0s - loss: 0.2031 - accuracy: 0.28 - ETA: 0s - loss: 0.2014 - accuracy: 0.29 - 0s 72us/step - loss: 0.2034 - accuracy: 0.3028 - val_loss: 0.1256 - val_accuracy: 0.3050\n",
      "Epoch 262/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0941 - accuracy: 0.100 - ETA: 0s - loss: 0.1165 - accuracy: 0.276 - ETA: 0s - loss: 0.1950 - accuracy: 0.28 - ETA: 0s - loss: 0.2239 - accuracy: 0.29 - 0s 71us/step - loss: 0.2067 - accuracy: 0.2972 - val_loss: 0.1470 - val_accuracy: 0.3137\n",
      "Epoch 263/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.50 - ETA: 0s - loss: 0.2038 - accuracy: 0.30 - ETA: 0s - loss: 0.2139 - accuracy: 0.30 - ETA: 0s - loss: 0.2085 - accuracy: 0.30 - 0s 70us/step - loss: 0.1993 - accuracy: 0.3022 - val_loss: 0.1239 - val_accuracy: 0.2950\n",
      "Epoch 264/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.3597 - accuracy: 0.400 - ETA: 0s - loss: 0.2130 - accuracy: 0.295 - ETA: 0s - loss: 0.2320 - accuracy: 0.29 - ETA: 0s - loss: 0.2147 - accuracy: 0.29 - 0s 70us/step - loss: 0.2042 - accuracy: 0.2978 - val_loss: 0.1225 - val_accuracy: 0.2950\n",
      "Epoch 265/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.40 - ETA: 0s - loss: 0.0146 - accuracy: 0.29 - ETA: 0s - loss: 0.2018 - accuracy: 0.31 - ETA: 0s - loss: 0.1996 - accuracy: 0.29 - 0s 71us/step - loss: 0.1922 - accuracy: 0.3053 - val_loss: 0.1297 - val_accuracy: 0.2912\n",
      "Epoch 266/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4166 - accuracy: 0.50 - ETA: 0s - loss: 0.2768 - accuracy: 0.29 - ETA: 0s - loss: 0.2287 - accuracy: 0.29 - ETA: 0s - loss: 0.2146 - accuracy: 0.30 - 0s 71us/step - loss: 0.2076 - accuracy: 0.2978 - val_loss: 0.1237 - val_accuracy: 0.3050\n",
      "Epoch 267/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6781 - accuracy: 0.30 - ETA: 0s - loss: 0.0330 - accuracy: 0.29 - ETA: 0s - loss: 0.1236 - accuracy: 0.29 - ETA: 0s - loss: 0.1966 - accuracy: 0.29 - 0s 71us/step - loss: 0.1960 - accuracy: 0.2988 - val_loss: 0.1248 - val_accuracy: 0.3063\n",
      "Epoch 268/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4700 - accuracy: 0.30 - ETA: 0s - loss: 0.0978 - accuracy: 0.30 - ETA: 0s - loss: 0.1290 - accuracy: 0.30 - ETA: 0s - loss: 0.2050 - accuracy: 0.29 - 0s 71us/step - loss: 0.1983 - accuracy: 0.2969 - val_loss: 0.1317 - val_accuracy: 0.3050\n",
      "Epoch 269/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.2466 - accuracy: 0.3131   - ETA: 0s - loss: 0.2942 - accuracy: 0.30 - ETA: 0s - loss: 0.2403 - accuracy: 0.30 - 0s 72us/step - loss: 0.2055 - accuracy: 0.3022 - val_loss: 0.1218 - val_accuracy: 0.2950\n",
      "Epoch 270/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.7555 - accuracy: 0.20 - ETA: 0s - loss: 0.0140 - accuracy: 0.29 - ETA: 0s - loss: 0.1556 - accuracy: 0.29 - ETA: 0s - loss: 0.1724 - accuracy: 0.29 - 0s 71us/step - loss: 0.2021 - accuracy: 0.2991 - val_loss: 0.1265 - val_accuracy: 0.3063\n",
      "Epoch 271/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.9384 - accuracy: 0.40 - ETA: 0s - loss: 0.2586 - accuracy: 0.32 - ETA: 0s - loss: 0.2694 - accuracy: 0.31 - ETA: 0s - loss: 0.2302 - accuracy: 0.31 - 0s 72us/step - loss: 0.1912 - accuracy: 0.3025 - val_loss: 0.1233 - val_accuracy: 0.2950\n",
      "Epoch 272/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1134 - accuracy: 0.30 - ETA: 0s - loss: 0.1543 - accuracy: 0.29 - ETA: 0s - loss: 0.1247 - accuracy: 0.29 - ETA: 0s - loss: 0.1989 - accuracy: 0.29 - 0s 71us/step - loss: 0.2031 - accuracy: 0.2988 - val_loss: 0.1264 - val_accuracy: 0.3075\n",
      "Epoch 273/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.30 - ETA: 0s - loss: 0.3168 - accuracy: 0.28 - ETA: 0s - loss: 0.2113 - accuracy: 0.29 - ETA: 0s - loss: 0.2000 - accuracy: 0.29 - 0s 74us/step - loss: 0.1945 - accuracy: 0.3028 - val_loss: 0.1263 - val_accuracy: 0.2925\n",
      "Epoch 274/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.3635 - accuracy: 0.300 - ETA: 0s - loss: 0.2859 - accuracy: 0.321 - ETA: 0s - loss: 0.2895 - accuracy: 0.31 - ETA: 0s - loss: 0.1951 - accuracy: 0.30 - 0s 73us/step - loss: 0.1992 - accuracy: 0.2988 - val_loss: 0.1204 - val_accuracy: 0.2988\n",
      "Epoch 275/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.30 - ETA: 0s - loss: 0.2922 - accuracy: 0.28 - ETA: 0s - loss: 0.1534 - accuracy: 0.29 - ETA: 0s - loss: 0.1527 - accuracy: 0.29 - 0s 72us/step - loss: 0.1966 - accuracy: 0.2969 - val_loss: 0.1885 - val_accuracy: 0.3237\n",
      "Epoch 276/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4848 - accuracy: 0.100 - ETA: 0s - loss: 0.2352 - accuracy: 0.277 - ETA: 0s - loss: 0.2303 - accuracy: 0.28 - ETA: 0s - loss: 0.2156 - accuracy: 0.29 - 0s 71us/step - loss: 0.2029 - accuracy: 0.2988 - val_loss: 0.1360 - val_accuracy: 0.3088\n",
      "Epoch 277/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1289 - accuracy: 0.40 - ETA: 0s - loss: 0.2280 - accuracy: 0.29 - ETA: 0s - loss: 0.1888 - accuracy: 0.29 - ETA: 0s - loss: 0.2024 - accuracy: 0.29 - 0s 71us/step - loss: 0.2004 - accuracy: 0.2962 - val_loss: 0.1442 - val_accuracy: 0.3075\n",
      "Epoch 278/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8546 - accuracy: 0.400 - ETA: 0s - loss: 0.1964 - accuracy: 0.309 - ETA: 0s - loss: 0.2816 - accuracy: 0.30 - ETA: 0s - loss: 0.2396 - accuracy: 0.30 - 0s 71us/step - loss: 0.1938 - accuracy: 0.3003 - val_loss: 0.1315 - val_accuracy: 0.2875\n",
      "Epoch 279/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4957 - accuracy: 0.30 - ETA: 0s - loss: 0.2405 - accuracy: 0.27 - ETA: 0s - loss: 0.2445 - accuracy: 0.30 - ETA: 0s - loss: 0.1879 - accuracy: 0.30 - 0s 70us/step - loss: 0.1977 - accuracy: 0.3006 - val_loss: 0.1214 - val_accuracy: 0.2937\n",
      "Epoch 280/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2925 - accuracy: 0.50 - ETA: 0s - loss: 0.3722 - accuracy: 0.28 - ETA: 0s - loss: 0.2549 - accuracy: 0.29 - ETA: 0s - loss: 0.1852 - accuracy: 0.30 - 0s 70us/step - loss: 0.1924 - accuracy: 0.3009 - val_loss: 0.1208 - val_accuracy: 0.3000\n",
      "Epoch 281/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.2081 - accuracy: 0.30 - ETA: 0s - loss: 0.1853 - accuracy: 0.32 - ETA: 0s - loss: 0.1362 - accuracy: 0.30 - ETA: 0s - loss: 0.1605 - accuracy: 0.30 - 0s 67us/step - loss: 0.1911 - accuracy: 0.2981 - val_loss: 0.1887 - val_accuracy: 0.3250\n",
      "Epoch 282/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0885 - accuracy: 0.40 - ETA: 0s - loss: 0.2030 - accuracy: 0.30 - ETA: 0s - loss: 0.2059 - accuracy: 0.31 - ETA: 0s - loss: 0.1696 - accuracy: 0.30 - 0s 73us/step - loss: 0.1956 - accuracy: 0.2978 - val_loss: 0.1438 - val_accuracy: 0.3088\n",
      "Epoch 283/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.9600 - accuracy: 0.20 - ETA: 0s - loss: 0.2376 - accuracy: 0.27 - ETA: 0s - loss: 0.2022 - accuracy: 0.30 - ETA: 0s - loss: 0.1714 - accuracy: 0.29 - 0s 71us/step - loss: 0.1969 - accuracy: 0.2972 - val_loss: 0.1343 - val_accuracy: 0.3113\n",
      "Epoch 284/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7304 - accuracy: 0.60 - ETA: 0s - loss: 0.2256 - accuracy: 0.30 - ETA: 0s - loss: 0.2078 - accuracy: 0.29 - ETA: 0s - loss: 0.1842 - accuracy: 0.28 - 0s 72us/step - loss: 0.1961 - accuracy: 0.2966 - val_loss: 0.1291 - val_accuracy: 0.3088\n",
      "Epoch 285/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8201 - accuracy: 0.20 - ETA: 0s - loss: 0.0354 - accuracy: 0.30 - ETA: 0s - loss: 0.1584 - accuracy: 0.29 - ETA: 0s - loss: 0.1871 - accuracy: 0.30 - 0s 71us/step - loss: 0.1933 - accuracy: 0.2978 - val_loss: 0.1242 - val_accuracy: 0.3050\n",
      "Epoch 286/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6400 - accuracy: 0.10 - ETA: 0s - loss: 0.2121 - accuracy: 0.29 - ETA: 0s - loss: 0.1809 - accuracy: 0.30 - ETA: 0s - loss: 0.1801 - accuracy: 0.30 - 0s 73us/step - loss: 0.1898 - accuracy: 0.2994 - val_loss: 0.1863 - val_accuracy: 0.3262\n",
      "Epoch 287/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.20 - ETA: 0s - loss: 0.1656 - accuracy: 0.29 - ETA: 0s - loss: 0.2108 - accuracy: 0.29 - ETA: 0s - loss: 0.1753 - accuracy: 0.29 - 0s 72us/step - loss: 0.1935 - accuracy: 0.3019 - val_loss: 0.1628 - val_accuracy: 0.3200\n",
      "Epoch 288/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6613 - accuracy: 0.10 - ETA: 0s - loss: 0.3074 - accuracy: 0.29 - ETA: 0s - loss: 0.3572 - accuracy: 0.30 - ETA: 0s - loss: 0.2595 - accuracy: 0.29 - 0s 72us/step - loss: 0.1864 - accuracy: 0.2981 - val_loss: 0.1195 - val_accuracy: 0.2912\n",
      "Epoch 289/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4716 - accuracy: 0.20 - ETA: 0s - loss: 0.2333 - accuracy: 0.27 - ETA: 0s - loss: 0.2537 - accuracy: 0.29 - ETA: 0s - loss: 0.2019 - accuracy: 0.29 - 0s 72us/step - loss: 0.1899 - accuracy: 0.2988 - val_loss: 0.1547 - val_accuracy: 0.3150\n",
      "Epoch 290/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.40 - ETA: 0s - loss: 0.0416 - accuracy: 0.30 - ETA: 0s - loss: 0.2038 - accuracy: 0.31 - ETA: 0s - loss: 0.1838 - accuracy: 0.30 - 0s 72us/step - loss: 0.1903 - accuracy: 0.2984 - val_loss: 0.1862 - val_accuracy: 0.3262\n",
      "Epoch 291/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3662 - accuracy: 0.10 - ETA: 0s - loss: 0.3647 - accuracy: 0.29 - ETA: 0s - loss: 0.2729 - accuracy: 0.30 - ETA: 0s - loss: 0.2354 - accuracy: 0.30 - 0s 72us/step - loss: 0.1837 - accuracy: 0.2975 - val_loss: 0.1209 - val_accuracy: 0.2900\n",
      "Epoch 292/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3710 - accuracy: 0.200 - ETA: 0s - loss: 0.3376 - accuracy: 0.307 - ETA: 0s - loss: 0.2079 - accuracy: 0.30 - ETA: 0s - loss: 0.2014 - accuracy: 0.30 - 0s 70us/step - loss: 0.1928 - accuracy: 0.2981 - val_loss: 0.1207 - val_accuracy: 0.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 293/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.3420 - accuracy: 0.300 - ETA: 0s - loss: 0.2563 - accuracy: 0.303 - ETA: 0s - loss: 0.1810 - accuracy: 0.30 - ETA: 0s - loss: 0.1489 - accuracy: 0.30 - 0s 72us/step - loss: 0.1942 - accuracy: 0.2975 - val_loss: 0.1203 - val_accuracy: 0.2900\n",
      "Epoch 294/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7583 - accuracy: 0.500 - ETA: 0s - loss: 0.0561 - accuracy: 0.291 - ETA: 0s - loss: 0.1351 - accuracy: 0.29 - ETA: 0s - loss: 0.1722 - accuracy: 0.29 - 0s 70us/step - loss: 0.1835 - accuracy: 0.2966 - val_loss: 0.1389 - val_accuracy: 0.3088\n",
      "Epoch 295/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0126 - accuracy: 0.200 - ETA: 0s - loss: 0.2539 - accuracy: 0.286 - ETA: 0s - loss: 0.1628 - accuracy: 0.28 - ETA: 0s - loss: 0.1700 - accuracy: 0.29 - 0s 74us/step - loss: 0.1928 - accuracy: 0.2966 - val_loss: 0.1266 - val_accuracy: 0.3050\n",
      "Epoch 296/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8199 - accuracy: 0.600 - ETA: 0s - loss: 0.1615 - accuracy: 0.293 - ETA: 0s - loss: 0.1647 - accuracy: 0.30 - ETA: 0s - loss: 0.2155 - accuracy: 0.30 - 0s 74us/step - loss: 0.1845 - accuracy: 0.2966 - val_loss: 0.1204 - val_accuracy: 0.2900\n",
      "Epoch 297/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3630 - accuracy: 0.30 - ETA: 0s - loss: 0.2798 - accuracy: 0.26 - ETA: 0s - loss: 0.1943 - accuracy: 0.28 - ETA: 0s - loss: 0.1577 - accuracy: 0.30 - 0s 74us/step - loss: 0.1901 - accuracy: 0.2962 - val_loss: 0.1237 - val_accuracy: 0.3050\n",
      "Epoch 298/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.30 - ETA: 0s - loss: 0.1334 - accuracy: 0.30 - ETA: 0s - loss: 0.1666 - accuracy: 0.29 - ETA: 0s - loss: 0.1934 - accuracy: 0.29 - 0s 73us/step - loss: 0.1864 - accuracy: 0.2969 - val_loss: 0.1191 - val_accuracy: 0.2988\n",
      "Epoch 299/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.1998 - accuracy: 0.3169   - ETA: 0s - loss: 0.1219 - accuracy: 0.30 - ETA: 0s - loss: 0.1375 - accuracy: 0.30 - 0s 74us/step - loss: 0.1901 - accuracy: 0.2988 - val_loss: 0.1285 - val_accuracy: 0.3063\n",
      "Epoch 300/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4380 - accuracy: 0.500 - ETA: 0s - loss: 0.1055 - accuracy: 0.309 - ETA: 0s - loss: 0.1462 - accuracy: 0.30 - ETA: 0s - loss: 0.1840 - accuracy: 0.29 - 0s 74us/step - loss: 0.1915 - accuracy: 0.2978 - val_loss: 0.1217 - val_accuracy: 0.2988\n",
      "Epoch 301/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2436 - accuracy: 0.500 - ETA: 0s - loss: 0.2138 - accuracy: 0.293 - ETA: 0s - loss: 0.2629 - accuracy: 0.29 - ETA: 0s - loss: 0.2026 - accuracy: 0.29 - 0s 74us/step - loss: 0.1781 - accuracy: 0.2981 - val_loss: 0.1525 - val_accuracy: 0.3137\n",
      "Epoch 302/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2380 - accuracy: 0.200 - ETA: 0s - loss: 0.1746 - accuracy: 0.309 - ETA: 0s - loss: 0.1279 - accuracy: 0.29 - ETA: 0s - loss: 0.1373 - accuracy: 0.29 - 0s 74us/step - loss: 0.1869 - accuracy: 0.2984 - val_loss: 0.1587 - val_accuracy: 0.3162\n",
      "Epoch 303/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9161 - accuracy: 0.200 - ETA: 0s - loss: 0.1056 - accuracy: 0.286 - ETA: 0s - loss: 0.1513 - accuracy: 0.28 - ETA: 0s - loss: 0.1556 - accuracy: 0.29 - 0s 74us/step - loss: 0.1834 - accuracy: 0.2984 - val_loss: 0.1439 - val_accuracy: 0.3100\n",
      "Epoch 304/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1436 - accuracy: 0.300 - ETA: 0s - loss: 0.1660 - accuracy: 0.314 - ETA: 0s - loss: 0.1518 - accuracy: 0.30 - ETA: 0s - loss: 0.1939 - accuracy: 0.29 - 0s 74us/step - loss: 0.1875 - accuracy: 0.2959 - val_loss: 0.1249 - val_accuracy: 0.3050\n",
      "Epoch 305/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3275 - accuracy: 0.10 - ETA: 0s - loss: 0.1456 - accuracy: 0.28 - ETA: 0s - loss: 0.1632 - accuracy: 0.28 - ETA: 0s - loss: 0.1995 - accuracy: 0.29 - 0s 74us/step - loss: 0.1850 - accuracy: 0.2944 - val_loss: 0.1288 - val_accuracy: 0.3075\n",
      "Epoch 306/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.30 - ETA: 0s - loss: 0.2523 - accuracy: 0.30 - ETA: 0s - loss: 0.1742 - accuracy: 0.30 - ETA: 0s - loss: 0.1447 - accuracy: 0.29 - 0s 74us/step - loss: 0.1846 - accuracy: 0.2975 - val_loss: 0.1975 - val_accuracy: 0.3338\n",
      "Epoch 307/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.20 - ETA: 0s - loss: 0.2864 - accuracy: 0.32 - ETA: 0s - loss: 0.2247 - accuracy: 0.31 - ETA: 0s - loss: 0.1968 - accuracy: 0.30 - 0s 73us/step - loss: 0.1800 - accuracy: 0.3009 - val_loss: 0.1181 - val_accuracy: 0.2900\n",
      "Epoch 308/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.4610 - accuracy: 0.100 - ETA: 0s - loss: 0.1332 - accuracy: 0.282 - ETA: 0s - loss: 0.1423 - accuracy: 0.29 - ETA: 0s - loss: 0.1807 - accuracy: 0.30 - 0s 73us/step - loss: 0.1834 - accuracy: 0.2959 - val_loss: 0.1180 - val_accuracy: 0.2975\n",
      "Epoch 309/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3989 - accuracy: 0.400 - ETA: 0s - loss: 0.3126 - accuracy: 0.284 - ETA: 0s - loss: 0.2585 - accuracy: 0.29 - ETA: 0s - loss: 0.1879 - accuracy: 0.30 - 0s 73us/step - loss: 0.1813 - accuracy: 0.2969 - val_loss: 0.1333 - val_accuracy: 0.3100\n",
      "Epoch 310/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.50 - ETA: 0s - loss: 0.1868 - accuracy: 0.30 - ETA: 0s - loss: 0.1251 - accuracy: 0.29 - ETA: 0s - loss: 0.1349 - accuracy: 0.29 - 0s 74us/step - loss: 0.1760 - accuracy: 0.2972 - val_loss: 0.1431 - val_accuracy: 0.3125\n",
      "Epoch 311/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.50 - ETA: 0s - loss: 0.2480 - accuracy: 0.29 - ETA: 0s - loss: 0.2367 - accuracy: 0.28 - ETA: 0s - loss: 0.1714 - accuracy: 0.29 - 0s 73us/step - loss: 0.1797 - accuracy: 0.2962 - val_loss: 0.1550 - val_accuracy: 0.3175\n",
      "Epoch 312/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7497 - accuracy: 0.500 - ETA: 0s - loss: 0.1615 - accuracy: 0.302 - ETA: 0s - loss: 0.1608 - accuracy: 0.28 - ETA: 0s - loss: 0.1806 - accuracy: 0.29 - 0s 74us/step - loss: 0.1797 - accuracy: 0.2997 - val_loss: 0.1152 - val_accuracy: 0.2950\n",
      "Epoch 313/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7235 - accuracy: 0.300 - ETA: 0s - loss: 0.0447 - accuracy: 0.302 - ETA: 0s - loss: 0.1690 - accuracy: 0.29 - ETA: 0s - loss: 0.1793 - accuracy: 0.29 - 0s 74us/step - loss: 0.1681 - accuracy: 0.2959 - val_loss: 0.1222 - val_accuracy: 0.2875\n",
      "Epoch 314/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2876 - accuracy: 0.300 - ETA: 0s - loss: 0.2125 - accuracy: 0.310 - ETA: 0s - loss: 0.2404 - accuracy: 0.29 - ETA: 0s - loss: 0.1633 - accuracy: 0.29 - 0s 73us/step - loss: 0.1805 - accuracy: 0.2937 - val_loss: 0.1152 - val_accuracy: 0.2988\n",
      "Epoch 315/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3579 - accuracy: 0.20 - ETA: 0s - loss: 0.1649 - accuracy: 0.27 - ETA: 0s - loss: 0.1343 - accuracy: 0.27 - ETA: 0s - loss: 0.1814 - accuracy: 0.29 - 0s 73us/step - loss: 0.1713 - accuracy: 0.2962 - val_loss: 0.1155 - val_accuracy: 0.3000\n",
      "Epoch 316/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3676 - accuracy: 0.40 - ETA: 0s - loss: 0.1332 - accuracy: 0.29 - ETA: 0s - loss: 0.1330 - accuracy: 0.29 - ETA: 0s - loss: 0.2077 - accuracy: 0.30 - 0s 73us/step - loss: 0.1726 - accuracy: 0.2988 - val_loss: 0.1132 - val_accuracy: 0.2975\n",
      "Epoch 317/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.0960 - accuracy: 0.300 - ETA: 0s - loss: -0.0090 - accuracy: 0.292 - ETA: 0s - loss: 0.0480 - accuracy: 0.290 - ETA: 0s - loss: 0.0794 - accuracy: 0.29 - ETA: 0s - loss: 0.1687 - accuracy: 0.29 - 0s 77us/step - loss: 0.1672 - accuracy: 0.2981 - val_loss: 0.1372 - val_accuracy: 0.3075\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: -1.5874 - accuracy: 0.300 - ETA: 0s - loss: 0.1325 - accuracy: 0.301 - ETA: 0s - loss: 0.1522 - accuracy: 0.29 - ETA: 0s - loss: 0.1866 - accuracy: 0.29 - ETA: 0s - loss: 0.1968 - accuracy: 0.29 - 0s 77us/step - loss: 0.1792 - accuracy: 0.2959 - val_loss: 0.1160 - val_accuracy: 0.2862\n",
      "Epoch 319/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4436 - accuracy: 0.300 - ETA: 0s - loss: 0.1962 - accuracy: 0.286 - ETA: 0s - loss: 0.1938 - accuracy: 0.29 - ETA: 0s - loss: 0.1485 - accuracy: 0.28 - ETA: 0s - loss: 0.1593 - accuracy: 0.29 - 0s 81us/step - loss: 0.1674 - accuracy: 0.2966 - val_loss: 0.1133 - val_accuracy: 0.3000\n",
      "Epoch 320/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0663 - accuracy: 0.400 - ETA: 0s - loss: 0.1031 - accuracy: 0.327 - ETA: 0s - loss: 0.1958 - accuracy: 0.30 - ETA: 0s - loss: 0.1482 - accuracy: 0.29 - ETA: 0s - loss: 0.1337 - accuracy: 0.29 - 0s 82us/step - loss: 0.1693 - accuracy: 0.2953 - val_loss: 0.1288 - val_accuracy: 0.3088\n",
      "Epoch 321/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2897 - accuracy: 0.200 - ETA: 0s - loss: 0.1715 - accuracy: 0.294 - ETA: 0s - loss: 0.1304 - accuracy: 0.29 - ETA: 0s - loss: 0.1451 - accuracy: 0.28 - ETA: 0s - loss: 0.1653 - accuracy: 0.29 - 0s 80us/step - loss: 0.1697 - accuracy: 0.2978 - val_loss: 0.1193 - val_accuracy: 0.3063\n",
      "Epoch 322/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1260 - accuracy: 0.40 - ETA: 0s - loss: 0.0346 - accuracy: 0.29 - ETA: 0s - loss: 0.1143 - accuracy: 0.30 - ETA: 0s - loss: 0.1443 - accuracy: 0.30 - ETA: 0s - loss: 0.1790 - accuracy: 0.29 - 0s 82us/step - loss: 0.1690 - accuracy: 0.2981 - val_loss: 0.1167 - val_accuracy: 0.3063\n",
      "Epoch 323/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2678 - accuracy: 0.500 - ETA: 0s - loss: 0.3727 - accuracy: 0.294 - ETA: 0s - loss: 0.1835 - accuracy: 0.30 - ETA: 0s - loss: 0.1895 - accuracy: 0.28 - ETA: 0s - loss: 0.1775 - accuracy: 0.29 - 0s 82us/step - loss: 0.1698 - accuracy: 0.2972 - val_loss: 0.1088 - val_accuracy: 0.2925\n",
      "Epoch 324/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.10 - ETA: 0s - loss: 0.1313 - accuracy: 0.29 - ETA: 0s - loss: 0.0955 - accuracy: 0.28 - ETA: 0s - loss: 0.1428 - accuracy: 0.28 - ETA: 0s - loss: 0.1565 - accuracy: 0.29 - 0s 82us/step - loss: 0.1686 - accuracy: 0.2959 - val_loss: 0.1165 - val_accuracy: 0.3063\n",
      "Epoch 325/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9433 - accuracy: 0.300 - ETA: 0s - loss: 0.1543 - accuracy: 0.320 - ETA: 0s - loss: 0.1239 - accuracy: 0.29 - ETA: 0s - loss: 0.1115 - accuracy: 0.29 - ETA: 0s - loss: 0.1631 - accuracy: 0.29 - 0s 79us/step - loss: 0.1704 - accuracy: 0.2978 - val_loss: 0.1099 - val_accuracy: 0.3038\n",
      "Epoch 326/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8116 - accuracy: 0.400 - ETA: 0s - loss: 0.0215 - accuracy: 0.273 - ETA: 0s - loss: 0.1041 - accuracy: 0.27 - ETA: 0s - loss: 0.1092 - accuracy: 0.29 - 0s 73us/step - loss: 0.1678 - accuracy: 0.2975 - val_loss: 0.1695 - val_accuracy: 0.3275\n",
      "Epoch 327/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8020 - accuracy: 0.200 - ETA: 0s - loss: 0.1077 - accuracy: 0.315 - ETA: 0s - loss: 0.1526 - accuracy: 0.31 - ETA: 0s - loss: 0.1857 - accuracy: 0.29 - 0s 73us/step - loss: 0.1719 - accuracy: 0.2969 - val_loss: 0.1335 - val_accuracy: 0.3125\n",
      "Epoch 328/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.8432 - accuracy: 0.10 - ETA: 0s - loss: 0.2236 - accuracy: 0.31 - ETA: 0s - loss: 0.2015 - accuracy: 0.30 - ETA: 0s - loss: 0.1949 - accuracy: 0.29 - 0s 73us/step - loss: 0.1652 - accuracy: 0.2972 - val_loss: 0.1076 - val_accuracy: 0.3038\n",
      "Epoch 329/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7855 - accuracy: 0.30 - ETA: 0s - loss: 0.1878 - accuracy: 0.28 - ETA: 0s - loss: 0.1472 - accuracy: 0.30 - ETA: 0s - loss: 0.1355 - accuracy: 0.29 - 0s 74us/step - loss: 0.1631 - accuracy: 0.2953 - val_loss: 0.1078 - val_accuracy: 0.3038\n",
      "Epoch 330/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.3849 - accuracy: 0.400 - ETA: 0s - loss: 0.0878 - accuracy: 0.304 - ETA: 0s - loss: 0.1113 - accuracy: 0.31 - ETA: 0s - loss: 0.1742 - accuracy: 0.29 - 0s 74us/step - loss: 0.1626 - accuracy: 0.2972 - val_loss: 0.1062 - val_accuracy: 0.3025\n",
      "Epoch 331/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.5104 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.2414 - accuracy: 0.2819    - ETA: 0s - loss: 0.2422 - accuracy: 0.29 - ETA: 0s - loss: 0.1740 - accuracy: 0.28 - 0s 73us/step - loss: 0.1618 - accuracy: 0.2988 - val_loss: 0.1059 - val_accuracy: 0.3025\n",
      "Epoch 332/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7030 - accuracy: 0.400 - ETA: 0s - loss: 0.2518 - accuracy: 0.306 - ETA: 0s - loss: 0.2287 - accuracy: 0.29 - ETA: 0s - loss: 0.1645 - accuracy: 0.28 - 0s 73us/step - loss: 0.1633 - accuracy: 0.2937 - val_loss: 0.1266 - val_accuracy: 0.3137\n",
      "Epoch 333/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5297 - accuracy: 0.300 - ETA: 0s - loss: 0.0683 - accuracy: 0.288 - ETA: 0s - loss: 0.1437 - accuracy: 0.29 - ETA: 0s - loss: 0.1850 - accuracy: 0.29 - 0s 73us/step - loss: 0.1705 - accuracy: 0.2991 - val_loss: 0.1062 - val_accuracy: 0.3038\n",
      "Epoch 334/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2034 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.1170 - accuracy: 0.2807    - ETA: 0s - loss: 0.1560 - accuracy: 0.30 - ETA: 0s - loss: 0.1739 - accuracy: 0.30 - ETA: 0s - loss: 0.1633 - accuracy: 0.29 - 0s 76us/step - loss: 0.1662 - accuracy: 0.2988 - val_loss: 0.1029 - val_accuracy: 0.3000\n",
      "Epoch 335/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7015 - accuracy: 0.400 - ETA: 0s - loss: 0.1070 - accuracy: 0.292 - ETA: 0s - loss: 0.1794 - accuracy: 0.29 - ETA: 0s - loss: 0.2026 - accuracy: 0.30 - ETA: 0s - loss: 0.1806 - accuracy: 0.29 - 0s 79us/step - loss: 0.1534 - accuracy: 0.2969 - val_loss: 0.1450 - val_accuracy: 0.2837\n",
      "Epoch 336/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.20 - ETA: 0s - loss: 0.1900 - accuracy: 0.29 - ETA: 0s - loss: 0.1553 - accuracy: 0.30 - ETA: 0s - loss: 0.1940 - accuracy: 0.29 - 0s 73us/step - loss: 0.1642 - accuracy: 0.2966 - val_loss: 0.1018 - val_accuracy: 0.2925\n",
      "Epoch 337/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7755 - accuracy: 0.500 - ETA: 0s - loss: 0.1839 - accuracy: 0.288 - ETA: 0s - loss: 0.2419 - accuracy: 0.29 - ETA: 0s - loss: 0.2051 - accuracy: 0.29 - 0s 73us/step - loss: 0.1662 - accuracy: 0.2966 - val_loss: 0.1044 - val_accuracy: 0.2887\n",
      "Epoch 338/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.0948 - accuracy: 0.30 - ETA: 0s - loss: 0.1425 - accuracy: 0.32 - ETA: 0s - loss: 0.1131 - accuracy: 0.30 - ETA: 0s - loss: 0.1753 - accuracy: 0.29 - 0s 71us/step - loss: 0.1587 - accuracy: 0.2962 - val_loss: 0.1123 - val_accuracy: 0.2862\n",
      "Epoch 339/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.8465 - accuracy: 0.10 - ETA: 0s - loss: 0.2877 - accuracy: 0.28 - ETA: 0s - loss: 0.1619 - accuracy: 0.28 - ETA: 0s - loss: 0.1458 - accuracy: 0.28 - ETA: 0s - loss: 0.1846 - accuracy: 0.29 - 0s 77us/step - loss: 0.1554 - accuracy: 0.2981 - val_loss: 0.1349 - val_accuracy: 0.2837\n",
      "Epoch 340/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.1593 - accuracy: 0.40 - ETA: 0s - loss: 0.1452 - accuracy: 0.27 - ETA: 0s - loss: 0.1486 - accuracy: 0.28 - ETA: 0s - loss: 0.1658 - accuracy: 0.28 - 0s 73us/step - loss: 0.1579 - accuracy: 0.2966 - val_loss: 0.1157 - val_accuracy: 0.3075\n",
      "Epoch 341/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2145 - accuracy: 0.400 - ETA: 0s - loss: -0.0151 - accuracy: 0.305 - ETA: 0s - loss: 0.1548 - accuracy: 0.299 - ETA: 0s - loss: 0.1258 - accuracy: 0.29 - ETA: 0s - loss: 0.1566 - accuracy: 0.30 - 0s 82us/step - loss: 0.1574 - accuracy: 0.2978 - val_loss: 0.1147 - val_accuracy: 0.3088\n",
      "Epoch 342/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6554 - accuracy: 0.50 - ETA: 0s - loss: 0.0361 - accuracy: 0.30 - ETA: 0s - loss: 0.1290 - accuracy: 0.29 - ETA: 0s - loss: 0.1877 - accuracy: 0.30 - ETA: 0s - loss: 0.1662 - accuracy: 0.30 - 0s 78us/step - loss: 0.1553 - accuracy: 0.3000 - val_loss: 0.1053 - val_accuracy: 0.2862\n",
      "Epoch 343/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2907 - accuracy: 0.200 - ETA: 0s - loss: 0.3041 - accuracy: 0.284 - ETA: 0s - loss: 0.2447 - accuracy: 0.30 - ETA: 0s - loss: 0.1797 - accuracy: 0.29 - ETA: 0s - loss: 0.1576 - accuracy: 0.29 - 0s 75us/step - loss: 0.1564 - accuracy: 0.2969 - val_loss: 0.1050 - val_accuracy: 0.3038\n",
      "Epoch 344/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.0533 - accuracy: 0.600 - ETA: 0s - loss: 0.2557 - accuracy: 0.278 - ETA: 0s - loss: 0.2113 - accuracy: 0.30 - ETA: 0s - loss: 0.1768 - accuracy: 0.30 - 0s 74us/step - loss: 0.1537 - accuracy: 0.3006 - val_loss: 0.0998 - val_accuracy: 0.3038\n",
      "Epoch 345/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.1598 - accuracy: 0.20 - ETA: 0s - loss: 0.2638 - accuracy: 0.29 - ETA: 0s - loss: 0.1906 - accuracy: 0.31 - ETA: 0s - loss: 0.1894 - accuracy: 0.30 - 0s 71us/step - loss: 0.1498 - accuracy: 0.2975 - val_loss: 0.1224 - val_accuracy: 0.2837\n",
      "Epoch 346/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6864 - accuracy: 0.10 - ETA: 0s - loss: 0.1958 - accuracy: 0.30 - ETA: 0s - loss: 0.2077 - accuracy: 0.30 - ETA: 0s - loss: 0.1380 - accuracy: 0.30 - 0s 70us/step - loss: 0.1584 - accuracy: 0.2981 - val_loss: 0.0991 - val_accuracy: 0.2887\n",
      "Epoch 347/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7094 - accuracy: 0.500 - ETA: 0s - loss: 0.2248 - accuracy: 0.311 - ETA: 0s - loss: 0.2055 - accuracy: 0.30 - ETA: 0s - loss: 0.1713 - accuracy: 0.30 - 0s 70us/step - loss: 0.1565 - accuracy: 0.2956 - val_loss: 0.1316 - val_accuracy: 0.3250\n",
      "Epoch 348/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0572 - accuracy: 0.200 - ETA: 0s - loss: 0.1356 - accuracy: 0.328 - ETA: 0s - loss: 0.1940 - accuracy: 0.31 - ETA: 0s - loss: 0.1858 - accuracy: 0.30 - 0s 70us/step - loss: 0.1610 - accuracy: 0.2959 - val_loss: 0.0941 - val_accuracy: 0.2925\n",
      "Epoch 349/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5522 - accuracy: 0.10 - ETA: 0s - loss: 0.2327 - accuracy: 0.29 - ETA: 0s - loss: 0.2040 - accuracy: 0.29 - ETA: 0s - loss: 0.1383 - accuracy: 0.29 - 0s 69us/step - loss: 0.1526 - accuracy: 0.2959 - val_loss: 0.1010 - val_accuracy: 0.3038\n",
      "Epoch 350/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2612 - accuracy: 0.400 - ETA: 0s - loss: 0.1277 - accuracy: 0.309 - ETA: 0s - loss: 0.1313 - accuracy: 0.30 - ETA: 0s - loss: 0.1447 - accuracy: 0.29 - 0s 74us/step - loss: 0.1504 - accuracy: 0.2981 - val_loss: 0.1123 - val_accuracy: 0.3113\n",
      "Epoch 351/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4960 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.2191 - accuracy: 0.3094   - ETA: 0s - loss: 0.1427 - accuracy: 0.30 - ETA: 0s - loss: 0.1475 - accuracy: 0.29 - 0s 70us/step - loss: 0.1549 - accuracy: 0.2969 - val_loss: 0.1145 - val_accuracy: 0.3162\n",
      "Epoch 352/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.10 - ETA: 0s - loss: 0.2010 - accuracy: 0.27 - ETA: 0s - loss: 0.1508 - accuracy: 0.28 - ETA: 0s - loss: 0.1598 - accuracy: 0.28 - 0s 68us/step - loss: 0.1547 - accuracy: 0.2937 - val_loss: 0.0937 - val_accuracy: 0.3050\n",
      "Epoch 353/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5206 - accuracy: 0.40 - ETA: 0s - loss: 0.0579 - accuracy: 0.29 - ETA: 0s - loss: 0.0987 - accuracy: 0.29 - ETA: 0s - loss: 0.1173 - accuracy: 0.29 - 0s 71us/step - loss: 0.1494 - accuracy: 0.2975 - val_loss: 0.1384 - val_accuracy: 0.3275\n",
      "Epoch 354/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1747 - accuracy: 0.60 - ETA: 0s - loss: 0.0520 - accuracy: 0.28 - ETA: 0s - loss: 0.1100 - accuracy: 0.29 - ETA: 0s - loss: 0.1542 - accuracy: 0.29 - ETA: 0s - loss: 0.1655 - accuracy: 0.29 - 0s 78us/step - loss: 0.1561 - accuracy: 0.2978 - val_loss: 0.0929 - val_accuracy: 0.3038\n",
      "Epoch 355/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.20 - ETA: 0s - loss: 0.0157 - accuracy: 0.30 - ETA: 0s - loss: 0.1700 - accuracy: 0.30 - ETA: 0s - loss: 0.1405 - accuracy: 0.30 - 0s 71us/step - loss: 0.1481 - accuracy: 0.3006 - val_loss: 0.0901 - val_accuracy: 0.3000\n",
      "Epoch 356/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.3364 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.2041 - accuracy: 0.2713   - ETA: 0s - loss: 0.2030 - accuracy: 0.27 - ETA: 0s - loss: 0.1232 - accuracy: 0.28 - 0s 75us/step - loss: 0.1514 - accuracy: 0.2959 - val_loss: 0.1543 - val_accuracy: 0.3313\n",
      "Epoch 357/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.1803 - accuracy: 0.300 - ETA: 0s - loss: 0.2688 - accuracy: 0.313 - ETA: 0s - loss: 0.2114 - accuracy: 0.29 - ETA: 0s - loss: 0.1599 - accuracy: 0.29 - ETA: 0s - loss: 0.1494 - accuracy: 0.29 - ETA: 0s - loss: 0.1391 - accuracy: 0.29 - 0s 109us/step - loss: 0.1547 - accuracy: 0.2962 - val_loss: 0.0984 - val_accuracy: 0.3088\n",
      "Epoch 358/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1476 - accuracy: 0.200 - ETA: 0s - loss: 0.3911 - accuracy: 0.343 - ETA: 0s - loss: 0.3264 - accuracy: 0.31 - ETA: 0s - loss: 0.1947 - accuracy: 0.30 - ETA: 0s - loss: 0.1613 - accuracy: 0.29 - 0s 78us/step - loss: 0.1364 - accuracy: 0.3006 - val_loss: 0.1120 - val_accuracy: 0.2850\n",
      "Epoch 359/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.3233 - accuracy: 0.300 - ETA: 0s - loss: 0.1152 - accuracy: 0.272 - ETA: 0s - loss: 0.1397 - accuracy: 0.29 - ETA: 0s - loss: 0.1314 - accuracy: 0.29 - 0s 69us/step - loss: 0.1543 - accuracy: 0.2941 - val_loss: 0.1396 - val_accuracy: 0.3275\n",
      "Epoch 360/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2026 - accuracy: 0.60 - ETA: 0s - loss: 0.2789 - accuracy: 0.29 - ETA: 0s - loss: 0.1596 - accuracy: 0.30 - ETA: 0s - loss: 0.1388 - accuracy: 0.29 - 0s 70us/step - loss: 0.1425 - accuracy: 0.3003 - val_loss: 0.0901 - val_accuracy: 0.3050\n",
      "Epoch 361/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4000 - accuracy: 0.10 - ETA: 0s - loss: 0.0898 - accuracy: 0.29 - ETA: 0s - loss: 0.1331 - accuracy: 0.30 - ETA: 0s - loss: 0.1183 - accuracy: 0.29 - ETA: 0s - loss: 0.1266 - accuracy: 0.30 - 0s 77us/step - loss: 0.1411 - accuracy: 0.3016 - val_loss: 0.2025 - val_accuracy: 0.3438\n",
      "Epoch 362/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6384 - accuracy: 0.500 - ETA: 0s - loss: 0.1775 - accuracy: 0.290 - ETA: 0s - loss: 0.2174 - accuracy: 0.29 - ETA: 0s - loss: 0.1736 - accuracy: 0.29 - 0s 66us/step - loss: 0.1539 - accuracy: 0.2991 - val_loss: 0.1136 - val_accuracy: 0.3175\n",
      "Epoch 363/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8153 - accuracy: 0.20 - ETA: 0s - loss: 0.0892 - accuracy: 0.31 - ETA: 0s - loss: 0.1415 - accuracy: 0.29 - ETA: 0s - loss: 0.1390 - accuracy: 0.29 - 0s 68us/step - loss: 0.1468 - accuracy: 0.2934 - val_loss: 0.1104 - val_accuracy: 0.3175\n",
      "Epoch 364/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.0125 - accuracy: 0.200 - ETA: 0s - loss: 0.2048 - accuracy: 0.342 - ETA: 0s - loss: 0.1486 - accuracy: 0.31 - ETA: 0s - loss: 0.1335 - accuracy: 0.30 - 0s 73us/step - loss: 0.1447 - accuracy: 0.3028 - val_loss: 0.0898 - val_accuracy: 0.3050\n",
      "Epoch 365/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.50 - ETA: 0s - loss: 0.2234 - accuracy: 0.32 - ETA: 0s - loss: 0.1286 - accuracy: 0.30 - ETA: 0s - loss: 0.1317 - accuracy: 0.29 - 0s 69us/step - loss: 0.1417 - accuracy: 0.2981 - val_loss: 0.0842 - val_accuracy: 0.2962\n",
      "Epoch 366/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7563 - accuracy: 0.70 - ETA: 0s - loss: 0.0917 - accuracy: 0.30 - ETA: 0s - loss: 0.2289 - accuracy: 0.30 - ETA: 0s - loss: 0.1684 - accuracy: 0.30 - 0s 73us/step - loss: 0.1358 - accuracy: 0.3028 - val_loss: 0.0944 - val_accuracy: 0.2850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 367/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3910 - accuracy: 0.500 - ETA: 0s - loss: 0.2000 - accuracy: 0.291 - ETA: 0s - loss: 0.0637 - accuracy: 0.30 - ETA: 0s - loss: 0.1236 - accuracy: 0.29 - 0s 68us/step - loss: 0.1402 - accuracy: 0.2994 - val_loss: 0.1120 - val_accuracy: 0.3175\n",
      "Epoch 368/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3029 - accuracy: 0.30 - ETA: 0s - loss: 0.1690 - accuracy: 0.26 - ETA: 0s - loss: 0.1226 - accuracy: 0.28 - ETA: 0s - loss: 0.1359 - accuracy: 0.28 - ETA: 0s - loss: 0.1171 - accuracy: 0.29 - 0s 88us/step - loss: 0.1477 - accuracy: 0.2944 - val_loss: 0.1234 - val_accuracy: 0.3275\n",
      "Epoch 369/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9405 - accuracy: 0.500 - ETA: 0s - loss: 0.2219 - accuracy: 0.273 - ETA: 0s - loss: 0.1320 - accuracy: 0.28 - ETA: 0s - loss: 0.1777 - accuracy: 0.29 - 0s 71us/step - loss: 0.1416 - accuracy: 0.2969 - val_loss: 0.0900 - val_accuracy: 0.3088\n",
      "Epoch 370/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4989 - accuracy: 0.300 - ETA: 0s - loss: -0.0412 - accuracy: 0.288 - ETA: 0s - loss: 0.0691 - accuracy: 0.294 - ETA: 0s - loss: 0.0876 - accuracy: 0.30 - 0s 71us/step - loss: 0.1419 - accuracy: 0.2984 - val_loss: 0.1052 - val_accuracy: 0.3175\n",
      "Epoch 371/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.40 - ETA: 0s - loss: 0.0954 - accuracy: 0.30 - ETA: 0s - loss: 0.2198 - accuracy: 0.30 - ETA: 0s - loss: 0.1766 - accuracy: 0.30 - 0s 70us/step - loss: 0.1405 - accuracy: 0.2962 - val_loss: 0.0821 - val_accuracy: 0.3050\n",
      "Epoch 372/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.2547 - accuracy: 0.300 - ETA: 0s - loss: 0.0814 - accuracy: 0.280 - ETA: 0s - loss: 0.0767 - accuracy: 0.30 - ETA: 0s - loss: 0.1516 - accuracy: 0.29 - 0s 70us/step - loss: 0.1395 - accuracy: 0.2975 - val_loss: 0.0914 - val_accuracy: 0.3088\n",
      "Epoch 373/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1565 - accuracy: 0.400 - ETA: 0s - loss: 0.1004 - accuracy: 0.288 - ETA: 0s - loss: 0.1587 - accuracy: 0.30 - ETA: 0s - loss: 0.1647 - accuracy: 0.29 - 0s 70us/step - loss: 0.1440 - accuracy: 0.2981 - val_loss: 0.0797 - val_accuracy: 0.3050\n",
      "Epoch 374/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.3892 - accuracy: 0.500 - ETA: 0s - loss: 0.0185 - accuracy: 0.277 - ETA: 0s - loss: 0.0925 - accuracy: 0.28 - ETA: 0s - loss: 0.1571 - accuracy: 0.29 - 0s 71us/step - loss: 0.1425 - accuracy: 0.2969 - val_loss: 0.0878 - val_accuracy: 0.3088\n",
      "Epoch 375/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.1670 - accuracy: 0.300 - ETA: 0s - loss: -0.0129 - accuracy: 0.294 - ETA: 0s - loss: 0.1171 - accuracy: 0.288 - ETA: 0s - loss: 0.1216 - accuracy: 0.29 - 0s 72us/step - loss: 0.1381 - accuracy: 0.2969 - val_loss: 0.0981 - val_accuracy: 0.3175\n",
      "Epoch 376/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.10 - ETA: 0s - loss: 0.1705 - accuracy: 0.31 - ETA: 0s - loss: 0.1589 - accuracy: 0.30 - ETA: 0s - loss: 0.1357 - accuracy: 0.29 - 0s 72us/step - loss: 0.1278 - accuracy: 0.3025 - val_loss: 0.1117 - val_accuracy: 0.2825\n",
      "Epoch 377/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3518 - accuracy: 0.200 - ETA: 0s - loss: 0.2117 - accuracy: 0.298 - ETA: 0s - loss: 0.0898 - accuracy: 0.28 - ETA: 0s - loss: 0.0521 - accuracy: 0.29 - 0s 71us/step - loss: 0.1384 - accuracy: 0.2962 - val_loss: 0.0904 - val_accuracy: 0.2850\n",
      "Epoch 378/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3101 - accuracy: 0.300 - ETA: 0s - loss: 0.0748 - accuracy: 0.296 - ETA: 0s - loss: 0.1232 - accuracy: 0.30 - ETA: 0s - loss: 0.1416 - accuracy: 0.29 - 0s 70us/step - loss: 0.1380 - accuracy: 0.2991 - val_loss: 0.0765 - val_accuracy: 0.2937\n",
      "Epoch 379/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.7695 - accuracy: 0.30 - ETA: 0s - loss: 0.2857 - accuracy: 0.28 - ETA: 0s - loss: 0.2419 - accuracy: 0.30 - ETA: 0s - loss: 0.1155 - accuracy: 0.29 - 0s 71us/step - loss: 0.1334 - accuracy: 0.2978 - val_loss: 0.0880 - val_accuracy: 0.3113\n",
      "Epoch 380/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3030 - accuracy: 0.40 - ETA: 0s - loss: 0.2853 - accuracy: 0.32 - ETA: 0s - loss: 0.1977 - accuracy: 0.30 - ETA: 0s - loss: 0.1840 - accuracy: 0.29 - 0s 70us/step - loss: 0.1324 - accuracy: 0.2984 - val_loss: 0.0792 - val_accuracy: 0.2875\n",
      "Epoch 381/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.7141 - accuracy: 0.20 - ETA: 0s - loss: 0.1208 - accuracy: 0.30 - ETA: 0s - loss: 0.1200 - accuracy: 0.29 - ETA: 0s - loss: 0.0951 - accuracy: 0.30 - 0s 70us/step - loss: 0.1398 - accuracy: 0.2966 - val_loss: 0.0893 - val_accuracy: 0.3113\n",
      "Epoch 382/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2183 - accuracy: 0.400 - ETA: 0s - loss: -0.0040 - accuracy: 0.295 - ETA: 0s - loss: 0.0988 - accuracy: 0.312 - ETA: 0s - loss: 0.1327 - accuracy: 0.30 - 0s 72us/step - loss: 0.1287 - accuracy: 0.3006 - val_loss: 0.0783 - val_accuracy: 0.2875\n",
      "Epoch 383/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4210 - accuracy: 0.100 - ETA: 0s - loss: 0.2641 - accuracy: 0.300 - ETA: 0s - loss: 0.2499 - accuracy: 0.30 - ETA: 0s - loss: 0.1507 - accuracy: 0.29 - 0s 72us/step - loss: 0.1336 - accuracy: 0.2988 - val_loss: 0.0996 - val_accuracy: 0.3162\n",
      "Epoch 384/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.0124 - accuracy: 0.500 - ETA: 0s - loss: 0.0516 - accuracy: 0.338 - ETA: 0s - loss: 0.1069 - accuracy: 0.31 - ETA: 0s - loss: 0.1432 - accuracy: 0.29 - 0s 73us/step - loss: 0.1312 - accuracy: 0.2978 - val_loss: 0.0739 - val_accuracy: 0.3050\n",
      "Epoch 385/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4430 - accuracy: 0.500 - ETA: 0s - loss: 0.1076 - accuracy: 0.291 - ETA: 0s - loss: 0.1495 - accuracy: 0.28 - ETA: 0s - loss: 0.1539 - accuracy: 0.29 - 0s 70us/step - loss: 0.1318 - accuracy: 0.2991 - val_loss: 0.0732 - val_accuracy: 0.2950\n",
      "Epoch 386/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2055 - accuracy: 0.200 - ETA: 0s - loss: 0.0692 - accuracy: 0.317 - ETA: 0s - loss: 0.1143 - accuracy: 0.31 - ETA: 0s - loss: 0.1254 - accuracy: 0.30 - ETA: 0s - loss: 0.1312 - accuracy: 0.29 - 0s 75us/step - loss: 0.1376 - accuracy: 0.2972 - val_loss: 0.0733 - val_accuracy: 0.3050\n",
      "Epoch 387/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2443 - accuracy: 0.20 - ETA: 0s - loss: -0.0414 - accuracy: 0.311 - ETA: 0s - loss: 0.0237 - accuracy: 0.302 - ETA: 0s - loss: 0.0996 - accuracy: 0.29 - ETA: 0s - loss: 0.1524 - accuracy: 0.29 - 0s 81us/step - loss: 0.1289 - accuracy: 0.2950 - val_loss: 0.0713 - val_accuracy: 0.2937\n",
      "Epoch 388/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4754 - accuracy: 0.30 - ETA: 0s - loss: 0.0839 - accuracy: 0.30 - ETA: 0s - loss: 0.1406 - accuracy: 0.29 - ETA: 0s - loss: 0.1649 - accuracy: 0.29 - ETA: 0s - loss: 0.1402 - accuracy: 0.29 - 0s 82us/step - loss: 0.1302 - accuracy: 0.2950 - val_loss: 0.1024 - val_accuracy: 0.3200\n",
      "Epoch 389/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3131 - accuracy: 0.300 - ETA: 0s - loss: 0.0604 - accuracy: 0.308 - ETA: 0s - loss: 0.0418 - accuracy: 0.28 - ETA: 0s - loss: 0.1064 - accuracy: 0.29 - ETA: 0s - loss: 0.1233 - accuracy: 0.29 - 0s 75us/step - loss: 0.1341 - accuracy: 0.2962 - val_loss: 0.0998 - val_accuracy: 0.3200\n",
      "Epoch 390/500\n",
      "3200/3200 [==============================] - ETA: 4s - loss: 0.9347 - accuracy: 0.30 - ETA: 0s - loss: 0.1693 - accuracy: 0.33 - ETA: 0s - loss: 0.1503 - accuracy: 0.31 - ETA: 0s - loss: 0.1175 - accuracy: 0.31 - 0s 82us/step - loss: 0.1325 - accuracy: 0.3022 - val_loss: 0.0691 - val_accuracy: 0.2925\n",
      "Epoch 391/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7785 - accuracy: 0.300 - ETA: 0s - loss: 0.0552 - accuracy: 0.312 - ETA: 0s - loss: 0.1370 - accuracy: 0.30 - ETA: 0s - loss: 0.1331 - accuracy: 0.29 - ETA: 0s - loss: 0.1353 - accuracy: 0.29 - 0s 82us/step - loss: 0.1282 - accuracy: 0.2991 - val_loss: 0.0707 - val_accuracy: 0.2925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8632 - accuracy: 0.300 - ETA: 0s - loss: 0.1445 - accuracy: 0.304 - ETA: 0s - loss: 0.2556 - accuracy: 0.31 - ETA: 0s - loss: 0.1587 - accuracy: 0.30 - 0s 72us/step - loss: 0.1226 - accuracy: 0.2972 - val_loss: 0.0715 - val_accuracy: 0.2887\n",
      "Epoch 393/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8255 - accuracy: 0.400 - ETA: 0s - loss: 0.1941 - accuracy: 0.301 - ETA: 0s - loss: 0.0704 - accuracy: 0.28 - ETA: 0s - loss: 0.1200 - accuracy: 0.29 - 0s 71us/step - loss: 0.1311 - accuracy: 0.2969 - val_loss: 0.0753 - val_accuracy: 0.3088\n",
      "Epoch 394/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5875 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.2407 - accuracy: 0.3161    - ETA: 0s - loss: 0.2233 - accuracy: 0.30 - ETA: 0s - loss: 0.1305 - accuracy: 0.29 - 0s 71us/step - loss: 0.1333 - accuracy: 0.2975 - val_loss: 0.0749 - val_accuracy: 0.3088\n",
      "Epoch 395/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4699 - accuracy: 0.300 - ETA: 0s - loss: 0.1459 - accuracy: 0.326 - ETA: 0s - loss: 0.2477 - accuracy: 0.30 - ETA: 0s - loss: 0.1848 - accuracy: 0.30 - 0s 71us/step - loss: 0.1264 - accuracy: 0.2988 - val_loss: 0.0959 - val_accuracy: 0.2825\n",
      "Epoch 396/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.50 - ETA: 0s - loss: 0.1624 - accuracy: 0.29 - ETA: 0s - loss: 0.2094 - accuracy: 0.30 - ETA: 0s - loss: 0.1746 - accuracy: 0.29 - 0s 71us/step - loss: 0.1264 - accuracy: 0.2950 - val_loss: 0.0652 - val_accuracy: 0.3038\n",
      "Epoch 397/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.30 - ETA: 0s - loss: 0.0347 - accuracy: 0.29 - ETA: 0s - loss: 0.1316 - accuracy: 0.29 - ETA: 0s - loss: 0.0946 - accuracy: 0.29 - 0s 71us/step - loss: 0.1263 - accuracy: 0.2991 - val_loss: 0.0699 - val_accuracy: 0.3075\n",
      "Epoch 398/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.2220 - accuracy: 0.10 - ETA: 0s - loss: 0.1520 - accuracy: 0.30 - ETA: 0s - loss: 0.0540 - accuracy: 0.29 - ETA: 0s - loss: 0.0936 - accuracy: 0.28 - 0s 71us/step - loss: 0.1297 - accuracy: 0.2947 - val_loss: 0.0690 - val_accuracy: 0.3075\n",
      "Epoch 399/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9183 - accuracy: 0.100 - ETA: 0s - loss: 0.1557 - accuracy: 0.297 - ETA: 0s - loss: 0.2619 - accuracy: 0.29 - ETA: 0s - loss: 0.1416 - accuracy: 0.29 - 0s 71us/step - loss: 0.1230 - accuracy: 0.2959 - val_loss: 0.0824 - val_accuracy: 0.3162\n",
      "Epoch 400/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6713 - accuracy: 0.0000e+ - ETA: 0s - loss: 0.1130 - accuracy: 0.3024   - ETA: 0s - loss: 0.1209 - accuracy: 0.29 - ETA: 0s - loss: 0.1457 - accuracy: 0.29 - 0s 71us/step - loss: 0.1275 - accuracy: 0.2975 - val_loss: 0.0838 - val_accuracy: 0.3175\n",
      "Epoch 401/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.30 - ETA: 0s - loss: 0.1117 - accuracy: 0.29 - ETA: 0s - loss: 0.0033 - accuracy: 0.30 - ETA: 0s - loss: 0.0293 - accuracy: 0.29 - 0s 71us/step - loss: 0.1261 - accuracy: 0.2991 - val_loss: 0.1178 - val_accuracy: 0.3275\n",
      "Epoch 402/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1646 - accuracy: 0.400 - ETA: 0s - loss: 0.1293 - accuracy: 0.278 - ETA: 0s - loss: 0.1881 - accuracy: 0.30 - ETA: 0s - loss: 0.1181 - accuracy: 0.29 - 0s 71us/step - loss: 0.1229 - accuracy: 0.2956 - val_loss: 0.1183 - val_accuracy: 0.3288\n",
      "Epoch 403/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2168 - accuracy: 0.20 - ETA: 0s - loss: 0.0391 - accuracy: 0.30 - ETA: 0s - loss: 0.1634 - accuracy: 0.29 - ETA: 0s - loss: 0.1241 - accuracy: 0.30 - 0s 71us/step - loss: 0.1291 - accuracy: 0.2959 - val_loss: 0.0625 - val_accuracy: 0.3038\n",
      "Epoch 404/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.4756 - accuracy: 0.300 - ETA: 0s - loss: 0.1144 - accuracy: 0.291 - ETA: 0s - loss: 0.1719 - accuracy: 0.29 - ETA: 0s - loss: 0.1522 - accuracy: 0.29 - 0s 71us/step - loss: 0.1202 - accuracy: 0.2972 - val_loss: 0.0661 - val_accuracy: 0.2900\n",
      "Epoch 405/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1350 - accuracy: 0.200 - ETA: 0s - loss: 0.1699 - accuracy: 0.292 - ETA: 0s - loss: 0.1657 - accuracy: 0.29 - ETA: 0s - loss: 0.1575 - accuracy: 0.29 - 0s 72us/step - loss: 0.1235 - accuracy: 0.2953 - val_loss: 0.0680 - val_accuracy: 0.3113\n",
      "Epoch 406/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6410 - accuracy: 0.300 - ETA: 0s - loss: 0.4390 - accuracy: 0.319 - ETA: 0s - loss: 0.2328 - accuracy: 0.29 - ETA: 0s - loss: 0.1635 - accuracy: 0.29 - 0s 70us/step - loss: 0.1114 - accuracy: 0.2988 - val_loss: 0.0604 - val_accuracy: 0.2962\n",
      "Epoch 407/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2220 - accuracy: 0.300 - ETA: 0s - loss: 0.2599 - accuracy: 0.299 - ETA: 0s - loss: 0.1047 - accuracy: 0.29 - ETA: 0s - loss: 0.1148 - accuracy: 0.29 - 0s 72us/step - loss: 0.1213 - accuracy: 0.2975 - val_loss: 0.0606 - val_accuracy: 0.3088\n",
      "Epoch 408/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.5487 - accuracy: 0.200 - ETA: 0s - loss: 0.1834 - accuracy: 0.304 - ETA: 0s - loss: 0.1292 - accuracy: 0.30 - 0s 65us/step - loss: 0.1134 - accuracy: 0.2991 - val_loss: 0.0680 - val_accuracy: 0.3113\n",
      "Epoch 409/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.5357 - accuracy: 0.60 - ETA: 0s - loss: 0.1498 - accuracy: 0.28 - ETA: 0s - loss: 0.0894 - accuracy: 0.29 - ETA: 0s - loss: 0.1127 - accuracy: 0.29 - 0s 72us/step - loss: 0.1241 - accuracy: 0.2969 - val_loss: 0.0586 - val_accuracy: 0.2988\n",
      "Epoch 410/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3005 - accuracy: 0.60 - ETA: 0s - loss: -0.0502 - accuracy: 0.286 - ETA: 0s - loss: 0.0791 - accuracy: 0.283 - ETA: 0s - loss: 0.0977 - accuracy: 0.29 - 0s 74us/step - loss: 0.1105 - accuracy: 0.2953 - val_loss: 0.0609 - val_accuracy: 0.3100\n",
      "Epoch 411/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5089 - accuracy: 0.50 - ETA: 0s - loss: 0.1898 - accuracy: 0.27 - ETA: 0s - loss: 0.1527 - accuracy: 0.28 - ETA: 0s - loss: 0.1324 - accuracy: 0.29 - 0s 72us/step - loss: 0.1202 - accuracy: 0.2984 - val_loss: 0.0661 - val_accuracy: 0.3113\n",
      "Epoch 412/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4193 - accuracy: 0.400 - ETA: 0s - loss: 0.1027 - accuracy: 0.298 - ETA: 0s - loss: 0.1064 - accuracy: 0.29 - ETA: 0s - loss: 0.1321 - accuracy: 0.29 - 0s 71us/step - loss: 0.1160 - accuracy: 0.2972 - val_loss: 0.0860 - val_accuracy: 0.3262\n",
      "Epoch 413/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.0573 - accuracy: 0.400 - ETA: 0s - loss: 0.0229 - accuracy: 0.300 - ETA: 0s - loss: 0.1037 - accuracy: 0.30 - ETA: 0s - loss: 0.0682 - accuracy: 0.29 - 0s 71us/step - loss: 0.1155 - accuracy: 0.2984 - val_loss: 0.0671 - val_accuracy: 0.3113\n",
      "Epoch 414/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.3595 - accuracy: 0.500 - ETA: 0s - loss: 0.1637 - accuracy: 0.283 - ETA: 0s - loss: 0.2380 - accuracy: 0.30 - ETA: 0s - loss: 0.1386 - accuracy: 0.29 - 0s 70us/step - loss: 0.1104 - accuracy: 0.2947 - val_loss: 0.0737 - val_accuracy: 0.3175\n",
      "Epoch 415/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.20 - ETA: 0s - loss: 0.1221 - accuracy: 0.28 - ETA: 0s - loss: 0.1648 - accuracy: 0.29 - ETA: 0s - loss: 0.1546 - accuracy: 0.29 - 0s 71us/step - loss: 0.1087 - accuracy: 0.2959 - val_loss: 0.0598 - val_accuracy: 0.2875\n",
      "Epoch 416/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5672 - accuracy: 0.300 - ETA: 0s - loss: -0.0405 - accuracy: 0.296 - ETA: 0s - loss: 0.0910 - accuracy: 0.283 - ETA: 0s - loss: 0.1119 - accuracy: 0.30 - 0s 71us/step - loss: 0.1112 - accuracy: 0.2991 - val_loss: 0.0797 - val_accuracy: 0.2837\n",
      "Epoch 417/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9085 - accuracy: 0.40 - ETA: 0s - loss: 0.2014 - accuracy: 0.28 - ETA: 0s - loss: 0.0926 - accuracy: 0.28 - ETA: 0s - loss: 0.0661 - accuracy: 0.30 - 0s 71us/step - loss: 0.1100 - accuracy: 0.2994 - val_loss: 0.0817 - val_accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0857 - accuracy: 0.300 - ETA: 0s - loss: 0.0300 - accuracy: 0.294 - ETA: 0s - loss: 0.0174 - accuracy: 0.28 - ETA: 0s - loss: 0.0852 - accuracy: 0.29 - 0s 71us/step - loss: 0.1052 - accuracy: 0.2959 - val_loss: 0.0507 - val_accuracy: 0.2962\n",
      "Epoch 419/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3935 - accuracy: 0.400 - ETA: 0s - loss: 0.0726 - accuracy: 0.286 - ETA: 0s - loss: 0.1428 - accuracy: 0.29 - ETA: 0s - loss: 0.0815 - accuracy: 0.29 - 0s 71us/step - loss: 0.1118 - accuracy: 0.2959 - val_loss: 0.0670 - val_accuracy: 0.3137\n",
      "Epoch 420/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.7511 - accuracy: 0.10 - ETA: 0s - loss: 0.1449 - accuracy: 0.30 - ETA: 0s - loss: 0.0208 - accuracy: 0.30 - ETA: 0s - loss: -0.0284 - accuracy: 0.297 - 0s 71us/step - loss: 0.0935 - accuracy: 0.2981 - val_loss: 0.0740 - val_accuracy: 0.3162\n",
      "Epoch 421/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2143 - accuracy: 0.200 - ETA: 0s - loss: 0.1898 - accuracy: 0.307 - ETA: 0s - loss: 0.1282 - accuracy: 0.28 - ETA: 0s - loss: 0.0662 - accuracy: 0.28 - 0s 71us/step - loss: 0.1059 - accuracy: 0.2944 - val_loss: 0.0465 - val_accuracy: 0.2950\n",
      "Epoch 422/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.50 - ETA: 0s - loss: 0.0834 - accuracy: 0.27 - ETA: 0s - loss: 0.0881 - accuracy: 0.29 - ETA: 0s - loss: 0.1312 - accuracy: 0.29 - 0s 71us/step - loss: 0.1018 - accuracy: 0.2972 - val_loss: 0.0458 - val_accuracy: 0.2875\n",
      "Epoch 423/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.30 - ETA: 0s - loss: 0.2250 - accuracy: 0.30 - ETA: 0s - loss: 0.2427 - accuracy: 0.29 - ETA: 0s - loss: 0.1314 - accuracy: 0.29 - 0s 70us/step - loss: 0.1063 - accuracy: 0.2959 - val_loss: 0.0532 - val_accuracy: 0.3075\n",
      "Epoch 424/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.3813 - accuracy: 0.20 - ETA: 0s - loss: 0.3113 - accuracy: 0.30 - ETA: 0s - loss: 0.1627 - accuracy: 0.29 - ETA: 0s - loss: 0.0957 - accuracy: 0.29 - 0s 70us/step - loss: 0.1055 - accuracy: 0.2981 - val_loss: 0.0675 - val_accuracy: 0.3175\n",
      "Epoch 425/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2140 - accuracy: 0.500 - ETA: 0s - loss: 0.2019 - accuracy: 0.303 - ETA: 0s - loss: 0.1459 - accuracy: 0.30 - ETA: 0s - loss: 0.1234 - accuracy: 0.30 - 0s 71us/step - loss: 0.0989 - accuracy: 0.2966 - val_loss: 0.0474 - val_accuracy: 0.3063\n",
      "Epoch 426/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.1139 - accuracy: 0.300 - ETA: 0s - loss: 0.2307 - accuracy: 0.283 - ETA: 0s - loss: 0.1087 - accuracy: 0.29 - ETA: 0s - loss: 0.1580 - accuracy: 0.29 - 0s 71us/step - loss: 0.1048 - accuracy: 0.2956 - val_loss: 0.0459 - val_accuracy: 0.3075\n",
      "Epoch 427/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.4315 - accuracy: 0.200 - ETA: 0s - loss: -0.2270 - accuracy: 0.271 - ETA: 0s - loss: 0.0461 - accuracy: 0.282 - ETA: 0s - loss: 0.0765 - accuracy: 0.28 - 0s 71us/step - loss: 0.0966 - accuracy: 0.2934 - val_loss: 0.1045 - val_accuracy: 0.3338\n",
      "Epoch 428/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.8015 - accuracy: 0.20 - ETA: 0s - loss: 0.0510 - accuracy: 0.29 - ETA: 0s - loss: 0.1329 - accuracy: 0.29 - ETA: 0s - loss: 0.1248 - accuracy: 0.29 - 0s 73us/step - loss: 0.1068 - accuracy: 0.2950 - val_loss: 0.0473 - val_accuracy: 0.3100\n",
      "Epoch 429/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7357 - accuracy: 0.20 - ETA: 0s - loss: 0.1527 - accuracy: 0.29 - ETA: 0s - loss: 0.1431 - accuracy: 0.29 - ETA: 0s - loss: 0.1037 - accuracy: 0.30 - 0s 72us/step - loss: 0.1033 - accuracy: 0.2966 - val_loss: 0.0471 - val_accuracy: 0.3088\n",
      "Epoch 430/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4886 - accuracy: 0.100 - ETA: 0s - loss: 0.1543 - accuracy: 0.334 - ETA: 0s - loss: 0.1312 - accuracy: 0.30 - ETA: 0s - loss: 0.1367 - accuracy: 0.30 - 0s 73us/step - loss: 0.1028 - accuracy: 0.2944 - val_loss: 0.0474 - val_accuracy: 0.2875\n",
      "Epoch 431/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7796 - accuracy: 0.30 - ETA: 0s - loss: 0.0395 - accuracy: 0.31 - ETA: 0s - loss: 0.1456 - accuracy: 0.30 - ETA: 0s - loss: 0.1001 - accuracy: 0.30 - 0s 72us/step - loss: 0.0999 - accuracy: 0.2975 - val_loss: 0.0533 - val_accuracy: 0.2837\n",
      "Epoch 432/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.1413 - accuracy: 0.500 - ETA: 0s - loss: 0.0097 - accuracy: 0.302 - ETA: 0s - loss: 0.2014 - accuracy: 0.30 - ETA: 0s - loss: 0.1924 - accuracy: 0.30 - 0s 73us/step - loss: 0.1034 - accuracy: 0.2950 - val_loss: 0.0884 - val_accuracy: 0.2812\n",
      "Epoch 433/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2890 - accuracy: 0.200 - ETA: 0s - loss: 0.1385 - accuracy: 0.298 - ETA: 0s - loss: 0.1654 - accuracy: 0.30 - ETA: 0s - loss: 0.1344 - accuracy: 0.30 - 0s 71us/step - loss: 0.0937 - accuracy: 0.2981 - val_loss: 0.0388 - val_accuracy: 0.2862\n",
      "Epoch 434/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8251 - accuracy: 0.20 - ETA: 0s - loss: -0.0074 - accuracy: 0.274 - ETA: 0s - loss: 0.0673 - accuracy: 0.287 - ETA: 0s - loss: 0.0997 - accuracy: 0.28 - 0s 70us/step - loss: 0.0943 - accuracy: 0.2894 - val_loss: 0.1512 - val_accuracy: 0.3425\n",
      "Epoch 435/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.0613 - accuracy: 0.300 - ETA: 0s - loss: 0.0284 - accuracy: 0.283 - ETA: 0s - loss: 0.1248 - accuracy: 0.29 - ETA: 0s - loss: 0.1538 - accuracy: 0.29 - 0s 72us/step - loss: 0.0999 - accuracy: 0.2950 - val_loss: 0.0733 - val_accuracy: 0.2825\n",
      "Epoch 436/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.20 - ETA: 0s - loss: 0.0915 - accuracy: 0.29 - ETA: 0s - loss: 0.1108 - accuracy: 0.29 - ETA: 0s - loss: 0.1148 - accuracy: 0.29 - 0s 72us/step - loss: 0.0996 - accuracy: 0.2934 - val_loss: 0.0495 - val_accuracy: 0.2825\n",
      "Epoch 437/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6291 - accuracy: 0.400 - ETA: 0s - loss: -0.1610 - accuracy: 0.290 - ETA: 0s - loss: 0.0092 - accuracy: 0.293 - ETA: 0s - loss: 0.0155 - accuracy: 0.29 - 0s 70us/step - loss: 0.0886 - accuracy: 0.2944 - val_loss: 0.0957 - val_accuracy: 0.3237\n",
      "Epoch 438/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0768 - accuracy: 0.20 - ETA: 0s - loss: 0.1417 - accuracy: 0.31 - ETA: 0s - loss: 0.0866 - accuracy: 0.30 - ETA: 0s - loss: 0.1038 - accuracy: 0.30 - 0s 74us/step - loss: 0.0898 - accuracy: 0.2978 - val_loss: 0.0408 - val_accuracy: 0.2825\n",
      "Epoch 439/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0736 - accuracy: 0.10 - ETA: 0s - loss: -0.0050 - accuracy: 0.321 - ETA: 0s - loss: 0.1033 - accuracy: 0.306 - ETA: 0s - loss: 0.0807 - accuracy: 0.29 - 0s 73us/step - loss: 0.0878 - accuracy: 0.2937 - val_loss: 0.0425 - val_accuracy: 0.3013\n",
      "Epoch 440/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4698 - accuracy: 0.300 - ETA: 0s - loss: 0.1564 - accuracy: 0.306 - ETA: 0s - loss: 0.1430 - accuracy: 0.30 - ETA: 0s - loss: 0.0786 - accuracy: 0.30 - 0s 73us/step - loss: 0.0927 - accuracy: 0.2978 - val_loss: 0.0314 - val_accuracy: 0.2950\n",
      "Epoch 441/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5497 - accuracy: 0.400 - ETA: 0s - loss: 0.0265 - accuracy: 0.281 - ETA: 0s - loss: 0.0866 - accuracy: 0.29 - ETA: 0s - loss: 0.0452 - accuracy: 0.29 - 0s 71us/step - loss: 0.0883 - accuracy: 0.2941 - val_loss: 0.0493 - val_accuracy: 0.3125\n",
      "Epoch 442/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.30 - ETA: 0s - loss: 0.0725 - accuracy: 0.31 - ETA: 0s - loss: 0.0816 - accuracy: 0.30 - ETA: 0s - loss: 0.1099 - accuracy: 0.30 - 0s 70us/step - loss: 0.0985 - accuracy: 0.2947 - val_loss: 0.0375 - val_accuracy: 0.3063\n",
      "Epoch 443/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7670 - accuracy: 0.30 - ETA: 0s - loss: 0.0960 - accuracy: 0.28 - ETA: 0s - loss: 0.1062 - accuracy: 0.30 - ETA: 0s - loss: 0.0796 - accuracy: 0.30 - 0s 70us/step - loss: 0.0966 - accuracy: 0.2953 - val_loss: 0.0684 - val_accuracy: 0.3237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.6765 - accuracy: 0.400 - ETA: 0s - loss: 0.1709 - accuracy: 0.311 - ETA: 0s - loss: 0.1419 - accuracy: 0.29 - ETA: 0s - loss: 0.1411 - accuracy: 0.29 - 0s 70us/step - loss: 0.0783 - accuracy: 0.2934 - val_loss: 0.0676 - val_accuracy: 0.2825\n",
      "Epoch 445/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.5901 - accuracy: 0.20 - ETA: 0s - loss: 0.2018 - accuracy: 0.27 - ETA: 0s - loss: 0.0676 - accuracy: 0.27 - ETA: 0s - loss: 0.0618 - accuracy: 0.28 - 0s 70us/step - loss: 0.0941 - accuracy: 0.2916 - val_loss: 0.0571 - val_accuracy: 0.3200\n",
      "Epoch 446/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9288 - accuracy: 0.600 - ETA: 0s - loss: 0.1086 - accuracy: 0.311 - ETA: 0s - loss: 0.0292 - accuracy: 0.29 - ETA: 0s - loss: 0.1054 - accuracy: 0.28 - 0s 71us/step - loss: 0.0904 - accuracy: 0.2931 - val_loss: 0.0449 - val_accuracy: 0.2862\n",
      "Epoch 447/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.10 - ETA: 0s - loss: 0.1973 - accuracy: 0.31 - ETA: 0s - loss: 0.0803 - accuracy: 0.30 - ETA: 0s - loss: 0.0976 - accuracy: 0.30 - 0s 73us/step - loss: 0.0892 - accuracy: 0.2931 - val_loss: 0.0284 - val_accuracy: 0.3038\n",
      "Epoch 448/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.6769 - accuracy: 0.300 - ETA: 0s - loss: 0.2242 - accuracy: 0.278 - ETA: 0s - loss: 0.1558 - accuracy: 0.28 - ETA: 0s - loss: 0.0451 - accuracy: 0.29 - 0s 73us/step - loss: 0.0769 - accuracy: 0.2959 - val_loss: 0.0599 - val_accuracy: 0.2825\n",
      "Epoch 449/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.9708 - accuracy: 0.30 - ETA: 0s - loss: 0.2512 - accuracy: 0.30 - ETA: 0s - loss: 0.1380 - accuracy: 0.29 - ETA: 0s - loss: 0.1503 - accuracy: 0.29 - 0s 71us/step - loss: 0.0923 - accuracy: 0.2944 - val_loss: 0.0313 - val_accuracy: 0.3013\n",
      "Epoch 450/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.40 - ETA: 0s - loss: 0.0324 - accuracy: 0.29 - ETA: 0s - loss: 0.0300 - accuracy: 0.28 - ETA: 0s - loss: 0.0734 - accuracy: 0.30 - 0s 72us/step - loss: 0.0927 - accuracy: 0.2919 - val_loss: 0.0259 - val_accuracy: 0.2850\n",
      "Epoch 451/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -3.0451 - accuracy: 0.300 - ETA: 0s - loss: 0.2087 - accuracy: 0.298 - ETA: 0s - loss: 0.1235 - accuracy: 0.30 - ETA: 0s - loss: 0.1066 - accuracy: 0.29 - 0s 71us/step - loss: 0.0723 - accuracy: 0.2966 - val_loss: 0.0390 - val_accuracy: 0.2837\n",
      "Epoch 452/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1657 - accuracy: 0.400 - ETA: 0s - loss: -0.0283 - accuracy: 0.300 - ETA: 0s - loss: -0.0084 - accuracy: 0.296 - ETA: 0s - loss: 0.1172 - accuracy: 0.303 - 0s 72us/step - loss: 0.0892 - accuracy: 0.2972 - val_loss: 0.0480 - val_accuracy: 0.2825\n",
      "Epoch 453/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2947 - accuracy: 0.200 - ETA: 0s - loss: 0.0508 - accuracy: 0.289 - ETA: 0s - loss: 0.0460 - accuracy: 0.28 - ETA: 0s - loss: 0.0536 - accuracy: 0.29 - 0s 72us/step - loss: 0.0731 - accuracy: 0.2950 - val_loss: 0.0324 - val_accuracy: 0.3125\n",
      "Epoch 454/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2016 - accuracy: 0.20 - ETA: 0s - loss: 0.0549 - accuracy: 0.29 - ETA: 0s - loss: 0.1119 - accuracy: 0.28 - ETA: 0s - loss: 0.0711 - accuracy: 0.28 - ETA: 0s - loss: 0.0860 - accuracy: 0.29 - 0s 76us/step - loss: 0.0872 - accuracy: 0.2934 - val_loss: 0.0248 - val_accuracy: 0.3025\n",
      "Epoch 455/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.9337 - accuracy: 0.400 - ETA: 0s - loss: 0.1619 - accuracy: 0.310 - ETA: 0s - loss: 0.1011 - accuracy: 0.29 - ETA: 0s - loss: 0.0952 - accuracy: 0.29 - 0s 75us/step - loss: 0.0781 - accuracy: 0.2922 - val_loss: 0.0801 - val_accuracy: 0.3250\n",
      "Epoch 456/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.6284 - accuracy: 0.40 - ETA: 0s - loss: 0.0893 - accuracy: 0.27 - ETA: 0s - loss: 0.0270 - accuracy: 0.28 - ETA: 0s - loss: 0.1354 - accuracy: 0.29 - 0s 72us/step - loss: 0.0772 - accuracy: 0.2953 - val_loss: 0.0787 - val_accuracy: 0.2800\n",
      "Epoch 457/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.40 - ETA: 0s - loss: -0.0578 - accuracy: 0.278 - ETA: 0s - loss: 0.1293 - accuracy: 0.287 - ETA: 0s - loss: 0.0535 - accuracy: 0.29 - ETA: 0s - loss: 0.0788 - accuracy: 0.29 - 0s 76us/step - loss: 0.0897 - accuracy: 0.2916 - val_loss: 0.0288 - val_accuracy: 0.2962\n",
      "Epoch 458/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1219 - accuracy: 0.500 - ETA: 0s - loss: 0.0540 - accuracy: 0.296 - ETA: 0s - loss: 0.0640 - accuracy: 0.28 - ETA: 0s - loss: 0.1030 - accuracy: 0.29 - 0s 73us/step - loss: 0.0758 - accuracy: 0.2962 - val_loss: 0.0212 - val_accuracy: 0.3050\n",
      "Epoch 459/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3860 - accuracy: 0.10 - ETA: 0s - loss: 0.1353 - accuracy: 0.28 - ETA: 0s - loss: 0.2159 - accuracy: 0.29 - ETA: 0s - loss: 0.1243 - accuracy: 0.29 - 0s 73us/step - loss: 0.0699 - accuracy: 0.2903 - val_loss: 0.0253 - val_accuracy: 0.2950\n",
      "Epoch 460/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9546 - accuracy: 0.300 - ETA: 0s - loss: 0.1652 - accuracy: 0.300 - ETA: 0s - loss: 0.1308 - accuracy: 0.29 - ETA: 0s - loss: 0.0863 - accuracy: 0.29 - 0s 74us/step - loss: 0.0764 - accuracy: 0.2931 - val_loss: 0.0234 - val_accuracy: 0.3063\n",
      "Epoch 461/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3992 - accuracy: 0.40 - ETA: 0s - loss: 0.0137 - accuracy: 0.29 - ETA: 0s - loss: -0.0198 - accuracy: 0.288 - ETA: 0s - loss: 0.0715 - accuracy: 0.293 - ETA: 0s - loss: 0.0569 - accuracy: 0.29 - 0s 76us/step - loss: 0.0510 - accuracy: 0.2962 - val_loss: 0.0169 - val_accuracy: 0.2850\n",
      "Epoch 462/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.3820 - accuracy: 0.300 - ETA: 0s - loss: 0.2308 - accuracy: 0.292 - ETA: 0s - loss: 0.1392 - accuracy: 0.28 - ETA: 0s - loss: 0.1033 - accuracy: 0.29 - 0s 70us/step - loss: 0.0716 - accuracy: 0.2931 - val_loss: 0.0215 - val_accuracy: 0.3063\n",
      "Epoch 463/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.4187 - accuracy: 0.300 - ETA: 0s - loss: -0.2418 - accuracy: 0.312 - ETA: 0s - loss: -0.0414 - accuracy: 0.294 - ETA: 0s - loss: 0.0286 - accuracy: 0.290 - 0s 70us/step - loss: 0.0710 - accuracy: 0.2931 - val_loss: 0.0156 - val_accuracy: 0.2850\n",
      "Epoch 464/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.4721 - accuracy: 0.40 - ETA: 0s - loss: 0.0752 - accuracy: 0.29 - ETA: 0s - loss: 0.0217 - accuracy: 0.29 - ETA: 0s - loss: 0.0450 - accuracy: 0.29 - 0s 70us/step - loss: 0.0666 - accuracy: 0.2944 - val_loss: 0.0355 - val_accuracy: 0.3137\n",
      "Epoch 465/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.8278 - accuracy: 0.200 - ETA: 0s - loss: 0.0655 - accuracy: 0.307 - ETA: 0s - loss: 0.0281 - accuracy: 0.29 - ETA: 0s - loss: 0.0723 - accuracy: 0.29 - 0s 70us/step - loss: 0.0838 - accuracy: 0.2962 - val_loss: 0.0189 - val_accuracy: 0.3013\n",
      "Epoch 466/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.8277 - accuracy: 0.500 - ETA: 0s - loss: 0.0583 - accuracy: 0.302 - ETA: 0s - loss: 0.0165 - accuracy: 0.29 - ETA: 0s - loss: 0.0606 - accuracy: 0.29 - 0s 69us/step - loss: 0.0758 - accuracy: 0.2937 - val_loss: 0.0152 - val_accuracy: 0.3025\n",
      "Epoch 467/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.4551 - accuracy: 0.300 - ETA: 0s - loss: -0.1143 - accuracy: 0.281 - ETA: 0s - loss: 0.0305 - accuracy: 0.286 - ETA: 0s - loss: 0.0054 - accuracy: 0.29 - 0s 70us/step - loss: 0.0632 - accuracy: 0.2944 - val_loss: 0.0578 - val_accuracy: 0.3187\n",
      "Epoch 468/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2680 - accuracy: 0.300 - ETA: 0s - loss: -0.0558 - accuracy: 0.294 - ETA: 0s - loss: -0.0297 - accuracy: 0.294 - ETA: 0s - loss: 0.0830 - accuracy: 0.293 - 0s 70us/step - loss: 0.0731 - accuracy: 0.2919 - val_loss: 0.0128 - val_accuracy: 0.2925\n",
      "Epoch 469/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: -0.7020 - accuracy: 0.0000e+0 - ETA: 0s - loss: 0.0751 - accuracy: 0.2736    - ETA: 0s - loss: 0.0783 - accuracy: 0.28 - ETA: 0s - loss: 0.0926 - accuracy: 0.28 - 0s 71us/step - loss: 0.0615 - accuracy: 0.2922 - val_loss: 0.0383 - val_accuracy: 0.3150\n",
      "Epoch 470/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8158 - accuracy: 0.40 - ETA: 0s - loss: 0.2070 - accuracy: 0.31 - ETA: 0s - loss: 0.1168 - accuracy: 0.29 - ETA: 0s - loss: 0.0943 - accuracy: 0.29 - 0s 70us/step - loss: 0.0545 - accuracy: 0.2937 - val_loss: 0.0247 - val_accuracy: 0.3125\n",
      "Epoch 471/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.5577 - accuracy: 0.400 - ETA: 0s - loss: 0.1514 - accuracy: 0.309 - ETA: 0s - loss: 0.1474 - accuracy: 0.29 - ETA: 0s - loss: 0.1495 - accuracy: 0.29 - 0s 70us/step - loss: 0.0630 - accuracy: 0.2956 - val_loss: 0.0225 - val_accuracy: 0.2837\n",
      "Epoch 472/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.4401 - accuracy: 0.300 - ETA: 0s - loss: 0.0433 - accuracy: 0.259 - ETA: 0s - loss: 0.0079 - accuracy: 0.28 - ETA: 0s - loss: 0.0868 - accuracy: 0.28 - 0s 70us/step - loss: 0.0803 - accuracy: 0.2897 - val_loss: 0.0283 - val_accuracy: 0.3162\n",
      "Epoch 473/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.9226 - accuracy: 0.300 - ETA: 0s - loss: 0.0173 - accuracy: 0.275 - ETA: 0s - loss: 0.0922 - accuracy: 0.28 - ETA: 0s - loss: 0.0499 - accuracy: 0.29 - 0s 70us/step - loss: 0.0550 - accuracy: 0.2975 - val_loss: 0.0076 - val_accuracy: 0.3013\n",
      "Epoch 474/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.1861 - accuracy: 0.20 - ETA: 0s - loss: 0.1880 - accuracy: 0.30 - ETA: 0s - loss: 0.0664 - accuracy: 0.29 - ETA: 0s - loss: 0.0566 - accuracy: 0.29 - 0s 71us/step - loss: 0.0556 - accuracy: 0.2919 - val_loss: 0.0179 - val_accuracy: 0.3113\n",
      "Epoch 475/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.3295 - accuracy: 0.10 - ETA: 0s - loss: -0.2246 - accuracy: 0.298 - ETA: 0s - loss: -0.0456 - accuracy: 0.301 - ETA: 0s - loss: 0.0485 - accuracy: 0.301 - 0s 70us/step - loss: 0.0699 - accuracy: 0.2969 - val_loss: 0.0400 - val_accuracy: 0.3125\n",
      "Epoch 476/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.2008 - accuracy: 0.10 - ETA: 0s - loss: 0.2103 - accuracy: 0.31 - ETA: 0s - loss: 0.1630 - accuracy: 0.30 - ETA: 0s - loss: 0.0946 - accuracy: 0.29 - 0s 71us/step - loss: 0.0602 - accuracy: 0.2928 - val_loss: 0.0557 - val_accuracy: 0.3237\n",
      "Epoch 477/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7435 - accuracy: 0.40 - ETA: 0s - loss: 0.0107 - accuracy: 0.27 - ETA: 0s - loss: 0.0924 - accuracy: 0.29 - ETA: 0s - loss: 0.1220 - accuracy: 0.29 - 0s 69us/step - loss: 0.0586 - accuracy: 0.2966 - val_loss: 0.0155 - val_accuracy: 0.2912\n",
      "Epoch 478/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8595 - accuracy: 0.20 - ETA: 0s - loss: 0.2220 - accuracy: 0.30 - ETA: 0s - loss: 0.0561 - accuracy: 0.29 - ETA: 0s - loss: 0.0108 - accuracy: 0.29 - 0s 73us/step - loss: 0.0596 - accuracy: 0.2941 - val_loss: 0.0578 - val_accuracy: 0.3225\n",
      "Epoch 479/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.40 - ETA: 0s - loss: 0.0487 - accuracy: 0.29 - ETA: 0s - loss: 0.0602 - accuracy: 0.29 - ETA: 0s - loss: 0.0299 - accuracy: 0.29 - 0s 74us/step - loss: 0.0570 - accuracy: 0.2984 - val_loss: 0.0056 - val_accuracy: 0.2950\n",
      "Epoch 480/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2063 - accuracy: 0.400 - ETA: 0s - loss: -0.1148 - accuracy: 0.294 - ETA: 0s - loss: -0.0093 - accuracy: 0.297 - ETA: 0s - loss: 0.0015 - accuracy: 0.292 - ETA: 0s - loss: 0.0587 - accuracy: 0.29 - 0s 76us/step - loss: 0.0540 - accuracy: 0.2944 - val_loss: 0.0035 - val_accuracy: 0.3025\n",
      "Epoch 481/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0954 - accuracy: 0.50 - ETA: 0s - loss: 0.1257 - accuracy: 0.30 - ETA: 0s - loss: 0.2031 - accuracy: 0.29 - ETA: 0s - loss: 0.1159 - accuracy: 0.29 - 0s 71us/step - loss: 0.0592 - accuracy: 0.2912 - val_loss: -0.0061 - val_accuracy: 0.2875\n",
      "Epoch 482/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.5900 - accuracy: 0.500 - ETA: 0s - loss: -0.0338 - accuracy: 0.273 - ETA: 0s - loss: 0.0198 - accuracy: 0.294 - ETA: 0s - loss: 0.0175 - accuracy: 0.29 - 0s 71us/step - loss: 0.0509 - accuracy: 0.2941 - val_loss: -0.0087 - val_accuracy: 0.2875\n",
      "Epoch 483/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.6476 - accuracy: 0.200 - ETA: 0s - loss: 0.2597 - accuracy: 0.295 - ETA: 0s - loss: 0.1064 - accuracy: 0.29 - ETA: 0s - loss: -0.0014 - accuracy: 0.293 - 0s 70us/step - loss: 0.0410 - accuracy: 0.2944 - val_loss: 0.0603 - val_accuracy: 0.3300\n",
      "Epoch 484/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.7080 - accuracy: 0.40 - ETA: 0s - loss: -0.1694 - accuracy: 0.301 - ETA: 0s - loss: 0.0141 - accuracy: 0.292 - ETA: 0s - loss: 0.0306 - accuracy: 0.29 - 0s 71us/step - loss: 0.0620 - accuracy: 0.2944 - val_loss: -0.0032 - val_accuracy: 0.2975\n",
      "Epoch 485/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.1060 - accuracy: 0.200 - ETA: 0s - loss: 0.0818 - accuracy: 0.285 - ETA: 0s - loss: 0.0306 - accuracy: 0.28 - ETA: 0s - loss: 0.0095 - accuracy: 0.29 - 0s 70us/step - loss: 0.0461 - accuracy: 0.2950 - val_loss: -0.0023 - val_accuracy: 0.2825\n",
      "Epoch 486/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.1464 - accuracy: 0.10 - ETA: 0s - loss: -0.0191 - accuracy: 0.269 - ETA: 0s - loss: -0.0310 - accuracy: 0.285 - ETA: 0s - loss: 0.0228 - accuracy: 0.287 - 0s 74us/step - loss: 0.0425 - accuracy: 0.2909 - val_loss: -0.0047 - val_accuracy: 0.2862\n",
      "Epoch 487/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 2.1972 - accuracy: 0.20 - ETA: 0s - loss: 0.1921 - accuracy: 0.29 - ETA: 0s - loss: 0.1295 - accuracy: 0.29 - ETA: 0s - loss: 0.1425 - accuracy: 0.29 - 0s 73us/step - loss: 0.0287 - accuracy: 0.2972 - val_loss: 0.0243 - val_accuracy: 0.2862\n",
      "Epoch 488/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.2045 - accuracy: 0.100 - ETA: 0s - loss: -0.0144 - accuracy: 0.277 - ETA: 0s - loss: 0.0641 - accuracy: 0.287 - ETA: 0s - loss: 0.0463 - accuracy: 0.28 - 0s 73us/step - loss: 0.0545 - accuracy: 0.2912 - val_loss: 0.0237 - val_accuracy: 0.3187\n",
      "Epoch 489/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.30 - ETA: 0s - loss: 0.0613 - accuracy: 0.32 - ETA: 0s - loss: 0.0658 - accuracy: 0.29 - ETA: 0s - loss: 0.0147 - accuracy: 0.29 - 0s 71us/step - loss: 0.0516 - accuracy: 0.2959 - val_loss: 0.0828 - val_accuracy: 0.3388\n",
      "Epoch 490/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -2.2768 - accuracy: 0.200 - ETA: 0s - loss: 0.1260 - accuracy: 0.276 - ETA: 0s - loss: 0.0414 - accuracy: 0.28 - ETA: 0s - loss: 0.0014 - accuracy: 0.29 - 0s 73us/step - loss: 0.0468 - accuracy: 0.2937 - val_loss: -0.0093 - val_accuracy: 0.2850\n",
      "Epoch 491/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.30 - ETA: 0s - loss: -0.1088 - accuracy: 0.300 - ETA: 0s - loss: -0.0196 - accuracy: 0.288 - ETA: 0s - loss: 0.0241 - accuracy: 0.298 - 0s 74us/step - loss: 0.0444 - accuracy: 0.2959 - val_loss: -0.0086 - val_accuracy: 0.2862\n",
      "Epoch 492/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.20 - ETA: 0s - loss: -0.0188 - accuracy: 0.282 - ETA: 0s - loss: 0.0107 - accuracy: 0.272 - ETA: 0s - loss: 0.0043 - accuracy: 0.28 - 0s 73us/step - loss: 0.0465 - accuracy: 0.2931 - val_loss: 0.0158 - val_accuracy: 0.3125\n",
      "Epoch 493/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -1.2205 - accuracy: 0.500 - ETA: 0s - loss: 0.0235 - accuracy: 0.308 - ETA: 0s - loss: 0.0765 - accuracy: 0.31 - ETA: 0s - loss: 0.0537 - accuracy: 0.30 - 0s 74us/step - loss: 0.0386 - accuracy: 0.3003 - val_loss: 0.0032 - val_accuracy: 0.2800\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.30 - ETA: 0s - loss: -0.0065 - accuracy: 0.306 - ETA: 0s - loss: 0.0194 - accuracy: 0.300 - ETA: 0s - loss: 0.0371 - accuracy: 0.29 - 0s 67us/step - loss: 0.0426 - accuracy: 0.2912 - val_loss: -0.0080 - val_accuracy: 0.2812\n",
      "Epoch 495/500\n",
      "3200/3200 [==============================] - ETA: 4s - loss: -0.5474 - accuracy: 0.200 - ETA: 0s - loss: 0.0043 - accuracy: 0.304 - ETA: 0s - loss: 0.0879 - accuracy: 0.30 - ETA: 0s - loss: 0.0767 - accuracy: 0.29 - 0s 77us/step - loss: 0.0580 - accuracy: 0.2909 - val_loss: -0.0164 - val_accuracy: 0.2862\n",
      "Epoch 496/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.4589 - accuracy: 0.20 - ETA: 0s - loss: 0.2339 - accuracy: 0.29 - ETA: 0s - loss: 0.0969 - accuracy: 0.28 - ETA: 0s - loss: 0.0925 - accuracy: 0.29 - 0s 74us/step - loss: 0.0463 - accuracy: 0.2900 - val_loss: -0.0046 - val_accuracy: 0.2875\n",
      "Epoch 497/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.1165 - accuracy: 0.20 - ETA: 0s - loss: 0.2094 - accuracy: 0.29 - ETA: 0s - loss: 0.0930 - accuracy: 0.29 - ETA: 0s - loss: 0.0061 - accuracy: 0.29 - ETA: 0s - loss: 0.0235 - accuracy: 0.29 - 0s 77us/step - loss: 0.0434 - accuracy: 0.2947 - val_loss: -0.0069 - val_accuracy: 0.3075\n",
      "Epoch 498/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -0.8332 - accuracy: 0.500 - ETA: 0s - loss: -0.1596 - accuracy: 0.293 - ETA: 0s - loss: 0.0367 - accuracy: 0.294 - ETA: 0s - loss: 0.0243 - accuracy: 0.29 - 0s 72us/step - loss: 0.0358 - accuracy: 0.2928 - val_loss: -0.0031 - val_accuracy: 0.2812\n",
      "Epoch 499/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: -3.8617 - accuracy: 0.100 - ETA: 0s - loss: 0.0972 - accuracy: 0.290 - ETA: 0s - loss: 0.1072 - accuracy: 0.29 - ETA: 0s - loss: 0.1073 - accuracy: 0.29 - 0s 70us/step - loss: 0.0382 - accuracy: 0.2937 - val_loss: -7.8176e-04 - val_accuracy: 0.2925\n",
      "Epoch 500/500\n",
      "3200/3200 [==============================] - ETA: 0s - loss: 1.0340 - accuracy: 0.10 - ETA: 0s - loss: -0.0307 - accuracy: 0.294 - ETA: 0s - loss: 0.0094 - accuracy: 0.284 - ETA: 0s - loss: -0.0260 - accuracy: 0.284 - ETA: 0s - loss: 0.0293 - accuracy: 0.292 - 0s 73us/step - loss: 0.0446 - accuracy: 0.2925 - val_loss: -0.0115 - val_accuracy: 0.3013\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_trainset, Y_trainset, validation_data=(X_testset,Y_testset), epochs=500, batch_size=10) #150 epochs normalyand 10 for batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Cross-Validation</h3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 32.50%\n",
      "accuracy: 33.75%\n",
      "accuracy: 32.75%\n",
      "accuracy: 31.25%\n",
      "accuracy: 31.75%\n",
      "accuracy: 34.75%\n",
      "accuracy: 32.50%\n",
      "accuracy: 33.25%\n",
      "accuracy: 31.25%\n",
      "accuracy: 31.25%\n",
      "32.50% (+/- 1.12%)\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    #Define Keras Model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_trainset, Y_trainset, validation_data=(X_testset,Y_testset), epochs=50, batch_size=10, verbose=0)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores =  model.evaluate(X_testset, Y_testset, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - ETA:  - 0s 17us/step\n",
      "Accuracy: 30.13 %\n",
      "Loss: -1.15 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_testset, Y_testset)\n",
    "print('Accuracy: %.2f' % (accuracy[1]*100),'%')\n",
    "print('Loss: %.2f' % (accuracy[0]*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(0, 25))\n",
    "y1 = hist.history['accuracy'] \n",
    "y2 = hist.history['val_accuracy'] \n",
    "yhat1 = savgol_filter(y1, 21, 5) # window size 51, polynomial order 3\n",
    "yhat2 = savgol_filter(y2, 21, 5) # window size 51, polynomial order 3\n",
    "plt.plot(x,y1, color='honeydew')\n",
    "plt.plot(x,y2, color='azure')\n",
    "plt.plot(x,yhat1, color='green')\n",
    "plt.plot(x,yhat2, color='blue')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['', '', 'Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = hist.history['accuracy']\n",
    "Y = hist.history['val_accuracy'] \n",
    "x_y = [x - y for x, y in zip(X, Y)]\n",
    "\n",
    "x = list(range(0, 25))\n",
    "y = x_y\n",
    "yhat = savgol_filter(y, 21, 5) # window size 51, polynomial order 3\n",
    "plt.plot(x,y, color='mistyrose')\n",
    "plt.plot(x,yhat, color='red')\n",
    "plt.title('Accuracy difference betwen train and test data')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['', '', 'train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(0, 25))\n",
    "y1 = hist.history['loss'] \n",
    "y2 = hist.history['val_loss'] \n",
    "yhat1 = savgol_filter(y1, 21, 2) # window size 501, polynomial order 3\n",
    "yhat2 = savgol_filter(y2, 21, 2) # window size 501, polynomial order 3\n",
    "plt.plot(x,y1, color='honeydew')\n",
    "plt.plot(x,y2, color='azure')\n",
    "plt.plot(x,yhat1, color='green')\n",
    "plt.plot(x,yhat2, color='blue')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['', '', 'train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = hist.history['val_loss']\n",
    "Y = hist.history['loss'] \n",
    "x_y = [x - y for x, y in zip(X, Y)]\n",
    "\n",
    "x = list(range(0, 25))\n",
    "y = x_y\n",
    "yhat = savgol_filter(y, 21, 3) # window size 51, polynomial order 3\n",
    "plt.plot(x,y, color='mistyrose')\n",
    "plt.plot(x,yhat, color='red')\n",
    "plt.title('Loss difference betwen train and test data')\n",
    "plt.ylabel('Difference')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['', '', 'train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Predictions</h3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_trainset)\n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('%d (expected %d)' % (predictions[i], Y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<h3>Plot the ANN</h3>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_viz(model, title=\"Auhtor_Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
